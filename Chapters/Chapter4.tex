
\chapter{Genomic prediction of phenotypic values of quantitative traits using Artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------



\section{Introduction}
\subsection{A brief history of machine learning }

While machine learning, neural networks, deep learning became essential tools for many applications in more recent years,
their mathematical principals date back to the early 1950s and 1960s. Figure \ref{fig:perceptron} schematically  show the
basic perceptron model as proposed by Rosenblatt, which was designed to mimic the information flow in biological nervous systems
\cite{rosenblatt1961}

\begin{figure}[th]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png}
\decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt}
\label{fig:perceptron}
\end{figure}

This basic perceptron, which contrary to perceptrons used nowadays does not have an activation function, takes n binary
inputs $x_1 , x_2 .... x_n$ and produces a single, likewise binary, output $y$ after being processed by the perceptron or neuron.
To achieve this Rosenblatt introduced the concept of weights which indicated a certain relative importance to the outcome of the
output. $w_1 , w_2 ... w_n$. The output $y$ is determined by the weighted sum of the weights and biases $\sum_i w_ix_i $. If a certain
threshold value is met  the neuron is either activated and outputs 1 or not and outputs 0. This is algebraically represented in
\ref{eqn:weights}
\begin{subequations}
  \begin{align}
    0  = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
    1 =  \mbox{if } \sum_i^n w_i x_i - \theta > 0
  \end{align}
  \label{eqn:weights}
\end{subequations}

Next to the weights $w_n$ and the inputs $x_n$ a third term $\theta$ is introduced in equation \ref{eqn:weights} which represents
the activation threshold in per definition is negative. A single perceptron is a linear classifier and can only be trained on
linearly separable functions and can used as shown by \cite{rosenblatt1961} to solve simple logical operations as AND, OR and not.
The simple perceptron fails, due to non-linearity, to perform XOR operations as shown by \cite{marvin1969}. This discovery let to
a near stillstance in the research of artificial neural networks in the 1970s.



\subsection{On the nature of quantitative traits}
According to the omnigenic model which is an extension of the polygenic model proposed by \cite{boyle2017expanded} and
thoroughly reviewed in \cite{timpson2018} all traits or phenotypic values are influenced
by a great number or all genes in the genome. Therefore resulting in traits following certain gradual statistical distributions
instead of being binned in classes or even binary. Intuitively this might be contradicting with the foundation of modern Genetics -
Mendel's three laws. That where derived from observations with where mainly influenced by one locus. But staying with one of
Mendel's examples the round or wrinkled surfaces of  peas \textit{Pisum sativum}, an assessment of a couple of thousands peas, would
most likely inevitably
lead to the conclusion that form the ``roundest'' to the ``wrinkliest'' pea any gradual step between those is possible and observable.
Mendel's third law of independent segregation also only holds true under certain assumptions. The most simplest one being that the traits
under investigation have to be located on different linkage groups. Otherwise for the 7 traits used in Mendel's initial studies would not have
segregated independently. The odds of 7 randomly selected traits being on 7 different linkage groups are rather small, especially taking
into account, that the genome of the \textit{P. sativum} consists of only 7 chromosomes itself \cite{kalo2004}. Mendel probably new about
traits not following its own law's,
as well as being aware of the quantitative nature of traits such as the constitution of surfaces of peas or the color of petals. But being the
pioneer of a then rather unexplored field of science, some of which big questions we fail to satisfactory answer today, he did not have the
resources or the knowledge to explain behavior's not ``mendeling'', that were only able to be deciphered in later decades and centuries
based on his ground-breaking work.  \\
Initially thought to be contradicting to Mendel's ideas Darwin proposed the concept's of evolution due to natural selection which also
introduce the idea of traits following a gradual distribution \cite{darwin1859}. This contrast led to a long lasting debate in the
scientific community in the early 1900s, between the Mendelians and the biometricians who believed in the quantitative nature of continuous
traits. This conflict has eventually been solved by Fisher's fundamental work published in 1918 \cite{fisher1919xv}. His theroies combined the
then in all fields of science popular research of distributions with genomics. He he mathematically proved that traits influenced by many
genes, with randomly-sampled alleles follow a continous normal distrubition in a population. While this combined the indeas of Mendel and the
biometricans it opened an other long debated queston of effect size and the overall architecture of complex traits. While in the theory of
monogenic traits the effect size of the single gene on the trait is 1 or 100 \% with an increasing number of genes influecning a complex
traits the \textit{per s√®} contribution of single gene has to decrease with an increasing number of loci determining the value a given
trait. In the 1990s it has been thought, that complex traits are predominantly controlled from few genes with a large to medium effect size,
while others had a minimal influence \cite{zhang2018esti}. \\
With the upcomming popularity of GWAS as the favored method to decipher genetic archtitectures of traits, or having pioneered in human
genetics in became clear that the majority of the effect sizes are tiny < 1 \% while there are very few loci which have a moderate
effect on the phenotypic variance of a population with around 10 \% or less \cite{korte2013advantages}, \cite{stringer2011}.
This nature of quantitative traits present greate challenges to animal \cite{goddard2009} and plant breeding \cite{wurschum2012},
in further improving crop or livestock performances, as well complicating the decomposition of genomic causes for diseases like shiozphrania
or autism in human medicine \cite{de2014}, \cite{purcell2014}. \\
While the complex nature of the architecture of quantitave traits provide enough challanges as is, all traits will also be influenced
by the evironment from which an induvidual originates.
Therefore the distribution of trait valaues in a given population can be expressed as the addition of the variances of its genetic and the
environemtnal effects \ref{eqn:PGE}.

\begin{equation}
  \sigma_{P} = \sigma_{G} + \sigma_{E}
  \label{eqn:PGE}
\end{equation}

The genomic and the evironmental effects not only influnce the phenotypic variance directly, but the environment also has an influnce
on gene expression methylation of DNA bases etc. and therefore the equation \ref{eqn:PGE} needs to be extend by the variance of the gene-
environment interactions $\sigma_{GxE}$  \ref{eqn:PGEGE} , \cite{lynch1998}, \cite{walsh2018}.
                                                               
\begin{equation}
  \sigma_{P} = \sigma_{G} + \sigma_{E} + \sigma_{GxE}
  \label{eqn:PGGE}
\end{equation}


Equation \ref{eqn:PGGE} shows the decomposition of the phenotypic variance, to thoroughly understand complex genetic archiectures of traits
the genetic variance needs to be decomposed further in its additve, dominance and epistatic components \ref{eqn:GAD}

\begin{equation}
  \sigma_{G} = \sigma_{A} + \sigma_{D} + \sigma_{I}
  \label{eqn:GAD}
\end{equation}

The addivte effects are caused by single, for this model mostly homozygous, loci while the variance caused by dominance effcts, is caused by
heterozygous loci and their resulting interactions being full-, over- , co- or underdominant. And lastely the interaction effects  that are a
result of two or more genes only having an impact if the involved genes cooccur in a certain state. The resulting variance is commonly known
as gene-gene interactions and/or epistasis \cite{falconer1996}. \\
Since possible interactions in a genome can happen between additve or dominant or a combinaton of those loci. The variance due to interaction
effects $\sigma_{I}$ can be further dissembled in the variance resulting from additive-addtive $\sigma_{AA}$ domiant-dominant $\sigma{DD}$ and
additive-dominant $sigma{AD}$ terms as represented in equation \ref{eqn:IAA}.


\begin{equation}
  \sigma_{I} = \sigma_{AxA} + \sigma_{DxD} + \sigma_{AxD}
  \label{eqn:IAA}
\end{equation}

Knowledge of the variace components involved in the expression of a trait in population, lead up to the estimation of the total influnce
of all genetic variances and the environmental variance one the phentypic distrubition. This concept if called heritabilty.
The heritabilty of a trait $h^2$ accounts for the proportion of the pheotyping variace controlled by the total phenotypic variance.




\begin{equation}
  h^2 = \frac{\sigma_{G}}{\sigma_{P}}
  \label{eqn:h2G}
\end{equation}


\begin{equation}
  h^2 = \frac{\sigma_{A}}{\sigma_{P}}
  \label{eqn:h2a}
\end{equation}







\subsection{Genomic selection using artificial neural networks }
Genomic selection (GS) has been successfully applied in animal \cite{gianola2015one}, \cite{hayes2010genome}
and plant breeding \cite{crossa2010}, \cite{desta2014genomic}, \cite{heffner2010plant}, \cite{crossa2017genomic}
as well as in medical applications, since it was first reported  \cite{hayes2001}. Since then the repertoire of
methods for predicting phenotypic values has increased rapidly e.g.\cite{dlc2009}, \cite{habier2011}, \cite{gianola2013} ,
\cite{crossa2017}. The most commonly applied methods include GULP and a set of related algorithms known as the bayesian alphabet
\cite{gianola2009}. 
Genomic prediction in general has repeatedly been shown to outperform pedigree-based methods \cite{crossa2010},
\cite{albrecht2011} and is nowadays used in many plant and animal breeding schemes. 
It has also been shown that using whole-genome information is superior to using only feature-selected markers
with known QTLs for a given trait \cite{bernardo2007}, \cite{heffner2011} in some cases. A more recent study
\cite{azodi2019} compared 11 different genomic prediction algorithms with a variety of data sets and found
contradicting results, indicating that feature selection can be usefull in some cases the when the whole
genome regression is performed by neural nets 
While every new method is a valuable addition to the tool-kits for genomic selection, some fundamental
problems remain unsolved, of which the n>>p problematic stands out. Usually in genomic selection settings
the size of the training population (TRN) with n phenotypes is substantially smaller than the number of
markers (p) \cite{fan2014challenges}. Making the number of features immensely large,  even when SNP-SNP
interactions are not considered.  Furthermore each marker is treated as an independent observation
neglecting collinearity and linkage disequilibrium (LD). 
Further difficulties arise through non-additive, epistatic and dominance marker effects. The main problem
with epistasis issue quantitative genetics is the almost infinite amount of different marker combinations,
that cannot be represented within the size of TRN in the thousands, the same problems arises for example
in GWA studies \cite{korte2013advantages}. With already large p the number of possible additive SNP-SNP interactions
potentiates to $p^{(p-1)}$. Methods that attempt to overcome those issues are EG-BLUP, using an enhanced
epistatic kinship matrix and reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015}, \cite{martini2017genomic}.

In the past 10 years, due to increasing availability of high performance computational hardware with decreasing
costs and parallel development of free easy-to-use software, most prominent being googles library TensorFlow
\cite{TF2016} and Keras \cite{keras2015}, machine learning (ML) has experienced a renaissance.
ML is a set of methods and algorithms used widely for regression and classification problems. popular
among those are e.g. support vector machines, multi-layer perceptrons (MLP) and convolutional neural
networks. The machine learning mimics the architecture of neural networks and are therefore commonly
referred to as artificial neural networks (ANN). Those algorithms have widely been applied in many
biological fields \cite{min2017deep} , \cite{lan2018survey}, \cite{mamoshina2016applications},
\cite{angermueller2016} , \cite{webb2018deep}, \cite{rampasek2016tensorflow}. 

A variety of studies assessed the usability of ML in genomic prediction \cite{gonzalez2018applications},
\cite{gonza2016}, \cite{ogutu2011comparison}, \cite{montesinos2019benchmarking}, \cite{grinberg2018evaluation}
, \cite{cuevas2019deep}, \cite{montesinos2019new}, \cite{ma2017deepgs}, \cite{qiu2016application},
\cite{gonza2012} \cite{li2018genomic}.
Through all those studies the common denominator is that there is no such thing as a gold standard
for genomic prediction. No single algorithm was able to outperform all the others tested in a single
of those studies, let alone in all. While the generally aptitude of ML for genomic selection has been
repeatedly shown, how no evidence exists that neural networks can outperform or in many cases perform
on that same level as mixed-model approaches as GBLUP \cite{hayes2001prediction}. While in other fields
like image classification neural networks have up to 100s of hidden layers \cite{he2016deep} the commonly
used fully-connected networks in genomic prediction of 1 - 3 hidden layers. 
With 1 layer networks often being the most successful among those. Contradicting to the idea behind machine
learning in genomic selection 1 hidden layer networks will be inapt to capture interactions between loci and
thus only account for additive effects. As shown in \cite{azodi2019} convolutional networks perform worse
than fully-connected networks in genomic selection, which again is contradicting to other fields where
convolutional layers are applied successfully, e.g natural language processing \cite{dos2014deep} or
medical image analysis  \cite{litjens2017survey}. Instead of using convolutional layers and fully-connected
layers only, as show in Pook et al 2019, we also propose to use locally-connected layer in combination with
fully-connected layers. While CL and LCL are closely related they have a significant difference. While in CL
weights are shared between neurons in LCLs each neuron as its own weight. This leads to a reduced number of
parameters to be trained in the following FCLs, and should therefore theoretically lead to a decrease in
overfitting a common problem in machine learning. 
To evaluate the results of Pook et al. 2019 accomplished with simulated data we used the data sets
generated in the scope of the 1001 genome project of \textit{Arabidopsis thaliana} \cite{1001genome}



\section{Proof of concept}



\section{Material}
\section{Methods}
\section{Results}
\section{Discussion}

