\chapter{Genomic prediction of phenotypic values of quantitative traits using Artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------



\section{Introduction}
\subsection{A brief history of machine learning} \label{introml}
\subsubsection{Basic perceptron model}

While machine learning, neural networks, deep learning became essential tools for many applications in more recent
years, their mathematical principals date back to the early 1950s and 1960s. Figure \ref{fig:perceptron} schematically
show the basic perceptron model as proposed by Rosenblatt, which was designed to mimic the information flow in
biological nervous systems \cite{rosenblatt1961}

\begin{figure}[th]
 \centering \includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png} \decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt}
\label{fig:perceptron}
\end{figure}

This basic perceptron, which contrary to perceptrons used nowadays does not have an activation function, takes n binary
inputs $x_1 , x_2 .... x_n$ and produces a single, likewise binary, output $y$ after being processed by the perceptron
or neuron. To achieve this Rosenblatt introduced the concept of weights which indicated a certain relative importance to
the outcome of the output. $w_1 , w_2 ... w_n$. The output $y$ is determined by the weighted sum of the weights and
biases $\sum_i w_ix_i $. If a certain threshold value is met the neuron is either activated and outputs 1 or not and
outputs 0. This is algebraically represented in \ref{eqn:weights}

\begin{subequations}
 \begin{align}
   0 = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
   1 = \mbox{if } \sum_i^n w_i x_i - \theta > 0
 \end{align}
 \label{eqn:weights}
\end{subequations}

Next to the weights $w_n$ and the inputs $x_n$ a third term $\theta$ is introduced in equation \ref{eqn:weights} which
represents the activation threshold in per definition is negative. A single perceptron is a linear classifier and can
only be trained on linearly separable functions and can used as shown by \cite{rosenblatt1961} to solve simple logical
operations as AND, OR and not. The simple perceptron fails, due to non-linearity, to perform XOR operations as shown by
\cite{marvin1969}. This discovery let to a near still stance in the research of artificial neural networks in the
1970s. This time period as now often referred to as the first AI-winter. Another reason that massively hindered the
applications and research of machine learning during that time, was the compared to
modern times incredibly small amount of computational power available \cite{nguyen1990truck}. \\
More complex decision making, like solving XOR problems, requires more complex structures than a single
perceptron. Continuing the trend of mimicking human neural networks, multiple artificial neurons are stacked into layers
and these layers, are connected to each other allowing communication between the many perceptrons in a such generated
network. Figure \ref{fig:nn} shows schematically the basic structure of such a network, now container three types of
layers. (i) the input layer, (ii) one or more hidden layers and (iii) one output layer, which in this case only consists
of one neuron.

\begin{figure}[H]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/neuralnet}
\decoRule
\caption[Schematic layout of a simple multi-layer perceptron]{Schematic layout of a simple multi-layer perceptron}
\label{fig:nn}
\end{figure}

In the sample layout of figure \ref{fig:nn} the neurons in the first column weigh the inputs and pass those the the
neurons on the second layer. In this case all neurons on the first layer or connected to all neurons on the second
layer, such layers are referred to fully-connected layers (FLC) , and their resulting networks are often called
multi-layer perceptrons (MLP). This architecture enables the network to perform more complex calculations and result in
more abstract decisions than single neurons or single layer architectures.

\subsubsection{Activation functions}

The neurons discussed so far are only capable of outputting binary results. Either 0 or 1, depending on the threshold
values being met or not. For more complex estimations it is desirable that small changes in the input also result in
small changes of the output. This requirement can not be met with binary outputs. Activation functions for a given node
provides rules for the output in accordance to the inputs \cite{vzilinskas2006practical}.

\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.85\textwidth]{Figures/activation}
\decoRule
\caption[Popular activation functions for neural networks]{Popular activation functions used in neural networks.
 \textbf{A} Binary step activation function.
 \textbf{B} Identity activation function.
 \textbf{C} Sigmoid or logistic activation function.
 \textbf{D} tangens hyperbolicus activation function.
 \textbf{E} rectified linear units activation function .
 \textbf{F} SoftPlus activation function.}
\label{fig:activation}
\end{figure}

Figure \ref{fig:activation} \textbf{A} shows six of the most commonly used activation functions
\cite{warner1996understanding}. The simplest one was introduced, is the binary step activation function equation
\ref{eqn:binary}, which properties have been discussed along the perceptron model. All other activation produce
continuous outputs from any given input. Basically any mathematical function can serve as an activation function in
neural nets, starting with a simple identity function \ref{eqn:ident} , \ref{fig:activation} \textbf{B}. Sigmoid figure
\ref{fig:activation} \textbf{C}, equation \ref{eqn:sigmoid} and tanh figure \ref{fig:activation} \textbf{D}, equation
\ref{eqn:tanh}, when $x \rightarrow \infty$ or $x \rightarrow -\infty$ they have similar properties to the binary
function, but produce continuous output around 0.

\begin{equation}
 f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
  0 \; for \; x < 0 \\ 
  1 \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:binary}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = x
 \label{eqn:ident}
\end{equation}
       
\begin{equation}
 f(x) = \sigma(x) = \frac{1}{1+e^{-x}} 
 \label{eqn:sigmoid}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
 \label{eqn:tanh}
\end{equation}

\begin{equation}
  f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
  0 \; for \; x < 0 \\ 
  x \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:relu}
\end{equation}

\begin{equation}
  f(x) = ln(1+e^x)
 \label{eqn:softplus}
\end{equation}

ReLU (equation \ref{eqn:relu}) and the softplus (equation \ref{eqn:softplus}) share similar properties as well, the
latter one being a smoothed version of ReLU. Rectifiers as activation functions have been introduced in 2000s
\cite{hahnloser2000digital} and have since then overtaken all others as the most popular activations functions in neural
networks and deep learning today \cite{lecun2015deep} and they have proven to be superior in deep-learning algorithms
that sigmoid or logistic functions. One of the advantages leading to the superiority of ReLUs is that with randomly
initialized weights only half of the ReLU neurons are activated, compared to tanh and sigmoid activation
\cite{glorot2011deep}. All activation functions shown in figure \ref{fig:activation}, but the binary step function,
share one common property: a small change of the input weight will result in small changes in the output, while a small
change of the input for the binary step function leads to either no or a complete change of the output. This property
is, as described below, is an
important prerequisite for networks being able to learn. \\

\subsubsection{Gradient descent algorithm}

Let the network shown in \ref{fig:nn} be for the classification of a arbitrary phenotype like blue petals with
$x_1 ... x_4$ on the input layers being genetic markers as features. And the output layer displaying a value from 0 to
1, meaning yes: blue petals from 0 - 0.5 and no blue petals from 0.5 to 1. To quantify how well the network performs on
achieving that goal a loss function is applied \cite{schmidhuber2015deep}. There is a large variety of different loss
functions available for neural networks like mean squared error (MSE), root mean squared error (RMSE), cross-entropy and
many others. In general MSE, MSE are commonly used for regression problems, with the latter being less popular and
cross-entropy also called log loss is used for binary or multi-class classification problems
\cite{janocha2017loss}. Since all problems presented in due course or regression problems, that use MSE as their loss
function, this will be the only one emphasized.

\begin{equation}
 MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \widetilde{y})^2
 \label{eqn:MSE}
\end{equation}

Equation \ref{eqn:MSE} shows the MSE function which is the sum of the squares of the differences of all the predicted
and the real values. The same function can be rewritten with the previously used terminology of weights and biases in
equation \ref{eqn:MSE}.

\begin{equation}
  L(w,b) =  \frac{1}{2n} \sum_x \| y(x) - \widetilde{y}\|^2.
 \label{eqn:MSE2}
\end{equation}

With $w$ and $b$ as the collection of all the weights and the biases in the network used to optimize the function
$y(x)$. Giving the quadratic nature of the function the $L(w,b)$ will always be positive. And if $L(w,b) \rightarrow 0$
the loss is minimal, meaning that the real and predicted values are close together and
the network found weights and biases that explain the output well. \\
A widely used function to find the optimum for such a loss function is gradient descent. Its objective is to fine the
minimum for the loss function \cite{bottou1991stochastic}.  The behind gradient descent or other optimizing algorithms
is start with randomly initialized weights and biases and repeatedly move them in direction $\Delta w$ and $\Delta
b$. This results in a change of the loss function as shown in equation \ref{eqn:deltaL}, making use of partial
derivatives.

\begin{equation}
 \Delta L = \frac{\partial L}{\partial w} \Delta w + \frac{\partial L}{\partial b} \Delta b
\label{eqn:deltaL}  
\end{equation}

Ideally $\Delta L$ is negative and the optimization algorithm found $\Delta w$ and $\Delta b$ that lead to a reduction
of the loss. To simplify this problem let $\Delta d$ be the vector of changes: $\Delta d = (\Delta w , \Delta b)^T $ and
$\nabla L$ the vector of the partial derivatives: equation \ref{eqn:nabla}

\begin{equation}
 \nabla L = \left(\frac{\partial L}{\partial w}, \frac{\partial L}{\partial w}\right)^T
\label{eqn:nabla}
\end{equation}

Having defined $\nabla L$ and $\Delta d$ the term \ref{eqn:deltaL} can be simplified as equation \ref{eqn:cd}

\begin{equation}
\Delta C = \nabla L * \Delta d
 \label{eqn:cd}
\end{equation}

Now the task of gradient descent or any other optimizer is to find $\Delta d$ that results in $\Delta C$ being negative
as shown in equation \ref{eqn:eta}

\begin{equation}
 \Delta d = -\eta \nabla L
 \label{eqn:eta}
\end{equation}

In this case $\eta$ is a small positive decimal number, commonly referred to as the learning rate, which usually, but
not exclusively ranges from 0.1 to 0.001. However it can be larger or much smaller in some cases. Having found a way to
ensure that $\Delta L$ always decreases according to equation \ref{eqn:eta} it is utilized to repeatedly update the
gradient $\nabla L$. To make the gradient descent algorithm efficient the learning rate $\eta$ must be chosen
correctly. If $\eta$ is too large, the gradient $\Delta L$ might end up being larger than zero, leading to an increase
of the loss, and if the step size is too small convergence will either take too long or not take place at all
\cite{bergstra2011algorithms}. In practical machine learning approaches different learning rates are tested. There are
also algorithmic approaches. While equation \ref{eqn:deltaL} only accounts for two inputs features, it can be
generalized to compute $n$ inputs shown in equation \ref{eqn:gd}.

\begin{eqnarray}
 \nabla L = \left(\frac{\partial L}{\partial w_1}, \ldots ,
 \frac{\partial L}{\partial w_n}\right)^T
 \label{eqn:gd}
\end{eqnarray}

Equation \ref{eqn:gdwb} shows the gradient descent how it is used to repetitively update the weights and biases to
optimize the loss function $L(c,w)$ with $w$ and $b$ as the weight and bias matrices and the learning rate $\eta$. In
machine learning each iterational update of the network is often called epoch or training epoch.

\begin{subequations}
 \begin{align}
   w = w_i - \eta \frac{\partial}{\partial w}L(w) \\
   b = b_i - \eta \frac{\partial}{\partial b}L(b) \\
 \end{align}
 \label{eqn:gdwb}
\end{subequations}

Substituting the partial differentials with $\nabla L$ equation \ref{eqn:gdwb} a simplifies to:

\begin{equation}
  w = w_i - \eta  \nabla L
  \label{eqn:simplegd}  
\end{equation}

\subsubsection{Optimizers}
The previous section introduced the concept of gradient descent, an algorithm to minimize the loss function of the
weights and biases of a neural network. All other optimizers introduced here, are either variations or extensions of the
basic gradient descent algorithm (GD) shown in \ref{eqn:gdwb}. One disadvantage of gradient descent is that if the data
sets grow larger, the demand in memory for computation increases exponentially. Taking into consideration machine
learning is a popular method ind big data applications this is a serious drawback. Methods to overcome that are
stochastic gradient descent and mini-batch gradient descent.  The idea behind the latter is to randomly divide the
entity of the training data in sub-samples called mini-batches \cite{bottou-bousquet-2008}. The network is then trained
iteratively over the mini batches. The batch size influences the accuracy and the training speed and is another
hyperparameter which has to be tuned. If the batch size is 1 mini batch GD is also referred to as stochastical gradient
descent (SGD).  During the optimization process optimizers can find local minima in the cost function without being able
to overcome them to find the desired global minimum. An algorithm extending GD to accelerate the search of the global
minimum is momentum. Which allows the GD to speed up when the loss is decreasing and to slow down when going in the
wrong direction - increasing the loss function $L(w,b)$. This is achieved by accounting for the gradient of the previous
step in the calculation of the current step. This concept was introduced by \cite{polyak1964} and re-popularized
alongside backpropagation learning by \cite{rumelhart1988learning}.

\begin{equation}
  w = w_i - \eta \nabla L + \alpha \Delta w
  \label{eqn:momentum}
\end{equation}

Equation \ref{eqn:momentum} shows how the momentum is mathematically represented in GD to update the weights $w$ or
likewise the biases the delta of the weights multiplied by the coefficient $\alpha$ - the momentum, which usually ranges
from 0.1 to 0.9 and is another parameter to tuned for successful training. If the momentum is two small the GD will not
be able to overcome local minima and if $\alpha$ is two large the loss functions tends to oscillate without finding an
optimum \cite{lecun2015deep}.  For both of the momentum and the learning rate it is impractical to remain on the same
level during all training epochs. Because after each epoch the loss function is either closer or further away from its
global minima and depending on the distance to that minimum it is desirable to have larger or smaller learning rates and
momenta. This can be achieved with naive approaches for example using a step function to gradually decrease those values
after each iteration, or to utilize algorithmic approaches \cite{michie1994machine}.  There is a large variety of
optimizers trying to find optimal values for $\alpha$ and $\eta$ and till today this field is under active research
\cite{goodfellow2016deep}. Popular among those are: RMSprop \cite{hinton2012neural}; Nesterov momentum
\cite{dozat2016incorporating}; Adadelta \cite{zeiler2012adadelta}; Adagrad \cite{ruder2016overview} and Adam
\cite{kingma2014adam}. With Adam being the most widely used optimizer today.  Nesterov momentum is slight change to the
normal momentum capable of having huge impacts in practical applications, because it helps avoiding oscillations around the minimum by using intermediate information to adapt the momentum. \\
RMSProp - root mean square propagation - is a method aiming to adapt the learning rate algorithmically, by choosing
$\eta$ for each parameter.  And lastly the wide-spread Adam optimizer combines both of the features of momentum and
RMSProp and adapts the learning rate as well as the momentum iteratively \cite{kingma2014adam}.


\subsubsection{Backpropagation}
maybe i will leave out backpropagation
$\odot$

Backpropagation \cite{rumelhart1988learning}

\subsubsection{Regularization parameters}

When applying the combined aforementioned algorithms and optimizers to find global minima of a loss function of a neural
network a problem arises, because optimizers like Adam work ``too'' well. This issues is due to the fact that neural
networks have 100s of thousand of free parameters to be trained, deep neural networks have billions and trillions of
parameters. If training of the neural net continues for enough epochs eventually. The loss function will approach a
minimum and as $ L(w,b)\rightarrow 0 $ the initial conclusion could be that the training was quite successful, but when
trying to apply the network trained on the training data set (TRN) to a testing data set (TST). The loss and accuracy of
the prediction of TST and TST are very large or accordingly small. This phenomenon is know as overfitting and a lot of
fine tuning of hyperparameters is devoted to minimizing this effect \cite{tetko1995neural}. Figure \ref{fig:overfitting}
visualizes the effects of overfitting during training \cite{goodfellow2016deep}.

\begin{figure}[H]
   \centering \includegraphics[height=.35\textheight, width=1.1\textwidth]{Figures/overfitting} \decoRule
   \caption[Training vs. validation loss over time]{Learning curves showing how a loss function changes during training
     in the loss and validation data set. While the training loss approaches 0 the validation loss starts increasing
     after hitting a minimum. This effect is due to overfitting on the training data set. Figure from
     \cite{goodfellow2016deep}.}
 \label{fig:overfitting}
\end{figure}

\textbf{Cross-validation} \\

A method that is used in basically every training of neural network is splitting up the data sets in multiple sub-sets.
More specifically a training set (TRN) and a testing set (TST). The training set is used to minimize the loss functions
and its success is evaluated on the TRN set, by comparing the predicted values $\hat{y}$ with the real values in TST
$y$.  For all neural nets in this study person's correlation coefficient was chosen as performance metric, as in
equation \ref{eqn:pearson} \cite{soper1917distribution}.

\begin{equation}
\rho(y,\hat{y})  =  \frac{cov(y,\hat{y})}{  \sigma_y \sigma_{\hat{y}}}
  \label{eqn:pearson}
\end{equation}

There are other popular performance metrics, especially for classification problems, like AUC (area under the curve) and
ROC (receiver operating characteristics), which basically evaluate by weighing sensitivity and specificity.  In
cross-validation compared to single validation the initial data set is split into TRN and TST multiple times e.g. if the
ratio is 80:20 5 times, and each TRN-TST pair is evaluated individually. Sometimes it becomes necessary to use a third
subset - the validation data. Because hyperparameter tuning is performed with the TRN and TST sets, a third portion of
the data needs to be assessed to check whether the neural network is able to generalize on global data.

\textbf{L1 and L2 loss} \\
 
L1 and L2


\textbf{Dropout}


\subsection{On the nature of quantitative traits} \label{quan} According to the omnigenic model which is an extension of
the polygenic model proposed by \cite{boyle2017expanded} and thoroughly reviewed in \cite{timpson2018} all traits or
phenotypic values are influenced by a great number or all genes in the genome. Therefore resulting in traits following
certain gradual statistical distributions instead of being binned in classes or even binary. Intuitively this might be
contradicting with the foundation of modern Genetics - Mendel's three laws. That where derived from observations with
where mainly influenced by one locus. But staying with one of Mendel's examples the round or wrinkled surfaces of peas
\textit{Pisum sativum}, an assessment of a couple of thousands peas, would most likely inevitably lead to the conclusion
that form the ``roundest'' to the ``wrinkliest'' pea any gradual step between those is possible and observable. Mendel's
third law of independent segregation also only holds true under certain assumptions. The most simplest one being that
the traits under investigation have to be located on different linkage groups. Otherwise for the 7 traits used in
Mendel's initial studies would not have segregated independently. The odds of 7 randomly selected traits being on 7
different linkage groups are rather small, especially taking into account, that the genome of the \textit{P. sativum}
consists of only 7 chromosomes itself \cite{kalo2004}. Mendel probably new about traits not following its own laws, as
well as being aware of the quantitative nature of traits such as the constitution of surfaces of peas or the color of
petals. But being the pioneer of a then rather unexplored field of science, some of which big questions we fail to
satisfactory answer today, he did not have the resources or the knowledge to explain behavior's not ``mendeling'', that
were only able to be deciphered in later decades and centuries
based on his ground-breaking work. \\
Initially thought to be contradicting to Mendel's ideas Darwin proposed the concept's of evolution due to natural
selection which also introduce the idea of traits following a gradual distribution \cite{darwin1859}. This contrast led
to a long lasting debate in the scientific community in the early 1900s, between the Mendelians and the biometricians
who believed in the quantitative nature of continuous traits. This conflict has eventually been solved by Fisher's
fundamental work published in 1918 \cite{fisher1919xv}. His theories combined the then in all fields of science popular
research of distributions with genomics. He he mathematically proved that traits influenced by many genes, with
randomly-sampled alleles follow a continuous normal distribution in a population. While this combined the ideas of
Mendel and the biometricians it opened an other long debated question of effect size and the overall architecture of
complex traits. While in the theory of monogenic traits the effect size of the single gene on the trait is 1 or 100 \%
with an increasing number of genes influencing a complex traits the \textit{per sè} contribution of single gene has to
decrease with an increasing number of loci determining the value a given trait. In the 1990s it has been thought, that
complex traits are predominantly controlled from few genes with a large to medium effect size, while others had a
minimal influence
\cite{zhang2018esti}. \\
With the upcoming popularity of GWAS as the favored method to decipher genetic architectures of traits, or having
pioneered in human genetics in became clear that the majority of the effect sizes are tiny < 1 \% while there are very
few loci which have a moderate effect on the phenotypic variance of a population with around 10 \% or less
\cite{korte2013advantages}, \cite{stringer2011}. This nature of quantitative traits present great challenges to animal
\cite{goddard2009} and plant breeding \cite{wurschum2012}, in further improving crop or livestock performances, as well
complicating the decomposition of genomic causes for diseases like
schizophrenia or autism in human medicine \cite{de2014}, \cite{purcell2014}. \\
While the complex nature of the architecture of quantitative traits provide enough challenges as is, all traits will
also be influenced by the environment from which an individual originates. Therefore the distribution of trait values in
a given population can be expressed as the addition of the variances of its genetic and the environmental effects
\ref{eqn:PGE}.

\begin{equation}
  \sigma_{P} = \sigma_{G} + \sigma_{E}
  \label{eqn:PGE}
\end{equation}

The genomic and the environmental effects not only influence the phenotypic variance directly, but the environment also
has an influence on gene expression methylation of DNA bases etc. and therefore the equation \ref{eqn:PGE} needs to be
extend by the variance of the gene-environment interactions $\sigma_{GxE}$ \ref{eqn:PGGE} , \cite{lynch1998},
\cite{walsh2018}.
        
\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E} + \sigma_{GxE}
 \label{eqn:PGGE}
\end{equation}

Equation \ref{eqn:PGGE} shows the decomposition of the phenotypic variance, to thoroughly understand complex genetic
architectures of traits the genetic variance needs to be decomposed further in its additive, dominance and epistatic
components \ref{eqn:GAD}

\begin{equation}
  \sigma_{G} = \sigma_{A} + \sigma_{D} + \sigma_{I}
  \label{eqn:GAD}
\end{equation}

The additive effects are caused by single, for this model mostly homozygous, loci while the variance caused by dominance
effects, is caused by heterozygous loci and their resulting interactions being full-, over- , co- or underdominant. And
lastly the interaction effects that are a result of two or more genes only having an impact if the involved genes
co-occur in a certain state. The resulting variance is commonly known
as gene-gene interactions and/or epistasis \cite{falconer1996}. \\
Since possible interactions in a genome can happen between additive or dominant or a combination of those loci. The
variance due to interaction effects $\sigma_{I}$ can be further dissembled in the variance resulting from
additive-additive $\sigma_{AA}$ dominant-dominant $\sigma{DD}$ and additive-dominant $sigma{AD}$ terms as represented in
equation \ref{eqn:IAA}.


\begin{equation}
  \sigma_{I} = \sigma_{AxA} + \sigma_{DxD} + \sigma_{AxD}
 \label{eqn:IAA}
\end{equation}

Knowledge of the variance components involved in the expression of a trait in population, lead up to the estimation of
the total influence of all genetic variances and the environmental variance one the phenotypic distribution. This
concept if called heritability. The heritability of a trait $H^2$ accounts for the proportion of the phenotypic variance
controlled by the total genetic variance as shown in equation \ref{eqn:h2G}. This is also referred to as broad sense
heritability, because all genetic effects including additive, dominance and epistatic effects are included
\cite{brooker1999genetics}.


\begin{equation}
 H^2 = \frac{\sigma_{A} + \sigma_{D} + \sigma_{I}}{\sigma_{P}}
 \label{eqn:h2G}
\end{equation}

The concept of narrow-sense heritability \ref{eqn:h2a} is similar to the broad-sense heritability, but only the additive
genetic effects are included in the genetic part of the equation. This differentiation is import for natural and
artificial selection and thus is commonly used in evolutionary genomics and breeding. Because in diploid species each
parent only passes down on a single a allele of a give locus. Dominance effects or interaction effects are not commonly
inherited from one parent. Therefore the it is mainly the additive genetic effects of a parent that influences its
offspring. While the dominance and epistatic variances are controlled by the combination of the parents
\cite{falconer1996}, \cite{walsh2018}.

\begin{equation}
 h^2 = \frac{\sigma_{A}}{\sigma_{P}}
 \label{eqn:h2a}
\end{equation}



\subsection{Artificial selection in plant and animal breeding in the genomics era}

Genomic prediction has been applied to almost all relevant crop and model species. Including: \textit{A. thaliana}
\cite{hu2015}; \cite{shen2013novel}. Alfalfa (\textit{Medicago sativa}) \cite{li2012applied};
\cite{annicchiarico2015accuracy}; \cite{li2015genomical}; \cite{biazzi2017genome}; \cite{hawkins2018recent}. Barley
\cite{neyhart2019}; \cite{oakey2016}; \cite{zhong2009factors}. Cassava (\textit{Manihot esculenta}) \cite{elias2018};
\cite{elias2018improving}. Cauliflower (\textit{Brassica olearacea spp}) \cite{thorwarth2018genomic}. Cotton
(\textit{Gossiypium spp.} \cite{gapare2018}. Maze (\textit{Zea mays}) \cite{moeinizade2019};
\cite{allier2019usefulness}; \cite{brauner2018genomic}; \cite{schrag2018beyond}; \cite{schopp2017genomic};
\cite{e2017genomic}; \cite{schopp2017accuracy}; \cite{kadam2016genomic}; \cite{bustos2016improvement};
\cite{montesinos2015threshold}; \cite{owens2014foundation}; \cite{lehermeier2014usefulness}; \cite{technow2014genome};
\cite{peiffer2014genetic}; \cite{riedelsheimer2013genomic}; \cite{guo2013accuracy}; \cite{technow2013genomic};
\cite{windhausen2012}; \cite{rincent2012}. Potato (\textit{Solanum tuberosum}); \cite{enciso2018genomic};
\cite{Endelman2018pot}. Rape seed (\textit{Brassica naps}) \cite{wurschum2014potential}; \cite{jan2016genomic};
\cite{luo2017genomic}; \cite{werner2018effective}; \cite{snowdon2012potential}; \cite{qian2014sub}. Rice (\textit{Oryza
  sativa}) \cite{Momen2019}; \cite{BenHassen2018}; \cite{Xu2013rice}; \cite{Grenier2015}. Rye (\textit{Secale cerale})
\cite{auinger2016model}; \cite{bernal2014importance}; \cite{wang2014accuracy}; \cite{bernal2017genomic};
\cite{marulanda2016optimum}. Soybean (\textit{Glycine max}) \cite{Stewart_Brown_2019}; \cite{Jarquin_2016};
\cite{Xavier_2016}. Switchgrass (\textit{Panicum virgatum}) \cite{Poudel_2019}; \cite{Ramstein_2019};
\cite{Ramstein_2016}. Wheat (\textit{Triticum aestivum}) \cite{Cuevas_2019}; \cite{Howard_2019}; \cite{Krause_2019};
\cite{Rincent_2018}; \cite{Norman_2018}; \cite{Belamkar_2018}; \cite{Ovenden_2018}; \cite{Sukumaran_2016};
\cite{Bustos_Korts_2016}; \cite{Gianola_2016_wheat}; \cite{Crossa_2016_wheat}; \cite{Thavamanikumar_2015};
\cite{Lopez_Cruz_2015}. As well as various tree species \cite{deAlmeidaFilho2019}; \cite{Rincent_2018};
\cite{Kainer_2018}; \cite{Ratcliffe_2017}; \cite{GamalElDien_2016}; \cite{Kumar_2015}; \cite{Jaramillo_Correa_2014};
\cite{Zapata_Valenzuela_2013}; \cite{Holliday_2012}; \cite{Resende_2012}.


\subsection{Genomic selection using artificial neural networks }
Genomic selection (GS) has been successfully applied in animal \cite{gianola2015one}, \cite{hayes2010genome} and plant
breeding \cite{crossa2010}, \cite{desta2014genomic}, \cite{heffner2010plant}, \cite{crossa2017genomic} as well as in
medical applications, since it was first reported \cite{hayes2001}. Since then the repertoire of methods for predicting
phenotypic values has increased rapidly e.g.\cite{dlc2009}, \cite{habier2011}, \cite{gianola2013} ,
\cite{crossa2017}. The most commonly applied methods include GULP and a set of related algorithms known as the bayesian
alphabet \cite{gianola2009}. Genomic prediction in general has repeatedly been shown to outperform pedigree-based
methods \cite{crossa2010}, \cite{albrecht2011} and is nowadays used in many plant and animal breeding schemes. It has
also been shown that using whole-genome information is superior to using only feature-selected markers with known QTLs
for a given trait \cite{bernardo2007}, \cite{heffner2011} in some cases. A more recent study \cite{azodi2019} compared
11 different genomic prediction algorithms with a variety of data sets and found contradicting results, indicating that
feature selection can be usefull in some cases the when the whole genome regression is performed by neural nets 1 While
every new method is a valuable addition to the tool-kits for genomic selection, some fundamental problems remain
unsolved, of which the n>>p problematic stands out. Usually in genomic selection settings the size of the training
population (TRN) with n phenotypes is substantially smaller than the number of markers (p)
\cite{fan2014challenges}. Making the number of features immensely large, even when SNP-SNP interactions are not
considered. Furthermore each marker is treated as an independent observation neglecting collinearity and linkage
disequilibrium (LD). Further difficulties arise through non-additive, epistatic and dominance marker effects. The main
problem with epistasis issue quantitative genetics is the almost infinite amount of different marker combinations, that
cannot be represented within the size of TRN in the thousands, the same problems arises for example in GWA studies
\cite{korte2013advantages}. With already large p the number of possible additive SNP-SNP interactions potentiates to
$p^{(p-1)}$. Methods that attempt to overcome those issues are EG-BLUP, using an enhanced epistatic kinship matrix and
reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015}, \cite{martini2017genomic}.

In the past 10 years, due to increasing availability of high performance computational hardware with decreasing costs
and parallel development of free easy-to-use software, most prominent being googles library TensorFlow \cite{TF2016} and
Keras \cite{keras2015}, machine learning (ML) has experienced a renaissance. ML is a set of methods and algorithms used
widely for regression and classification problems. popular among those are e.g. support vector machines, multi-layer
perceptrons (MLP) and convolutional neural networks. The machine learning mimics the architecture of neural networks and
are therefore commonly referred to as artificial neural networks (ANN). Those algorithms have widely been applied in
many biological fields \cite{min2017deep} , \cite{lan2018survey}, \cite{mamoshina2016applications},
\cite{angermueller2016} , \cite{webb2018deep}, \cite{rampasek2016tensorflow}.

A variety of studies assessed the usability of ML in genomic prediction \cite{gonzalez2018applications},
\cite{gonza2016}, \cite{ogutu2011comparison}, \cite{montesinos2019benchmarking}, \cite{grinberg2018evaluation} ,
\cite{cuevas2019deep}, \cite{montesinos2019new}, \cite{ma2017deepgs}, \cite{qiu2016application}, \cite{gonza2012}
\cite{li2018genomic}. Through all those studies the common denominator is that there is no such thing as a gold standard
for genomic prediction. No single algorithm was able to outperform all the others tested in a single of those studies,
let alone in all. While the generally aptitude of ML for genomic selection has been repeatedly shown, how no evidence
exists that neural networks can outperform or in many cases perform on that same level as mixed-model approaches as
GBLUP \cite{hayes2001}. While in other fields like image classification neural networks have up to 100s of hidden layers
\cite{he2016deep} the commonly used fully-connected networks in genomic prediction of 1 - 3 hidden layers. With 1 layer
networks often being the most successful among those. Contradicting to the idea behind machine learning in genomic
selection 1 hidden layer networks will be inapt to capture interactions between loci and thus only account for additive
effects. As shown in \cite{azodi2019} convolutional networks perform worse than fully-connected networks in genomic
selection, which again is contradicting to other fields where convolutional layers are applied successfully, e.g natural
language processing \cite{dos2014deep} or medical image analysis \cite{litjens2017survey}. Instead of using
convolutional layers and fully-connected layers only, as show in Pook et al 2019, we also propose to use
locally-connected layer in combination with fully-connected layers. While CL and LCL are closely related they have a
significant difference. While in CL weights are shared between neurons in LCLs each neuron as its own weight. This leads
to a reduced number of parameters to be trained in the following FCLs, and should therefore theoretically lead to a
decrease in overfitting a common problem in machine learning. To evaluate the results of Pook et al. 2019 accomplished
with simulated data we used the data sets generated in the scope of the 1001 genome project of \textit{Arabidopsis
  thaliana} \cite{1001genome}



\section{Proof of concept for ANN-based genomic selection} \label{POC}

Having established the quantitative architecture of traits in section \ref{quan} an the basics of machine learning and
neural nets in section \ref{introml}, that knowledge can be used to provide a proof of concept that neural networks are
a candidate for GP.  Table \ref{tab:simmarker} provides also the possible genotypes that can be derived by two
bi-allelic markers $G_1 \dots G_4$ on a fictional haploid organism. In this simulation the effect sizes for each marker
$\beta_1$ and $\beta_2$ are constant with a value of 1.

\begin{table}[H]
  \caption{Simple simulated phenotypes and genotypes for genomic prediction with genotypes $G_1 \dots G_4$, $M_1$ and
    $M_2$ and phenotypes based on additive effects or $and$, $or$, $xor$ logic gates.}
\label{tab:simmarker}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $Y_{ADD}$ & $Y_{AND}$ & $Y_{OR}$ & $Y_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0 & 0 & 0 & 0 \\
  $G_2$ & 0 & 1 & 1 & 0 & 1 & 1 \\
  $G_3$ & 1 & 0 & 1 & 0 & 1 & 1 \\
  $G_4$ & 1 & 1 & 2 & 1 & 1 & 0 \\
  \bottomrule
\end{tabular}
\end{table}

The four phenotypes $Y_{ADD}$, $Y_{AND}$, $Y_{OR}$ and $Y_{XOR}$ , which were derived from their respective marker
effects. $Y_{ADD}$ is a phenotype with purely additive effects. So in the nomenclature introduced in chapter \ref{quan}
$\sigma_A = \sigma_G$ and $\sigma_{I} = 0$. Since the hypothetical organism is haploid there are dominance effects to be
accounted for $\sigma_D = 0$. Since all the genetic effects are caused by additive effects and there are now
environmental effects $\sigma_E$, the narrow sense heritability $h^2$ - equation \ref{eqn:h2a} - and the broad sense
heritability $H^2$ - equation \ref{eqn:h2G} - are equally 1. The other three phenotypes are base on epistatic effects
$\sigma_I$ generated by passing the markers $M_1$ and $M_2$ through their respective logic gates. This theoretically in
results in $h^2 = 0$ and $H^2 = 1$, because there are no additive effects. For $y_{AND}$ however $h \approx 0.5$,
because there is a correlation between $Y_{ADD}$ and $Y_{AND}$. In practical applications this allows methods like
GBLUP, designed to account for additive genetic effects to capture some of the epistatic effects of $\sigma_I$
\cite{vieira2017assessing}.

According to chapter \ref{introml} a single perceptron would fail to solve $xor$ gates. While a network with multiple
nodes and layers should be able to overcome that deficit. A relatively simple neural network with two fully-connected
hidden layers with 10 and 5 nodes, was trained for the prediction of each phenotypes. To keep the simulation as
possible, no regularization parameters, dropout etc. was included. The activation function was ReLU (\ref{eqn:relu})
with an Adam optimizer. The results of the prediction are shown in table \ref{tab:simgpres}.


\begin{table}[H]
\caption{Results of genomic prediction from phenotypes and genotypes in table \ref{tab:simmarker}}
\label{tab:simgpres}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $\hat{Y}_{ADD}$ & $\hat{Y}_{AND}$ & $\hat{Y}_{OR}$ & $\hat{Y}_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0.01 & 0.00 & 0.00 & 0.01 \\
  $G_2$ & 0 & 1 & 0.99 & 0.01 & 0.99 & 0.98 \\
  $G_3$ & 1 & 0 & 0.99 & 0.00 & 0.99 & 1.01 \\
  $G_4$ & 1 & 1 & 1.99 & 0.98 & 1.01 & 0.02 \\
  \bottomrule
\end{tabular}
\end{table}


\section{Material}
\subsection{DH populations derived from MAZE landraces}
\subsection{A. thaliana}
\section{Methods}
\subsection{ANN}
\subsection{GBLUP}


\section{Results}
\section{Discussion}
