
\chapter{Genomic prediction of phenotypic values of quantitative traits using Artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------



\section{Introduction}
\subsection{A brief history of machine learning }

While machine learning, neural networks, deep learning became essential tools for many applications in more recent years,
their mathematical principals date back to the early 1950s and 1960s. Figure \ref{fig:perceptron} schematically  show the
basic perceptron model as proposed by Rosenblatt, which was designed to mimic the information flow in biological nervous systems
\cite{rosenblatt1961}

\begin{figure}[th]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png}
\decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt}
\label{fig:perceptron}
\end{figure}

This basic perceptron, which contrary to perceptrons used nowadays does not have an activation function, takes n binary
inputs $x_1 , x_2 .... x_n$ and produces a single, likewise binary, output $y$ after being processed by the perceptron or neuron.
To achieve this Rosenblatt introduced the concept of weights which indicated a certain relative importance to the outcome of the
output. $w_1 , w_2 ... w_n$. The output $y$ is determined by the weighted sum of the weights and biases $\sum_i w_ix_i $. If a certain
threshold value is met  the neuron is either activated and outputs 1 or not and outputs 0. This is algebraically represented in
\ref{eqn:weights}
\begin{subequations}
  \begin{align}
    0  = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
    1 =  \mbox{if } \sum_i^n w_i x_i - \theta > 0
  \end{align}
  \label{eqn:weights}
\end{subequations}

Next to the weights $w_n$ and the inputs $x_n$ a third term $\theta$ is introduced in equation \ref{eqn:weights} which represents
the activation threshold in per definition is negative. A single perceptron is a linear classifier and can only be trained on
linearly seperable functions and can used as shown by \cite{rosenblatt1961} to solve simple logical operations as AND, OR and not.
The simple perceptron fails, due to non-linearity, to perfrom XOR operations as shown by \cite{marvin1969}. This discoverey let to
a near stillstance in the research of artificial neural networks in the 1970s.



\subsection{Of the nature of quantitative traits}
According to the omnigenic model thouroughly reviewd in \cite{timpson2018} all traits or phenotypic values are influenced
by a great number or all genes in the genome. Therefore resulting in traits following certain gradual statistical distributions
insetead of being binned in classes or even binary. Intiutivly this might be contradicting with the foundation of modern Genetics -
Mendel's three laws. That where derived from observations with where mainly influenced by one locus. But staying with one of
Mendel's exampels the round or wrinkeled surfaces of  peas \textit{Pisum sativum}, an assessment of a couple of thousands peas, would
most likely inevitably
lead to the conclusion that form the ``roundest'' to the ``wrinkliest'' pea any gradual step between those is possible and obersable.
Mendel's third law of independent segregation also only holds true under certain assumptions. The most simplest one being that the traits
under investigation have to be located on different linkage groups. Otherwise for the 7 traits used in Mendel's inital studies would not have
segregated independently. The odds of 7 randomly selected traits being on 7 different linkage groups are rather small, espacially taking
into account, that the genome of the \textit{P. sativum} consists of only 7 chromosomes itself \cite{kalo2004}. Mendel probably new about
traits not following its own law's,
as well as being aware of the qunatitave nature of traits suchs as the constitution of surfaces of peas or the color of petals. But being the
pioneer of a then rather unexplored field of science, some of whose big questions we fail to satisfactory answer today, he did not have the
resources or the knowledge to exlpain behavior's not ``mendeling'', that were only able to be deciphered in later decades and centueries
based on his ground-breaking work. 



\subsection{Genomic selection using artificial neural networks }
Genomic selection (GS) has been successfully applied in animal \cite{gianola2015one}, \cite{hayes2010genome}
and plant breeding \cite{crossa2010}, \cite{desta2014genomic}, \cite{heffner2010plant}, \cite{crossa2017genomic}
as well as in medical applications, since it was first reported  \cite{hayes2001}. Since then the repertoire of
methods for predicting phenotypic values has increased rapidly e.g.\cite{dlc2009}, \cite{habier2011}, \cite{gianola2013} ,
\cite{crossa2017}. The most commonly applied methods include GBLUP and a set of related algorithms known as the bayesian alphabet
\cite{gianola2009}. 
Genomic prediction in general has repeatedly been shown to outperform pedigree-based methods \cite{crossa2010},
\cite{albrecht2011} and is nowadays used in many plant and animal breeding schemes. 
It has also been shown that using whole-genome information is superior to using only feature-selected markers
with known QTLs for a given trait \cite{bernardo2007}, \cite{heffner2011} in some cases. A more recent study
\cite{azodi2019} compared 11 different genomic prediction algorithms with a variety of data sets and found
contradicting results, indicating that feature selection can be usefull in some cases the when the whole
genome regression is performed by neural nets 
While every new method is a valuable addition to the tool-kits for genomic selection, some fundamental
problems remain unsolved, of which the n>>p problematic stands out. Usually in genomic selection settings
the size of the training population (TRN) with n phenotypes is substantially smaller than the number of
markers (p) \cite{fan2014challenges}. Making the number of features immensely large,  even when SNP-SNP
interactions are not considered.  Furthermore each marker is treated as an independent observation
neglecting collinearity and linkage disequilibrium (LD). 
Further difficulties arise through non-additive, epistatic and dominance marker effects. The main problem
with epistasis issue quantitative genetics is the almost infinite amount of different marker combinations,
that cannot be represented within the size of TRN in the thousands, the same problems arises for example
in GWA studies \cite{korte2013}. With already large p the number of possible additive SNP-SNP interactions
potentiates to $p^{(p-1)}$. Methods that attempt to overcome those issues are EG-BLUP, using an enhanced
epistatic kinship matrix and reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015}, \cite{martini2017genomic}.

In the past 10 years, due to increasing availability of high performance computational hardware with decreasing
costs and parallel development of free easy-to-use software, most prominent being googles library TensorFlow
\cite{TF2016} and Keras \cite{keras2015}, machine learning (ML) has experienced a renaissance.
ML is a set of methods and algorithms used widely for regression and classification problems. popular
among those are e.g. support vector machines, multi-layer perceptrons (MLP) and convolutional neural
networks. The machine learning mimics the architecture of neural networks and are therefore commonly
referred to as artificial neural networks (ANN). Those algorithms have widely been applied in many
biological fields \cite{min2017deep} , \cite{lan2018survey}, \cite{mamoshina2016applications},
\cite{angermueller2016} , \cite{webb2018deep}, \cite{rampasek2016tensorflow}. 

A variety of studies assessed the usability of ML in genomic prediction \cite{gonzalez2018applications},
\cite{gonza2016}, \cite{ogutu2011comparison}, \cite{montesinos2019benchmarking}, \cite{grinberg2018evaluation}
, \cite{cuevas2019deep}, \cite{montesinos2019new}, \cite{ma2017deepgs}, \cite{qiu2016application},
\cite{gonza2012} \cite{li2018genomic}.
Through all those studies the common denominator is that there is no such thing as a gold standard
for genomic prediction. No single algorithm was able to outperform all the others tested in a single
of those studies, let alone in all. While the generally aptitude of ML for genomic selection has been
repeatedly shown, how no evidence exists that neural networks can outperform or in many cases perform
on that same level as mixed-model approaches as GBLUP \cite{hayes2001prediction}. While in other fields
like image classification neural networks have up to 100s of hidden layers \cite{he2016deep} the commonly
used fully-connected networks in genomic prediction of 1 - 3 hidden layers. 
With 1 layer networks often being the most successful among those. Contradicting to the idea behind machine
learning in genomic selection 1 hidden layer networks will be inapt to capture interactions between loci and
thus only account for additive effects. As shown in \cite{azodi2019} convolutional networks perform worse
than fully-connected networks in genomic selection, which again is contradicting to other fields where
convolutional layers are applied successfully, e.g natural language processing \cite{dos2014deep} or
medical image analysis  \cite{litjens2017survey}. Instead of using convolutional layers and fully-connected
layers only, as show in Pook et al 2019, we also propose to use locally-connected layer in combination with
fully-connected layers. While CL and LCL are closely related they have a significant difference. While in CL
weights are shared between neurons in LCLs each neuron as its own weight. This leads to a reduced number of
parameters to be trained in the following FCLs, and should therefore theoretically lead to a decrease in
overfitting a common problem in machine learning. 
To evaluate the results of Pook et al. 2019 accomplished with simulated data we used the data sets
generated in the scope of the 1001 genome project of \textit{Arabidopsis thaliana} \cite{1001genome}



\section{Proof of concept}



\section{Material}
\section{Methods}
\section{Results}
\section{Discussion}

