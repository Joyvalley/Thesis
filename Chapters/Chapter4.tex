\chapter{Genomic prediction of phenotypic values of quantitative traits using Artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------



\section{Introduction}
\subsection{A brief history of machine learning }

While machine learning, neural networks, deep learning became essential tools for many applications in more
recent years, their mathematical principals date back to the early 1950s and 1960s. Figure
\ref{fig:perceptron} schematically show the basic perceptron model as proposed by Rosenblatt, which was
designed to mimic the information flow in biological nervous systems \cite{rosenblatt1961}

\begin{figure}[th]
  \centering \includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png} \decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt}
\label{fig:perceptron}
\end{figure}

This basic perceptron, which contrary to perceptrons used nowadays does not have an activation function, takes
n binary inputs $x_1 , x_2 .... x_n$ and produces a single, likewise binary, output $y$ after being processed
by the perceptron or neuron.  To achieve this Rosenblatt introduced the concept of weights which indicated a
certain relative importance to the outcome of the output. $w_1 , w_2 ... w_n$. The output $y$ is determined by
the weighted sum of the weights and biases $\sum_i w_ix_i $. If a certain threshold value is met the neuron is
either activated and outputs 1 or not and outputs 0. This is algebraically represented in \ref{eqn:weights}
\begin{subequations}
 \begin{align}
 0 = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
 1 = \mbox{if } \sum_i^n w_i x_i - \theta > 0
 \end{align}
 \label{eqn:weights}
\end{subequations}

Next to the weights $w_n$ and the inputs $x_n$ a third term $\theta$ is introduced in equation
\ref{eqn:weights} which represents the activation threshold in per definition is negative. A single perceptron
is a linear classifier and can only be trained on linearly separable functions and can used as shown by
\cite{rosenblatt1961} to solve simple logical operations as AND, OR and not.  The simple perceptron fails, due
to non-linearity, to perform XOR operations as shown by \cite{marvin1969}. This discovery let to a near still
stance in the research of artificial neural networks in the 1970s. This time period as now often referred to
as the first AI-winter. Another reason that massively hindered the applications and research of machine
learning during that time, was the compared to
modern times incredibly small amount of computational power available \cite{nguyen1990truck}. \\
More complex decision making, like solving XOR problems, requires more complex structures than a single
perceptron. Continuing the trend of mimicking human neural networks, multiple artificial neurons are stacked
into layers and these layers, are connected to each other allowing communication between the many perceptrons
in a such generated network. Figure \ref{fig:nn} shows schematically the basic structure of such a network,
now container three types of layers. (i) the input layer, (ii) one or more hidden layers and (iii) one output
layer, which in this case only consists of one neuron.

\begin{figure}[H]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/neuralnet}
\decoRule
\caption[Schematic layout of a simple multi-layer perceptron]{Schematic layout of a simple multi-layer perceptron}
\label{fig:nn}
\end{figure}

In the sample layout of figure \ref{fig:nn} the neurons in the first column weigh the inputs and pass those
the the neurons on the second layer. In this case all neurons on the first layer or connected to all neurons
on the second layer, such layers are referred to fully-connected layers (FLC) , and their resulting networks
are often called multi-layer perceptrons (MLP). This architecture enables the network to perform more complex
calculations and result in more abstract decisions than single neurons or single layer architectures.

The neurons discussed so far are only capable of outputting binary results. Either 0 or 1, depending on the
threshold values being met or not.  For more complex estimations it is desirable that small changes in the
input also result in small changes of the output. This requirement can not be met with binary
outputs. Activation functions for a given node provides rules for the output in accordance to the inputs
\cite{vzilinskas2006practical}.

\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.85\textwidth]{Figures/activation}
\decoRule
\caption[Popular activation functions for neural networks]{Popular activation functions used in neural networks.
  \textbf{A} Binary step activation function.
  \textbf{B} Identity activation function.
  \textbf{C} Sigmoid or logistic activation function.
  \textbf{D} tangens hyperbolicus activation function.
  \textbf{E} rectified linear units activation function .
  \textbf{F} SoftPlus activation function.}
\label{fig:activation}
\end{figure}

Figure \ref{fig:activation} \textbf{A} shows six of the most commonly used activation functions
\cite{warner1996understanding}.  The simplest one was introduced, is the binary step activation function
equation \ref{eqn:binary}, which properties have been discussed along the perceptron model. All other
activation produce continuous outputs from any given input. Basically any mathematical function can serve as
an activation function in neural nets, starting with a simple identity function \ref{eqn:ident} ,
\ref{fig:activation} \textbf{B}.  Sigmoid figure \ref{fig:activation} \textbf{C}, equation \ref{eqn:sigmoid}
and tanh figure \ref{fig:activation} \textbf{D}, equation \ref{eqn:tanh}, when $x \rightarrow \infty$ or
$x \rightarrow -\infty$ they have similar properties to the binary function, but produce continous output
around 0.


\begin{equation}
  f(x)= \sigma(x) = \left\{
  \begin{array}{ll}
    0 \; for \; x < 0 \\ 
    1 \; for \; x \geq 0
  \end{array}
\right .
\label{eqn:binary}
\end{equation}


\begin{equation}
  f(x) = \sigma(x) = x
  \label{eqn:ident}
\end{equation}

              
\begin{equation}
  f(x) = \sigma(x) =  \frac{1}{1+e^{-x}} 
  \label{eqn:sigmoid}
\end{equation}

\begin{equation}
  f(x) = \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \label{eqn:tanh}
\end{equation}


\begin{equation}
  f(x)= \sigma(x) = \left\{
  \begin{array}{ll}
    0 \; for \; x < 0 \\ 
    x \; for \; x \geq 0
  \end{array}
\right .
\label{eqn:relu}
\end{equation}


\begin{equation}
  f(x) = ln(1+e^x)
  \label{eqn:softplus}
\end{equation}


ReLU (equation \ref{eqn:relu}) and the softplus (equation \ref{eqn:softplus}) share similar properties as
well, the latter one being a smoothed version of ReLU. Rectfiers as activation functions have been introduced
in 2000s \cite{hahnloser2000digital} and have since then overtaken all others as the most popular activations
functions in neural networks and deep learning today \cite{lecun2015deep} and they have proven to be superior
in deep-learning algorithms that sigmoid or logistic functions. One of the advandtages leading to the
superiority of ReLUs is that with randomly initialized weights only half of the ReLU neurons are activated,
compared to tanh and sigmoid activation \cite{glorot2011deep}. All activation functions shown in figure
\ref{fig:equation}, but the binary step function, share one common property: a small change of the input
weight will result in small changes in the output, while a small change of the input for the binary step
function leads to either no or a complete change of the output.  This property is, as described below, is an
important prerequisite for networks being able to learn.




Backpropagation \cite{rumelhart1988learning}




\subsection{On the nature of quantitative traits}
According to the omnigenic model which is an extension of the polygenic model proposed by
\cite{boyle2017expanded} and thoroughly reviewed in \cite{timpson2018} all traits or phenotypic values are
influenced by a great number or all genes in the genome. Therefore resulting in traits following certain
gradual statistical distributions instead of being binned in classes or even binary. Intuitively this might be
contradicting with the foundation of modern Genetics - Mendel's three laws. That where derived from
observations with where mainly influenced by one locus. But staying with one of Mendel's examples the round or
wrinkled surfaces of peas \textit{Pisum sativum}, an assessment of a couple of thousands peas, would most
likely inevitably lead to the conclusion that form the ``roundest'' to the ``wrinkliest'' pea any gradual step
between those is possible and observable.  Mendel's third law of independent segregation also only holds true
under certain assumptions. The most simplest one being that the traits under investigation have to be located
on different linkage groups. Otherwise for the 7 traits used in Mendel's initial studies would not have
segregated independently. The odds of 7 randomly selected traits being on 7 different linkage groups are
rather small, especially taking into account, that the genome of the \textit{P. sativum} consists of only 7
chromosomes itself \cite{kalo2004}. Mendel probably new about traits not following its own laws, as well as
being aware of the quantitative nature of traits such as the constitution of surfaces of peas or the color of
petals. But being the pioneer of a then rather unexplored field of science, some of which big questions we
fail to satisfactory answer today, he did not have the resources or the knowledge to explain behavior's not
``mendeling'', that were only able to be deciphered in later decades and centuries
based on his ground-breaking work. \\
Initially thought to be contradicting to Mendel's ideas Darwin proposed the concept's of evolution due to
natural selection which also introduce the idea of traits following a gradual distribution
\cite{darwin1859}. This contrast led to a long lasting debate in the scientific community in the early 1900s,
between the Mendelians and the biometricians who believed in the quantitative nature of continuous
traits. This conflict has eventually been solved by Fisher's fundamental work published in 1918
\cite{fisher1919xv}.  His theories combined the then in all fields of science popular research of
distributions with genomics. He he mathematically proved that traits influenced by many genes, with
randomly-sampled alleles follow a continuous normal distribution in a population. While this combined the
ideas of Mendel and the biometricians it opened an other long debated question of effect size and the overall
architecture of complex traits. While in the theory of monogenic traits the effect size of the single gene on
the trait is 1 or 100 \% with an increasing number of genes influencing a complex traits the \textit{per sè}
contribution of single gene has to decrease with an increasing number of loci determining the value a given
trait. In the 1990s it has been thought, that complex traits are predominantly controlled from few genes with
a large to medium effect size, while others had a minimal influence
\cite{zhang2018esti}. \\
With the upcoming popularity of GWAS as the favored method to decipher genetic architectures of traits, or
having pioneered in human genetics in became clear that the majority of the effect sizes are tiny < 1 \% while
there are very few loci which have a moderate effect on the phenotypic variance of a population with around 10
\% or less \cite{korte2013advantages}, \cite{stringer2011}.  This nature of quantitative traits present great
challenges to animal \cite{goddard2009} and plant breeding \cite{wurschum2012}, in further improving crop or
livestock performances, as well complicating the decomposition of genomic causes for diseases like
schizophrenia or autism in human medicine \cite{de2014}, \cite{purcell2014}. \\
While the complex nature of the architecture of quantitative traits provide enough challenges as is, all
traits will also be influenced by the environment from which an individual originates.  Therefore the
distribution of trait values in a given population can be expressed as the addition of the variances of its
genetic and the environmental effects \ref{eqn:PGE}.

\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E}
 \label{eqn:PGE}
\end{equation}

The genomic and the environmental effects not only influence the phenotypic variance directly, but the
environment also has an influence on gene expression methylation of DNA bases etc. and therefore the equation
\ref{eqn:PGE} needs to be extend by the variance of the gene- environment interactions $\sigma_{GxE}$
\ref{eqn:PGEGE} , \cite{lynch1998}, \cite{walsh2018}.
                
\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E} + \sigma_{GxE}
 \label{eqn:PGGE}
\end{equation}


Equation \ref{eqn:PGGE} shows the decomposition of the phenotypic variance, to thoroughly understand complex
genetic architectures of traits the genetic variance needs to be decomposed further in its additive, dominance
and epistatic components \ref{eqn:GAD}

\begin{equation}
 \sigma_{G} = \sigma_{A} + \sigma_{D} + \sigma_{I}
 \label{eqn:GAD}
\end{equation}

The additive effects are caused by single, for this model mostly homozygous, loci while the variance caused by
dominance effects, is caused by heterozygous loci and their resulting interactions being full-, over- , co- or
underdominant. And lastly the interaction effects that are a result of two or more genes only having an impact
if the involved genes co-occur in a certain state. The resulting variance is commonly known
as gene-gene interactions and/or epistasis \cite{falconer1996}. \\
Since possible interactions in a genome can happen between additive or dominant or a combination of those
loci. The variance due to interaction effects $\sigma_{I}$ can be further dissembled in the variance resulting
from additive-additive $\sigma_{AA}$ dominant-dominant $\sigma{DD}$ and additive-dominant $sigma{AD}$ terms as
represented in equation \ref{eqn:IAA}.


\begin{equation}
 \sigma_{I} = \sigma_{AxA} + \sigma_{DxD} + \sigma_{AxD}
 \label{eqn:IAA}
\end{equation}

Knowledge of the variance components involved in the expression of a trait in population, lead up to the
estimation of the total influence of all genetic variances and the environmental variance one the phenotypic
distribution. This concept if called heritability.  The heritability of a trait $H^2$ accounts for the
proportion of the phenotypic variance controlled by the total genetic variance as shown in equation
\ref{eqn:h2G}. This is also referred to as broad sense heritability, because all genetic effects including
additive, dominance and epistatic effects are included \cite{brooker1999genetics}.


\begin{equation}
 H^2 = \frac{\sigma_{A} + \sigma_{D} + \sigma_{I}}{\sigma_{P}}
 \label{eqn:h2G}
\end{equation}

The concept of narrow-sense heritability \ref{eqn:h2a} is similar to the broad-sense heritability, but only
the additive genetic effects are included in the genetic part of the equation. This differentiation is import
for natural and artificial selection and thus is commonly used in evolutionary genomics and breeding. Because
in diploid species each parent only passes down on a single a allele of a give locus. Dominance effects or
interaction effects are not commonly inherited from one parent. Therefore the it is mainly the additive
genetic effects of a parent that influences its offspring. While the dominance and epistatic variances are
controlled by the combination of the parents \cite{falconer1996}, \cite{walsh2018}.



\begin{equation}
 h^2 = \frac{\sigma_{A}}{\sigma_{P}}
 \label{eqn:h2a}
\end{equation}

The 


\subsection{Artificial selection in plant and animal breeding in the genomics era}

Genomic prediction has been applied to almost all relevant crop and model species. Including:
\textit{A. thaliana} \cite{hu2015}, \cite{shen2013novel}, alfalfa (\textit{Medicago sativa})
\cite{li2012applied}; \cite{annicchiarico2015accuracy}, \cite{li2015genomical}, \cite{biazzi2017genome},
\cite{hawkins2018recent}; barley \cite{neyhart2019}, \cite{oakey2016} , \cite{zhong2009factors} ; cassava
(\textit{Manihot esculenta}) \cite{elias2018}, \cite{elias2018improving} ; cauliflower (\textit{Brassica
  olearacea spp}) \cite{thorwarth2018genomic} ; cotton (\textit{Gossiypium spp.} \cite{gapare2018} ; maze
(\textit{Zea mays}) \cite{moeinizade2019} , \cite{allier2019usefulness} , \cite{brauner2018genomic},
\cite{schrag2018beyond}, \cite{schopp2017genomic}, \cite{e2017genomic}, \cite{schopp2017accuracy},
\cite{kadam2016genomic}, \cite{bustos2016improvement}, \cite{montesinos2015threshold},
\cite{owens2014foundation}, \cite{lehermeier2014usefulness}, \cite{technow2014genome},
\cite{peiffer2014genetic} , \cite{riedelsheimer2013genomic}, \cite{guo2013accuracy},
\cite{technow2013genomic}, \cite{windhausen2012} , \cite{rincent2012} ; potato (\textit{Solanum tuberosum}),
\cite{enciso2018genomic}, \cite{Endelman2018pot} ; rape seed (\textit{Brassica naps})
. \cite{wurschum2014potential}, \cite{jan2016genomic}, \cite{luo2017genomic}, \cite{werner2018effective},
\cite{snowdon2012potential}, \cite{qian2014sub} ; rice (\textit{Oryza sativa}) \cite{Momen2019},
\cite{BenHassen2018}, \cite{Xu2013rice}, \cite{Grenier2015} ; rye (\textit{Secale cerale})
\cite{auinger2016model}, \cite{bernal2014importance}, \cite{wang2014accuracy}, \cite{bernal2017genomic},
\cite{marulanda2016optimum}; soybean (\textit{Glycine max}) \cite{Stewart_Brown_2019} , \cite{Jarquin_2016},
\cite{Xavier_2016} ; switchgrass (\textit{Panicum virgatum}) \cite{Poudel_2019} ,\cite{Ramstein_2019} ,
\cite{Ramstein_2016} ; wheat (\textit{Triticum aestivum}) \cite{Cuevas_2019} , \cite{Howard_2019}
,\cite{Krause_2019}, \cite{Rincent_2018}, \cite{Norman_2018}, \cite{Belamkar_2018}, \cite{Ovenden_2018},
\cite{Sukumaran_2016}, \cite{Bustos_Korts_2016}, \cite{Gianola_2016_wheat}, \cite{Crossa_2016_wheat},
\cite{Thavamanikumar_2015}, \cite{Lopez_Cruz_2015},

and various tree species \cite{deAlmeidaFilho2019,}, \cite{Rincent_2018}, \cite{Kainer_2018}, \cite{Ratcliffe_2017},
\cite{GamalElDien_2016},
\cite{Kumar_2015}, \cite{Jaramillo_Correa_2014}, \cite{Zapata_Valenzuela_2013}, \cite{Holliday_2012}, \cite{Resende_2012}


\subsection{Genomic selection using artificial neural networks }
Genomic selection (GS) has been successfully applied in animal \cite{gianola2015one}, \cite{hayes2010genome}
and plant breeding \cite{crossa2010}, \cite{desta2014genomic}, \cite{heffner2010plant},
\cite{crossa2017genomic} as well as in medical applications, since it was first reported
\cite{hayes2001}. Since then the repertoire of methods for predicting phenotypic values has increased rapidly
e.g.\cite{dlc2009}, \cite{habier2011}, \cite{gianola2013} , \cite{crossa2017}. The most commonly applied
methods include GULP and a set of related algorithms known as the bayesian alphabet \cite{gianola2009}.
Genomic prediction in general has repeatedly been shown to outperform pedigree-based methods
\cite{crossa2010}, \cite{albrecht2011} and is nowadays used in many plant and animal breeding schemes.  It has
also been shown that using whole-genome information is superior to using only feature-selected markers with
known QTLs for a given trait \cite{bernardo2007}, \cite{heffner2011} in some cases. A more recent study
\cite{azodi2019} compared 11 different genomic prediction algorithms with a variety of data sets and found
contradicting results, indicating that feature selection can be usefull in some cases the when the whole
genome regression is performed by neural nets 1 While every new method is a valuable addition to the tool-kits
for genomic selection, some fundamental problems remain unsolved, of which the n>>p problematic stands
out. Usually in genomic selection settings the size of the training population (TRN) with n phenotypes is
substantially smaller than the number of markers (p) \cite{fan2014challenges}. Making the number of features
immensely large, even when SNP-SNP interactions are not considered. Furthermore each marker is treated as an
independent observation neglecting collinearity and linkage disequilibrium (LD).  Further difficulties arise
through non-additive, epistatic and dominance marker effects. The main problem with epistasis issue
quantitative genetics is the almost infinite amount of different marker combinations, that cannot be
represented within the size of TRN in the thousands, the same problems arises for example in GWA studies
\cite{korte2013advantages}. With already large p the number of possible additive SNP-SNP interactions
potentiates to $p^{(p-1)}$. Methods that attempt to overcome those issues are EG-BLUP, using an enhanced
epistatic kinship matrix and reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015},
\cite{martini2017genomic}.

In the past 10 years, due to increasing availability of high performance computational hardware with
decreasing costs and parallel development of free easy-to-use software, most prominent being googles library
TensorFlow \cite{TF2016} and Keras \cite{keras2015}, machine learning (ML) has experienced a renaissance.  ML
is a set of methods and algorithms used widely for regression and classification problems. popular among those
are e.g. support vector machines, multi-layer perceptrons (MLP) and convolutional neural networks. The machine
learning mimics the architecture of neural networks and are therefore commonly referred to as artificial
neural networks (ANN). Those algorithms have widely been applied in many biological fields \cite{min2017deep}
, \cite{lan2018survey}, \cite{mamoshina2016applications}, \cite{angermueller2016} , \cite{webb2018deep},
\cite{rampasek2016tensorflow}.

A variety of studies assessed the usability of ML in genomic prediction \cite{gonzalez2018applications},
\cite{gonza2016}, \cite{ogutu2011comparison}, \cite{montesinos2019benchmarking}, \cite{grinberg2018evaluation}
, \cite{cuevas2019deep}, \cite{montesinos2019new}, \cite{ma2017deepgs}, \cite{qiu2016application},
\cite{gonza2012} \cite{li2018genomic}.  Through all those studies the common denominator is that there is no
such thing as a gold standard for genomic prediction. No single algorithm was able to outperform all the
others tested in a single of those studies, let alone in all. While the generally aptitude of ML for genomic
selection has been repeatedly shown, how no evidence exists that neural networks can outperform or in many
cases perform on that same level as mixed-model approaches as GBLUP \cite{hayes2001prediction}. While in other
fields like image classification neural networks have up to 100s of hidden layers \cite{he2016deep} the
commonly used fully-connected networks in genomic prediction of 1 - 3 hidden layers.  With 1 layer networks
often being the most successful among those. Contradicting to the idea behind machine learning in genomic
selection 1 hidden layer networks will be inapt to capture interactions between loci and thus only account for
additive effects. As shown in \cite{azodi2019} convolutional networks perform worse than fully-connected
networks in genomic selection, which again is contradicting to other fields where convolutional layers are
applied successfully, e.g natural language processing \cite{dos2014deep} or medical image analysis
\cite{litjens2017survey}. Instead of using convolutional layers and fully-connected layers only, as show in
Pook et al 2019, we also propose to use locally-connected layer in combination with fully-connected
layers. While CL and LCL are closely related they have a significant difference. While in CL weights are
shared between neurons in LCLs each neuron as its own weight. This leads to a reduced number of parameters to
be trained in the following FCLs, and should therefore theoretically lead to a decrease in overfitting a
common problem in machine learning.  To evaluate the results of Pook et al. 2019 accomplished with simulated
data we used the data sets generated in the scope of the 1001 genome project of \textit{Arabidopsis thaliana}
\cite{1001genome}



\section{Proof of concept on ANN-based genomic selection}

\begin{table}[H]
\caption{The effects of treatments X and Y on the four groups studied.}
\label{tab:treatments}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $Y_{ADD}$ & $Y_{AND}$ & $Y_{OR}$ & $Y_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0 & 0 & 0 & 0 \\
  $G_2$ & 0 & 1 & 1 & 0 & 1 & 1 \\
  $G_3$ & 1 & 0 & 1 & 0 & 1 & 1 \\
  $G_4$ & 1 & 1 & 2 & 1 & 1 & 0 \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{The effects of treatments X and Y on the four groups studied.}
\label{tab:treatments}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $\hat{Y}_{ADD}$ & $\hat{Y}_{AND}$ & $\hat{Y}_{OR}$ & $\hat{Y}_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0.01 & 0.00 & 0.00 & 0.01 \\
  $G_2$ & 0 & 1 & 0.99 & 0.01 & 0.99 & 0.98 \\
  $G_3$ & 1 & 0 & 0.99 & 0.00 & 0.99 & 1.01 \\
  $G_4$ & 1 & 1 & 1.99 & 0.98 & 1.01 & 0.02 \\
  \bottomrule
\end{tabular}
\end{table}


\section{Material}
\subsection{DH populations derived from MAZE landraces}
\subsection{A. thaliana}
\section{Methods}
\subsection{ANN}
\subsection{GBLUP}


\section{Results}
\section{Discussion}
