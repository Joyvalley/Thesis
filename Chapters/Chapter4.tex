\chapter{Genomic prediction of phenotypic values of quantitative traits using artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 


\section{Introduction to machine learning}
\subsection{A brief history of machine learning} \label{introml}
\subsubsection{The basic perceptron model}

While machine learning, neural networks and deep learning became essential tools for many
applications in more recent years, their mathematical principals date back to the early 1950s and
1960s. Figure \ref{fig:perceptron} schematically shows the basic perceptron model as proposed by
Rosenblatt, one of the founders of machine learning, as the set of related statistical algorithms
would be defined today. Rosenblatt designed his perceptron to mimic the information flow in
biological nervous systems \cite{rosenblatt1961}.

\begin{figure}[th]
  \centering \includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png}
  \decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt \cite{rosenblatt1961}}
\label{fig:perceptron}
\end{figure}


This basic perceptron, which contrary to perceptrons used nowadays, does not have an embedded
activation function, takes $n$ binary inputs $x_1 , x_2 .... x_n$ and produces a single, likewise
binary, output $y$ after being processed. To achieve this Rosenblatt introduced the concept of
weights, which determine a certain input's relative importance to the outcome of the output
$w_1 , w_2 ... w_n$. The output $y$ is determined by the weighted sum of the weights
$\sum_i w_ix_i $. If a certain threshold value is met the neuron is either activated and outputs 1
or not activated resulting in and output of 0. This is algebraically represented in equation
\ref{eqn:weights}:

\begin{subequations}
 \begin{align}
  0 = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
  1 = \mbox{if } \sum_i^n w_i x_i - \theta > 0
 \end{align}
 \label{eqn:weights}
\end{subequations}


Next to the weights $w_n$ and the inputs $x_n$, a third term $\theta$ is introduced in equation
\ref{eqn:weights}, which represents the activation threshold and per definition is a negative
value. A single perceptron is a linear classifier and can only be trained on linearly separable
functions and can applied, as shown by \cite{rosenblatt1961}, to solve simple logical operations as
AND, OR and NOT. The basic perceptron fails, however, due to non-linearity to perform XOR
operations, which was proven by \cite{marvin1969}. This discovery led to a near stillstance in the
research of artificial neural networks in the 1970s. That time period is now often referred to as
the first AI-winter. Another reason that massively hindered the applications and research of machine
learning during that span was the, compared to
modern times, incredibly small amount of computational power available \cite{nguyen1990truck}. \\
More complex decision making, like solving XOR problems, requires more complex structures than a
single perceptron can provide. Continuing the trend of mimicking human neural networks, multiple
artificial neurons were stacked into layers and these layers were connected to each other allowing
communication between the many perceptrons in such a network. Figure \ref{fig:nn} schematically
shows the basic structure of an artificial neural network, now harboring three types of layers.
\begin{enumerate}[(i)]
\item the input layer
\item one or more hidden layers
\item the output layer, which in this case only consists of one only neuron
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/neuralnet}
\decoRule
\caption[Schematic layout of a simple multi-layer perceptron]{Schematic layout of a simple multi-layer perceptron}
\label{fig:nn}
\end{figure}


In the sample layout of figure \ref{fig:nn} the neurons in the first column weigh the inputs and
pass the gathered information to the neurons into the second layer. In the case above neurons in the
first layer are connected to all neurons on the second layer. Such layers are referred to as
fully-connected layers (FLC) and their resulting networks are often called multi-layer perceptrons
(MLP) or fully-connected networks. This architecture enables the network to perform more complex
calculations resulting in more abstract decision
making than single neurons or single layer architectures.\\
There are other architectures, where neurons in the previous layer are only connected with
neighboring neurons in the succeeding layers. Those are known as locally-connected layers
(LCL). Related to them are convolutional layers which share weights between selected neurons,
building convolutional neural networks (CNN) \cite{lecun1999object}.

\subsubsection{Activation functions}

The neurons discussed so far are only capable of outputting binary results, depending on whether
threshold values are being reached or not. For more complex estimations it is desirable that small
changes in the input also result in small changes of the output. This requirement cannot be easily
met with binary outputs. Activation functions for a given node provide more sophisticated rules for
the output in accordance to their inputs \cite{vzilinskas2006practical}.

\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.85\textwidth]{Figures/activation}
\decoRule
\caption[Popular activation functions for neural networks]{Popular activation functions used in
  neural networks.
  \textbf{A} Binary step activation function \\
  \textbf{B} Identity activation function \\
  \textbf{C} Sigmoid or logistic activation function \\
  \textbf{D} tangens hyperbolicus activation function \\
  \textbf{E} rectified linear units activation function \\
  \textbf{F} SoftPlus activation function\\}
\label{fig:activation}
\end{figure}


Figure \ref{fig:activation} shows six of the most commonly used activation functions
\cite{warner1996understanding}. The simplest one, the binary step activation was already introduced
(function \textbf{A} in equation \ref{eqn:binary}), which properties have been discussed along the
perceptron model. All other activation produce continuous outputs from given inputs. \\
Any mathematical function can serve as an activation function in neural nets, starting with a simple
identity function (equation \ref{eqn:ident}, figure \ref{fig:activation} \textbf{B}). The sigmoid
function (figure \ref{fig:activation} \textbf{C}, equation \ref{eqn:sigmoid}) and tanh (figure
\ref{fig:activation} \textbf{D}, equation \ref{eqn:tanh}), when $x \rightarrow \infty$ or
$x \rightarrow -\infty$ have similar properties as the binary function, but produce continuous
output around threshold values of 0.

\begin{equation}
 f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
 0 \; for \; x < 0 \\ 
 1 \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:binary}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = x
 \label{eqn:ident}
\end{equation}
    
\begin{equation}
 f(x) = \sigma(x) = \frac{1}{1+e^{-x}} 
 \label{eqn:sigmoid}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
 \label{eqn:tanh}
\end{equation}

\begin{equation}
 f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
 0 \; for \; x < 0 \\ 
 x \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:relu}
\end{equation}

\begin{equation}
 f(x) = ln(1+e^x)
 \label{eqn:softplus}
\end{equation}


ReLU (equation \ref{eqn:relu}) and the softplus (equation \ref{eqn:softplus}) share similar
properties as well, the latter one being a smoothed version of ReLU. Rectifiers as activation
functions have been introduced in the 2000s \cite{hahnloser2000digital} and have since then
overtaken all others as the most popular activations functions in neural networks and deep learning
\cite{lecun2015deep}. They have proven to be superior in many deep-learning applications over
sigmoid or logistic functions. One of the advantages leading to the superiority of ReLUs is that
with randomly initialized weights only half of the ReLU neurons are activated at start compared to
tanh and sigmoid activation \cite{glorot2011deep}. All activation functions shown in figure
\ref{fig:activation}, but the binary step function, share one common property: a small change of the
input weight will result in small changes of the output, while a small change of the input for the
binary step function leads to either no or a complete change of the output, except for ReLU when
$x<0$. This property is, as described below, is an important prerequisite for networks being able to learn. \\

\subsubsection{Gradient descent algorithm}

Let a network alike the one shown in figure \ref{fig:nn} be designed for the classification of an
arbitrary phenotype like blue petals with $x_1 \dots x_4$ on the input layers being genetic markers
as features. The output layer displays value from 0 to 1 giving the probability of the petals being
blue or not. To quantify how well the network performs on predicting the color of the petals a loss
function is applied \cite{schmidhuber2015deep}. \\
There is a large variety of different loss functions available for neural networks like mean squared
error (MSE), root mean squared error (RMSE) and cross-entropy among others. In generally MSE and
RMSE are used for regression problems, with the latter being less popular, and cross-entropy also
called log-loss is used for binary or multi-class classification settings
\cite{janocha2017loss}. Since all problems presented in due course are regression problems that use
MSE as their loss function this will be the only loss function further elaborated upon in. MSE or
the quadratic loss function can be written as:
\begin{equation}
 MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y})^2
 \label{eqn:MSE}
\end{equation}

Equation \ref{eqn:MSE} shows the MSE function which is the sum of the squares of the differences of
all the predicted $\hat{y}$ and the real values $y_i$. The same function can be rewritten with the
previously used terminology of weights and biases in equation \ref{eqn:MSE} with $L(w,b)$ as the
loss.

\begin{equation}
  L(w,b) = \frac{1}{2n} \sum_x \| y(x) - \hat{y}\|^2.
 \label{eqn:MSE2}
\end{equation}


With $w$ and $b$ as the collection of all the weights and the biases in the network used to optimize
the function $y(x)$. Giving the quadratic nature of the function $L(w,b)$ will always be
positive. If $L(w,b) \rightarrow 0$ the loss is minimal, meaning that the real and predicted values
are close together and
the network found weights and biases that explain the output well. \\
A widely used function to find the optimum for such a loss function by finding its minimum is
gradient descent (GD) \cite{bottou1991stochastic}. The idea behind GD or other optimizing algorithms
is to start with randomly initialized weights and biases and repeatedly move them in direction
$\Delta w$ and $\Delta b$. This results in a change of the loss function which can be represented
using partial derivatives as shown in equation \ref{eqn:deltaL}.

\begin{equation}
  \Delta L = \frac{\partial L}{\partial w} \Delta w + \frac{\partial L}{\partial b} \Delta b
\label{eqn:deltaL} 
\end{equation}

Ideally $\Delta L$ is negative and the optimization algorithm found $\Delta w$ and $\Delta b$ that
lead to a reduction of the loss. To simplify this problem let $\Delta d$ be the vector of changes:
$\Delta d = (\Delta w , \Delta b)^T $ and $\nabla L$ the vector of the partial derivatives as in
equation \ref{eqn:nabla}.


\begin{equation}
 \nabla L = \left(\frac{\partial L}{\partial w}, \frac{\partial L}{\partial w}\right)^T
\label{eqn:nabla}
\end{equation}

Having defined $\nabla L$ and $\Delta d$ the term \ref{eqn:deltaL} can be simplified to:

\begin{equation}
\Delta C = \nabla L * \Delta d
 \label{eqn:cd}
\end{equation}

Now the task of gradient descent is to find $\Delta d$ that results in $\Delta C$ being negative as
shown in equation \ref{eqn:eta}

\begin{equation}
 \Delta d = -\eta \nabla L
 \label{eqn:eta}
\end{equation}

Here $\eta$ is a small positive decimal number, commonly referred to as the learning rate, which
usually, but not exclusively, ranges from 0.1 to 0.001.  Having found a way to ensure that
$\Delta L$ always decreases according to equation \ref{eqn:eta} it can be utilized to repeatedly
update the gradient $\nabla L$ over time steps $T$. To make the gradient descent algorithm efficient
the learning rate $\eta$ has to be chosen correctly. If $\eta$ is too large the gradient $\Delta L$
possibly ends up being larger than zero leading to an increase of the loss. If the step size is too
small convergence will either take too long or not take place at all
\cite{bergstra2011algorithms}. In practical machine learning approaches different learning rates are
tested. There are also algorithmic approaches available. Equation \ref{eqn:deltaL} only accounts for
two inputs features but it can be generalized to compute $n$ inputs as shown in equation
\ref{eqn:gd}.

\begin{eqnarray}
  \nabla L = \left(\frac{\partial L}{\partial w_1}, \ldots ,
  \frac{\partial L}{\partial w_n}\right)^T
 \label{eqn:gd}
\end{eqnarray}

Equation \ref{eqn:gdwb} shows the gradient descent how it is used to repetitively update the weights
and biases to optimize the loss function $L(c,w)$ with $w$ and $b$ as the weight and bias matrices
and the learning rate $\eta$. In machine learning each iterational update of the network is often
called epoch or training epoch.

\begin{subequations}
 \begin{align}
  w = w_i - \eta \frac{\partial}{\partial w}L(w) \\
  b = b_i - \eta \frac{\partial}{\partial b}L(b) \\
 \end{align}
 \label{eqn:gdwb}
\end{subequations}

Substituting the partial differentials with $\nabla L$ equation \ref{eqn:gdwb} simplifies to:

\begin{equation}
 w = w_i - \eta \nabla L
 \label{eqn:simplegd} 
\end{equation}

\subsubsection{Optimizers}
The previous section introduced the concept of gradient descent, an algorithm to minimize the loss
function of the weights and biases of a neural network. All other optimizers introduced in
the following chapter are either variations or extensions of the basic gradient descent algorithm shown in equation \ref{eqn:gdwb}. \\
One disadvantage of gradient descent is that if the data set grows larger the demand in memory for
computation increases exponentially. Taking into consideration that machine learning is a
popular method in big data applications this is a serious drawback.\\
Methods to solve that issue are stochastic gradient descent and mini batch gradient descent.  The
idea behind the latter is to randomly divide the entity of the training data into subsamples called
mini batches \cite{bottou-bousquet-2008}. The network is then trained iteratively with all mini
batches. The batch size has a significant influence on the accuracy and the training speed of the
network and is a hyperparameter, which has to be tuned by iteratively testing different settings. If
the batch size is one mini batch GD is also referred to as stochastical gradient descent (SGD). \\
During the optimization process optimizers can descent into local minima of the cost function
without being able to overcome them enabling to reach the global minimum. An algorithm extending GD
to accelerate the search of the global minimum is momentum, which allows the GD to speed up when the
loss is decreasing and to slow down when going in the wrong direction, hence the loss function
$L(w,b)$ is increasing. This is achieved by accounting for the gradient of the previous step in the
calculation of the current step. This concept was introduced by \cite{polyak1964} and re-popularized
alongside the introduction of backpropagation learning by \cite{rumelhart1988learning} an algorithm
to efficiently update the weights and biases.

\begin{equation}
 w = w_i - \eta \nabla L + \alpha \Delta w
 \label{eqn:momentum}
\end{equation}

Equation \ref{eqn:momentum} shows how the momentum is mathematically represented in GD to update the
weights $w$ or likewise the biases. The delta of the weights multiplied by an coefficient $\alpha$
is the momentum. $\alpha$ is usually ranges from 0.1 to 0.9 and is another parameter to be tuned for
successful training. If the momentum is two small the GD will not be able to overcome local minima
and if $\alpha$ is two large the loss functions tends to oscillate without ever finding an optimum \cite{lecun2015deep}. \\
For both the momentum and the learning rate it is impractical to maintain the same level during all
training epochs. After each epoch the loss function is either closer or further away from its global
minima and depending on the distance to that minimum it is desirable to have larger or smaller
learning rates and momenta. This can be achieved with naive approaches, for example using a step
function to gradually decrease those values after each iteration, or to utilize algorithmic
approaches \cite{michie1994machine}. There is a large variety of optimizers trying to find optimal
values for $\alpha$ and $\eta$ and till today this field is under active research
\cite{goodfellow2016deep}. Popular among those are: RMSprop \cite{hinton2012neural}; Nesterov
momentum \cite{dozat2016incorporating}; Adadelta \cite{zeiler2012adadelta}; Adagrad
\cite{ruder2016overview} and Adam \cite{kingma2014adam}. With Adam being the most popular
optimizer today. \\
Nesterov momentum is slight change to the normal momentum algorithm, capable of having huge impacts in practical applications, because it helps avoiding oscillations around the minimum by using intermediate information to adapt the momentum. \\
RMSProp - root mean square propagation - is a method aiming to adapt the learning rate
algorithmically, by choosing $\eta$ for each iteration. Lastly the wide-spread Adam optimizer
combines both of the features of momentum and RMSProp and adapts the learning rate as well as the
momentum iteratively \cite{kingma2014adam}.

\subsubsection{Regularization parameters and overfitting}

A common problem in machine learning is over parameterize the model on the training data and losing
the ability to generalize on validation data. This issue occurs because neural networks have
hundreds of thousand of free parameters to be trained. Deeper neural networks even have billions or
trillions of parameters. If training of the neural net continues for enough epochs eventually the
loss function will approach a minimum. As $ L(w,b) \rightarrow 0 $ the initially drawn conclusion
could mislead to assuming that training was successful. However, when trying to apply the trained
network on the training data set (TRN) to a testing data set (TST) the loss and accuracy of the
prediction of TST are very large or accordingly small. This phenomenon is known as overfitting and a
lot of fine tuning of hyperparameters is devoted to minimizing this effect
\cite{tetko1995neural}. Figure \ref{fig:overfitting} visualizes this effects of during training
\cite{goodfellow2016deep} of a neural networks.

\begin{figure}[H]
  \centering \includegraphics[height=.35\textheight, width=1.1\textwidth]{Figures/overfitting} \decoRule
  \caption[Training vs. validation loss over time]{Learning curves showing how a loss function
    changes during training in the training and validation data set. While the training loss
    approaches 0 the validation loss starts increasing after hitting a minimum. This effect is due
    to overfitting on the training data set. Figure from \cite{goodfellow2016deep}.}
 \label{fig:overfitting}
\end{figure}

\textbf{Cross-validation} \\

A method that is used in basically every training of neural networks is splitting up the data in
multiple subsets. More specifically in a training set (TRN) and a testing set (TST). The training
set is used to minimize the loss functions and its success is evaluated on the TRN set by comparison
of the predicted values $\hat{y}$ and the real values in TST $y$. For all neural nets in this study
person's correlation coefficients were chosen as performance metric, calculated according to
equation \ref{eqn:pearson} \cite{soper1917distribution}.

\begin{equation}
  \rho(y,\hat{y}) = \frac{cov(y,\hat{y})}{ \sigma_y \sigma_{\hat{y}}}
 \label{eqn:pearson}
\end{equation}

There are other popular performance metrics, especially for classification problems, like AUC (area
under the curve) and ROC (receiver operating characteristics), which evaluate the success of
learning by weighing sensitivity and specificity. \\
In cross-validation compared to single validation the initial data set is split into TRN and TST
multiple times, e.g. if the ratio is 80:20 five times, and each TRN-TST pair is evaluated
individually. Sometimes it is necessary to use a third subset - the validation data set. Because
hyperparameter tuning is performed with the TRN and TST sets, a third portion of the data is needed
to asses whether the neural network is able to generalize on global data or not.

\textbf{L1 and L2 loss} \\
 
L1 and L2

\textbf{Dropout}

\section{Introduction to genomic selection}
\subsection{On the nature of quantitative traits} \label{quan}

According to the omnigenic model, which is an extension of the polygenic model, proposed
by \cite{boyle2017expanded} and thoroughly reviewed in \cite{timpson2018}, all traits or
phenotypic values are influenced by a great number or even all genes in the genome. This
therefore results in traits following gradual statistical distributions instead of being
binned in classes or
binary.\\
Intuitively this might be contradicting to the theoretical foundation of modern genetics -
Mendel's three laws. They were derived from observations, which were mainly influenced by
one locus. Using one of Mendel's phenotypes as an example - the round or wrinkled surfaces
of peas (\textit{Pisum sativum})- an assessment of a couple of thousands peas would
inevitably lead to the conclusion that from the ``roundest'' to the ``wrinkliest'' pea any
gradual step between those two classes is
possible and observable. \\
Mendel's third law of independent segregation also only holds true under certain
assumptions. The most simplest one being that the traits under investigation have to be
located on different linkage groups. Otherwise the seven traits used in Mendel's initial
studies would not have segregated independently. The odds of seven randomly selected
traits being on seven different linkage groups are rather small, especially taking into
account that the genome of the \textit{P. sativum} consists of only 7 chromosomes itself
\cite{kalo2004}. Mendel most likely new about traits not following its own laws, as well
as being aware of the quantitative nature of traits such as the constitution of pea's
surface or the color of its petals. However, being the pioneer of a then rather unexplored
field of science, some of which big questions we fail to satisfactory answer today, he did
not have
the resources or the knowledge to explain traits that were not ``mendeling''. \\
Initially thought to be contradicting to Mendel's ideas, Darwin proposed the concept of
evolution due to natural selection, which introduced the idea of traits following a
gradual distributions \cite{darwin1859}. This contrast led to a long lasting debate in the
scientific community in the early 1900s between the Mendelians and the biometricians, who
believed in the quantitative nature of continuous traits. This conflict has eventually
been solved by Fisher's fundamental work published in 1919 \cite{fisher1919xv}. His
theories combined the then in all fields of science popular research on statistical
distributions and genomics. He mathematically proved that traits influenced by many genes,
with randomly-sampled alleles follow a continuous normal distribution in a
population. \\
While this combined the ideas of Mendel and of the biometricians it opened an other long
debated question of effect sizes and the overall genetic architecture of complex
traits. While in the theory of monogenic traits the effect size of the single gene on the
trait is 100 \%, with an increasing number of genes influencing a complex trait the
\textit{per se} contribution of single gene has to decrease with an increasing number of
loci determining the value a given trait. Until the 1990s it has been believed that
complex traits are predominantly controlled by few genes with a
large to medium effect size, while others supposedly have a minimal influences \cite{zhang2018esti}. \\
With the upcoming popularity of GWAS as the favored method to decipher genetic
architectures of traits, having pioneered in human genetics, it became clear that the
majority of effect sizes are tiny < 1\%, while there are very few loci which have a
moderate effect on the phenotypic variance of a population with around 10\% or less
\cite{korte2013advantages}; \cite{stringer2011}. This nature of quantitative traits
presents great challenges to animal \cite{goddard2009} and plant breeding
\cite{wurschum2012} in further improving crop or livestock performances, as well as
complicating the decomposition of genomic causes for diseases like schizophrenia or autism in human medicine \cite{de2014}; \cite{purcell2014}. \\
While the complex nature of the architecture of quantitative traits provides enough
challenges as is, all traits are also influenced by the environment surrounding the individual.\\
Therefore the distribution of trait values in a given population can be expressed as the
addition of the variances of its genetic and the environmental effects \ref{eqn:PGE}.

\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E}
 \label{eqn:PGE}
\end{equation}

The genomic and the environmental effects do not only influence the phenotypic variance directly, but
the environment also has an influence on gene expression, methylation of DNA bases etc. and
therefore the equation \ref{eqn:PGE} extends by the variance of the gene-environment
interactions $\sigma_{GxE}$  to equation \ref{eqn:PGGE}  \cite{lynch1998}; \cite{walsh2018}.
    
\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E} + \sigma_{GxE}
 \label{eqn:PGGE}
\end{equation}

Equation \ref{eqn:PGGE} shows the decomposition of the phenotypic variance. To thoroughly
understand the complex genetic architectures of traits the genetic variance needs to be
decomposed further in its additive, dominance and epistatic components as in equation
\ref{eqn:GAD}.

\begin{equation}
 \sigma_{G} = \sigma_{A} + \sigma_{D} + \sigma_{I}
 \label{eqn:GAD}
\end{equation}

The additive effects are caused by single, for this model mostly homozygous, loci while
the variance caused by dominance effects is caused by heterozygous loci with their
resulting interactions being full-, over- , co- or underdominant. Lastly, the interaction
effects are a result of two or more genes only having an impact if they co-occur in a
certain state. The resulting variance is commonly known as the gene-gene interaction or epistasis \cite{falconer1996}. \\
Since possible interactions in a genome can appear between additive or dominant or a
combination of those loci, the variance due to interaction effects $\sigma_{I}$ can be
further dissembled into the variances resulting from additive-additive $\sigma_{AA}$
dominant-dominant $\sigma_{DD}$ and additive-dominant $\sigma_{AD}$ interactions as shown
in equation \ref{eqn:IAA}.

\begin{equation}
 \sigma_{I} = \sigma_{AxA} + \sigma_{DxD} + \sigma_{AxD}
 \label{eqn:IAA}
\end{equation}

Knowledge of the variance components involved in the expression of a trait in a given
population leads up to the estimation of the total influence of all genetic variances and
the environmental variance one the phenotypic distribution. This concept is called
heritability. \\
The heritability of a trait $H^2$ accounts for the proportion of the phenotypic variance
controlled by the total genetic variance as shown in equation \ref{eqn:h2G}. This is also
referred to as broad sense heritability, because all genetic effects, including additive,
dominance and epistatic effects, are included \cite{brooker1999genetics}.

\begin{equation}
 H^2 = \frac{\sigma_{A} + \sigma_{D} + \sigma_{I}}{\sigma_{P}}
 \label{eqn:h2G}
\end{equation}

The concept of narrow-sense heritability \ref{eqn:h2a} is similar to the broad-sense
heritability, but only the additive genetic effects are included in the equation. This
differentiation is import for natural and artificial selection and thus is commonly used
in evolutionary genomics and breeding. Because in diploid species each parent only passes
down on a single a allele of a given locus, dominance effects or interaction effects are
not commonly inherited from one parent. Therefore mainly the additive genetic effects of a
parent influence its offspring. While the dominance and epistatic variances are controlled
by the combination of the parents \cite{falconer1996}, \cite{walsh2018}.

\begin{equation}
  h^2 = \frac{\sigma_{A}}{\sigma_{P}}
 \label{eqn:h2a}
\end{equation}

\subsection{Artificial selection in plant and animal breeding in the genomics era}
\subsubsection{Introduction to genomic selection } \label{intro:gs}

Genomic prediction has been applied to almost all relevant crop and model species. This
includes: \textit{A.thaliana}; \cite{shen2013novel}; \cite{hu2015}.
Alfalfa (\textit{Medicago sativa}) \cite{li2012applied}; \cite{annicchiarico2015accuracy}; \cite{li2015genomical}; \cite{biazzi2017genome}; \cite{hawkins2018recent}. \\
Barley (\textit{Hordeum vulgare}) \cite{zhong2009factors}; \cite{oakey2016}; \cite{neyhart2019}. \\
Cassava (\textit{Manihot esculenta}) \cite{elias2018}; \cite{elias2018improving}.
Cauliflower (\textit{Brassica olearacea spp}) \cite{thorwarth2018genomic}.\\
Cotton (\textit{Gossiypium spp.}) \cite{gapare2018}.\\
Maize (\textit{Zea mays}) \cite{rincent2012}; \cite{windhausen2012};
\cite{technow2013genomic}; \cite{riedelsheimer2013genomic}; \cite{guo2013accuracy};
\cite{peiffer2014genetic}; \cite{technow2014genome}; \cite{lehermeier2014usefulness};
\cite{owens2014foundation}; \cite{montesinos2015threshold};\cite{bustos2016improvement};
\cite{kadam2016genomic}; \cite{schopp2017accuracy};\cite{schopp2017genomic};
\cite{e2017genomic}; \cite{brauner2018genomic};
\cite{schrag2018beyond}; \cite{moeinizade2019}; \cite{allier2019usefulness}. \\
Potato (\textit{Solanum tuberosum}) \cite{enciso2018genomic}; \cite{Endelman2018pot}.\\
Rape seed (\textit{Brassica naps}) \cite{snowdon2012potential}; \cite{wurschum2014potential}; \cite{qian2014sub}; \cite{jan2016genomic}; \cite{luo2017genomic}; \cite{werner2018effective}.\\
Rice (\textit{Oryza sativa})  \cite{Xu2013rice}; \cite{Grenier2015}; \cite{BenHassen2018}; \cite{Momen2019}. \\
Rye (\textit{Secale cerale}) \cite{bernal2014importance}; \cite{wang2014accuracy};
\cite{auinger2016model}; \cite{marulanda2016optimum}; \cite{bernal2017genomic}.
Sugar beet (\textit{Beta vulgaris}), \cite{wurschum2013genomic}; \cite{biscarini2014genome}.\\
Sugar cane (\textit{Saccharum officinarum}) \cite{gouy2013experimental}.\\
Soybean (\textit{Glycine max}) \cite{Jarquin_2016}; \cite{Xavier_2016};
\cite{Stewart_Brown_2019}.
Switchgrass (\textit{Panicum virgatum}) \cite{Ramstein_2016}; \cite{Poudel_2019}; \cite{Ramstein_2019}. \\
Wheat (\textit{Triticum aestivum}) \cite{Thavamanikumar_2015}; \cite{Lopez_Cruz_2015};
\cite{Sukumaran_2016}; \cite{Bustos_Korts_2016}; \cite{Gianola_2016_wheat};
\cite{Crossa_2016_wheat}; \cite{Rincent_2018}; \cite{Norman_2018}; \cite{Belamkar_2018};
\cite{Ovenden_2018}; \cite{Cuevas_2019}; \cite{Howard_2019}; \cite{Krause_2019}. \\
As well as various tree species \cite{Holliday_2012}; \cite{Resende_2012}; \cite{Zapata_Valenzuela_2013};  \cite{Jaramillo_Correa_2014}; \cite{Kumar_2015}; \cite{GamalElDien_2016}; \cite{Rincent_2018}; \cite{Ratcliffe_2017}; \cite{Kainer_2018}; \cite{deAlmeidaFilho2019}. \\
Even though GS finds broad applications in plant breeding it has been originally developed
for the use in animal breeding \cite{hayes2010genome}; \cite{goddard2011using}. The gold
standard is a method known as genomic BLUP \cite{vanraden2008efficient}, which utilizes a
relationship matrix based on the co-occurrence of genetic markers. This method is derived
from the pre-genomic era in animal breeding, where the relationship matrix was constructed
after pedigrees according to the best linear unbiased predictors based on the linear mixed
model equations developed by \cite{henderson1975best}. \\
GBLUP accounts only for additive-genetic effects \cite{vanraden2008efficient}. There are
other methods that are able to account for more complex genomic effects that are
non-additive. Popular among those are for example Reproducing Kernel Hilbert Spaces (RKHS)
\cite{gianola2008reproducing}. Alternatively to Henderson's linear mixed models a large
variety of different Bayesian methods became popular \cite{hayes2001}; \cite{gianola2009};
\cite{habier2011}; \cite{gianola2013}; \cite{crossa2017}.

\subsubsection{Genomic prediction in recurrent selection and the breeders equation}

While the quantitative genetic methods breeders utilize are complex their goals can be
defined in one sentence: To genetically improve plant germplasms for agriculture. The
breeding process started at the same time as farming around 10,000 BC in the region
between the Euphrat and Tigris rivers known as the fertile crescent
\cite{kingsbury2009hybrid}. This changed the phenotypic appearance of the early crops
dramatically to the point where they share little external traits with their wild
ancestors. Those changes have been deeply carved into the genomes, which underwent serious
alterations, including hybridization, duplications etc. This lead to most crop plants not
having any wild ancestors with whom they could naturally mate. For example wheat
(\textit{Triticum aestivum}), one of the three most important sources of food on a global
scale, underwent multiple hybridization steps \cite{ozkan2001allopolyploidy}. Wheat is a
hybrid from either the diploid emmer (\textit{T. diccoides}) or durum wheat
(\textit{T. durum}) and \textit{{Aegilops tauschii}}, while emmer and durum are hybrids
derived from wild emmer, which is a hybrid of wild grass of the genus of \textit{Aegilops}
and \textit{T. urata} \cite{friebe2000development}; \cite{feldman2012genome}.\\
While being ignorant of modern genetics early ``plant breeders'' must have had an
intuitive, yet naive, understanding of the general concept of heritability in a way that
they must have comprehended that offsprings share properties with their parents, which
motivated them to regrow individuals with desired traits generations after
generation. This induced many changes including that artificial selected plants are
commonly largely inbred. That process could be considered an early form of recurrent
truncation selection. Truncation selection on a normal distributed phenotype is shown in
figure \ref{fig:trunSel}.
 
\begin{figure}[H]
  \centering \includegraphics[height=.25\textheight, width=0.6\textwidth]{Figures/truncSel} \decoRule
  \caption[Truncation selection of a normal distributed phenotype]{Truncation selection
    from a normal distributed phenotype with selection a threshold value of $T$, $\mu$ as
    the mean of the total population and $\mu^{\ast}$ as the mean of the selected
    phenotypes. Graphic from \cite{walsh2018short}}
 \label{fig:trunSel}
\end{figure}

Like the early breeders modern breeders have to determine a selection threshold $T$ to divide the
total population with the mean $\mu$ into two groups: the individuals culled and the ones allowed to
reproduce with the mean $\mu^{\ast}$. The difference between those two is the selection differential
$S$:

\begin{equation}
 S = \mu^{\ast} - \mu
\label{eqn:S}
\end{equation}
\noindent
In the case of normal distributed data as depicted in figure \ref{fig:trunSel} $S$ can be expressed as:

\begin{equation}
S = \varphi (\frac{T - \mu}{\sigma}) \frac{\sigma}{p}
\end{equation}
\noindent
From which we can obtain the selection intensity $i$, which makes $i$ solely a function of $p$.

\begin{equation}
i = \frac{S}{\sigma} = \frac{\varphi (z_{|1-p|})}{p}
\end{equation}

With recurrent truncation selection over many generations the population mean of the trait
$\mu$ will change (hopefully in the desired direction), if the heritability (in this case
the narrow sense heritability) $h^2$ > 0. It is impossible to breed for traits that do not
contain any genetic components in their architecture \cite{walsh2018}.\\
Next to $i$ the selection intensity and $h^2$ the accuracy of the selection process
$r_{uA}$ is important for the success of a breeding program. Those three terms can be
applied to estimate the gain of selection $R$ over one generation (equation
\ref{eqn:Breeders}). Due to its importance in the evaluation of breeding schemes it is
known as the breeder's equation \cite{mousseau1987natural}; \cite{falconer1996};
\cite{kingsolver2001strength}.

\begin{equation}
 R = i r_{uA} \sigma_A
\label{eqn:Breeders}
\end{equation}

The accuracy $r_{uA}$ of equation \ref{eqn:Breeders} in cases when only phenotypic
selection is conducted is the narrow-sense heritability and in cases where the selection
process is aided by genomic prediction it is the prediction accuracy. According to the
breeder's equation there are three parameters, which can be influenced through genomic
prediction. \\

\begin{enumerate}[(i)]
\item The prediction accuracy, which is usually smaller than the heritability, varies for
  different prediction equations and an increase in the accuracy will lead to an
  proportional increase in $R$. For this reason since 2001, in quantitative genetics one
  very active field of research was and still is to find new, better algorithms for GS as
  presented in the next chapter (\ref{blup:bayes}. As later evaluated on more than 150
  phenotypes in chapter \ref{gpdis} $h^2$ is always larger than $r_{uA}$. Which if it was
  the only variable factor in equation \ref{eqn:Breeders}, would make genomic selection
  inferior to phenotypic selection, which from a certain point of view it is. Phenotypic
  trials are better approximations for phenotypic appearance as GEBVs. However, as the
  cost of genotyping has decreased dramatically in the last 20 years, phenotyping with
  field trials remains tedious, laborious and mostly vastly expensive. Taking into account
  that field trials have to be repeated in several years and locations to produce reliable
  accounts it becomes clear that genotyping 10th of thousands of accessions is much
  cheaper than conducting field trials with 1000 of them.
\item The selection intensity can be much stricter if the total population that is
  selected from is larger. In genomic prediction settings they are, because breeders can
  select from two pools. First the pool of plants with known phenotypes \underline{and}
  known genotype information and from those were just genomic data is available. When
  selecting from a pool of 1000 with $p=0.05$ with the goal to keep 50 plants in the next
  breeding cycle, the same goal can be reached when genomically selecting from a pool of
  10000 with and intensity of $p=0.005$.
\item The decrease in time per generation is probably the largest advantage of genomic
  selection, when applied to breeding. While in field trials it is only possible to have
  one generation per year, genomic selection does not require the plants to be grown in
  the field. For GS it is only necessary to grow the plants large enough, that DNA can be
  extracted from the tissues and evaluated. After selection only the ones above the
  threshold are grown until they bear seats (or other reproductive organs) and be used
  for the next selection cycle, allowing up to ten generations per year. This development
  has lead to the rise to a new branch of breeding: speed breeding \cite{ghosh2018speed};
  \cite{watson2018speed}. In practical, company-level breeding, genomic prediction as largely
  contributed to an increase by a factor of 2 to the gain in selection in recent years
  (personal communication with breeding company employees).
\end{enumerate}

The last term in equation \ref{eqn:Breeders}, the additive genetic variance $\sigma_A$, is
not directly, yet heavily influenced by the described breeding scheme. Artificial
selection has similar effects on the genetic variance as bottlenecks do in natural selection
have. It decreases, thus making it harder to increase $R$ in later selection cycles
\cite{walsh2018}.

\subsubsection{Genomic BLUP and Bayesian methods}\label{blup:bayes}

All methods share a common statistical obstacle, which is commonly referred to as the
$n >> p$ problem, which arises because the number $n$ of markers is usually significantly
larger than the number of observations $p$. In practical applications it is not uncommon
the have more than 100k markers while the number of phenotypes is no larger than 100. This
does not allow to obtain genomic estimated breeding values (GEBV) by single marker
regression as done by GWAS, which estimates highly inflated SNP-effects
\cite{korte2013advantages}. One possibility is to include effect sizes as random effects
and make prior assumptions about their distribution. The difference in prior distribution
is the main distinction between the many methods of the Bayesian alphabet introduced in
the following chapter \cite{gianola2013}.\\

\noindent
\textbf{Genomic BLUP} \\ In the early years of research on genomic prediction algorithms
were not solely benchmarked against each other, but had to compete with the previously
popular pedigree methods. Quickly in the course of the first decade of this millennium the
superiority of the genomic methods were elucidated in livestock and plant breeding
\cite{habier2007impact}; \cite{vanraden2008efficient}; \cite{vanraden2008reliability};
\cite{harris2009genomic}. While the genomic methods are superior to non-genomic methods,
there is no clear evidence that either of the genomic methods are superior to each other
and there is lack of empirical evidence that the Bayesian methods generally outperform
GBLUP \cite{moser2009comparison} ; \cite{bernardo2010breeding}; \cite{azodi2019}. \\
Like pedigree BLUP for genomic BLUP the co-variance between related individuals is used
for the predictions. In the latter case it is calculated from marker information.
\footnote{In the GWAS terminology the relationship matrix is referred to as $K$ for
  kinship, while in GS circumstances it is called GRM (genomic relationship matrix) or
  abbreviated as $G$. This study will remain consistent with the circumstantial literature
  and therefore purposely inconsistent within itself. In the chapter addressing GWAS the
  it will be called $K$ for kinship matrix and it the following chapter elucidating GBLUP
  it will be referred to as $G$.}
\\
The general genomic prediction model (equation \ref{eqn:blup}) is derived from mixed
models \cite{henderson1975best}; \cite{vanraden2008efficient} and implemented as:

\begin{equation}
Y = X \beta + Zu + \varepsilon
 \label{eqn:blup}
\end{equation}

where $Y$ is and $n\;x\;1$ vector of phenotypic observations, $X$ the matrix of the fixed
effects and $\beta$ the vector of the fixed effects. $Z$ is the incidence matrix for the
combined marker effects and $u$ iss a $n\; x\; 1$ vector of the additive genetic effect
the vector of the residuals $\varepsilon$.\\
To construct a GBLUP model lets assume a matrix of size $(n\; x\; m)$ with $n$ individuals
and $m$ loci $M$ containing marker information for three individuals on four loci, thus being of
size $3x4$. The four markers of matrix \ref{arr:M} can take values of $-1$, $0$ and $1$,
translating into minor allele, heterozygous locus and major allele. \footnote{This example
calculation has been adapted from \cite{isik2013}.}

\begin{equation}
 M = 
 \begin{pmatrix}[r]
  -1 & 0 & 1 & -1 \\
  -1 & 0 & 0 & 0 \\
   0 & 1 & 1 & -1 
 \end{pmatrix}
 \label{arr:M}
\end{equation}

The $M$ matrix contains all the information that are necessary for the computation of the K matrix and other viable genetic parameters. The $MM'$ matrix of size $n\; x\; n$ (\ref{arr:MM'}) bears additional parameters.

\begin{equation}
 MM' = 
 \begin{pmatrix}[r]
  3 & 1 & 2 \\
  -1 & 1 & 0 \\
  2 & 0 & 3 
 \end{pmatrix}
 \label{arr:MM'}
\end{equation}

The diagonal shows the number of homozygous loci per individual, while the other elements
of the matrix indicate the number of markers shared by related individuals. This is an
indicator for the distance of the relationship between individuals, as defined by
identity-by-descent \cite{vanraden2008efficient}; \cite{misztal2013methods}. While matrix
\ref{arr:MM'} calculates the metrics per individual, the $M'M$ matrix (\ref{arr:M'M})
accounts for metrics per marker. Likewise the diagonal contains the number of homozygous
individuals per marker.

\begin{equation}
 M'M = 
 \begin{pmatrix}[r]
  3 & -1 & 0 & 0 \\
  -1 & 1 & 1 & 1 \\
  0 & 1 & 2 & 1 \\
  0 & 1 & 1 & 2 
 \end{pmatrix}
 \label{arr:M'M}
\end{equation}

The next step is to obtain a matrix of the allele frequencies at each locus also of size
$n\; x\; m$ like matrix $M$. For the design of matrix $P$ (\ref{arr:P}) let the minor
allele frequencies $p_1 \dots p_4$ be $\{0.3, 0.2, 0.1, 0.15\}$. The allele frequency of
the $i^{th}$ column of $P$ is expressed according to the $n^{th}$ marker of matrix $M$ as
$P_i = 2(p_i - 0.5)$ resulting in:

\begin{equation}
 P = 
 \begin{pmatrix}[r]
  -0.4 & -0.6 & -0.8 & -0.7 \\
  -0.4 & -0.6 & -0.8 & -0.7 \\
  -0.4 & -0.6 & -0.8 & -0.7
 \end{pmatrix}
 \label{arr:P}
\end{equation}

The allele frequencies, as in this simulated example, should be drawn from the entire
population and not only the subsample used for the calculation
\cite{vanraden2008efficient}. The final step to obtain the Z matrix for the us in equation
\ref{eqn:blup} is to subtract the P matrix from the M matrix $Z= M - P$ resulting in:

\begin{equation}
 Z = 
 \begin{pmatrix}[r]
  1.4 & 0.6 & 1.8 & -0.3 \\
  -0.6 & 0.6 & 0.8 & 0.7 \\
  0.4 & 1.6 & 1.8 & -0.3 \\
 \end{pmatrix}
 \label{arr:Z}
\end{equation}

In $Z$ the mean values of the allele effects are set to 0 and the subtraction of $P$
emphasizes the effect of rare variants \cite{vanraden2008efficient}. There is a large
variety of methods to generate the genomic relationship matrices and here lies the major
difference between different genomic BLUP methods, but K is always of size $n\;x\;n$.

\begin{enumerate}[(i)]
\item The naive approach is to iterate over each individual and count the common markers
  with every other individual. This approach is suited for inbred or doubled-haploid
  populations, less so for outcrossed populations with high degrees of heterozygosity,
  because as in the sample implementation it does only account for homozygous loci. This
  method becomes computationally intense when the data sets grow larger as common today
  (personal observation).
\item Probably the most popular method in GS is to obtain $K$ as proposed by
  \cite{vanraden2008efficient} designed after Wright's \cite{wright1922coefficients}
  equations for the covariance in structured populations, as described by equation
  \ref{eqn:vanraden} with $Z$ as in \ref{arr:Z}.

\begin{equation}
 G = \frac{ZZ'}{2 \Sigma p_i (1-p_i)} 
\label{eqn:vanraden}
\end{equation}

\item The unified additive relationship $G_{UAR}$ according to \cite{yang2010common} and equation \ref{eqn:uar}

\begin{equation}
 G_{UAR} = A_{jk} = \frac{1}{N} \Sigma_i{A_{ijk}} = \left\{
  \!\begin{aligned}
   \frac{1}{N} \Sigma_{i} \frac{(x_{ij} - 2p_i)(x_{ik} - 2p_i)}{2p_i (1-p_i)}, j \ne k \\
   1 + \frac{1}{N} \Sigma_i \frac{x_{ij}^2 (1+2p_i) x_{ij} + 2p_i^2 }{2p_i (1-p_i)}, j = k   
  \end{aligned}
 \right.
 \label{eqn:uar}
\end{equation}

where $p_i$ is the allele frequency at locus $i$ and $x_{ij}$ the genotype for the
$j^{th}$ individual at the $i^th$ locus. Another method also proposed by
\cite{yang2010common} is to adjust $G_{UAR}$ with $\beta$ as in equation \ref{eqn:uaradj}

\begin{equation}
 G_{UARadj} = \left\{
  \!\begin{aligned}
   \beta A_{jk}, \;\; j \ne k \\
   1 + \beta (A_{jk}-1 ), \;\; j = k   
  \end{aligned}
  \right.
 \label{eqn:uaradj}
\end{equation}

\item Another approach is to weigh marker by the reciprocals of their expected variance
  according to the model \ref{eqn:ZDZ}. This was originally designed to investigate
  population structures in human genomics \cite{leutenegger2003estimation};
  \cite{amin2007genomic}.

\begin{equation}
 \!\begin{aligned}
  G = ZDZ' , with \\
  D_{ii} = \frac{1}{m | 2p_i(1-p_i) | }
 \end{aligned}
 \label{eqn:ZDZ}
\end{equation}

\item Other methods like the gaussian kernel compute kinship between individuals by the euclidean distance
between the respective genotypes \cite{morota2014kernel}.

\begin{equation}
 \!\begin{aligned}
  K(x_i,x_j) = exp (- \theta d_{ij}^2) \\
  = \prod_{k=1}^m exp (- \theta(x_{ik} - x_{jk})^2)
 \end{aligned}
 \label{eqn:gauss}
\end{equation}

with
$d_{ij} = \sqrt{(x_{i1} - x_{j1})^2 + \dots + (x_{ik} - x_{jk})^2 + \dots + (x_{im} - x_{j,a})^2 }$
and
$ x_{ik}(i,j = 1, \dots , n,k = 1, \dots , m)$ and $x_{ik}$ as the $i^{th}$ individual at SNP $k$. \\
\end{enumerate}

The linear model of equation \ref{eqn:blup} $Y = X \beta + Zu + \varepsilon$, with $\beta$
as the vector fixed effects and $u$ as the vector of additive genetic effects, can be
solved to obtain genomic estimated breeding values as:

\begin{equation}
 \begin{pmatrix}[ccc]
  X'X & X'Z & 0 \\ 
  Z'X & Z'Z + G^{11} & G^{12} \\ 
  0 & G^{21} & G^{22} \\ 
 \end{pmatrix}
 \begin{pmatrix}[r]
  \hat{b} \\ 
  \hat{y}_1 \\ 
  \hat{y}_2 \\ 
 \end{pmatrix}
 =
 \begin{pmatrix}[r]
  X'y \\ 
  Z'y \\ 
  0 \\ 
 \end{pmatrix}
 \label{eqn:pBLUP}
\end{equation}

with $G^{12}$ as the part of $G^{-1}$ containing individuals \underline{with} phenotypic data and with
$G^{22}$ as the part of $G^{-1}$ containing individuals \underline{without} phenotypic data and just
marker information available.

This can be algebraically solved to compute the GEBV of the unknown phenotypes $\hat{y}_2$  as:

\begin{equation}
\hat{y}_2 = -\left( G^{22}\right)^{-1}G^{21}\hat{y}_1
\label{eqn:gpred}
\end{equation}

GBLUP is fairly easy compared to more complex Bayesian methods and can be quickly implemented in any
programming language capable of solving liner equations like R or Python \cite{CRAN};
\cite{van1995python}. Computational as the number of phenotypes in the study increases in numbers
the timed demand grows exponentially, because the kinship matrix quadruples in size and
it becomes more complicated to compute the inverse of $G$ (personal observations). \\

\noindent
\textbf{Bayesian methods} \\ 

Next to the universal GBLUP a set of related algorithms became popular for solving the mixed models
involved in genomic selection, known as the Bayesian alphabet \cite{gianola2009};
\cite{gianola2013}. They are all based on Bayes' fundamental theorem (equation \ref{eqn:bayes}).

\begin{equation}
P(\theta | y) = \frac{P(\theta )P(y | \theta)}{P(y)} 
\label{eqn:bayes}
\end{equation}

with $P(\theta )$ as the prior distribution, $P(y|\theta )$ as the likelihood and $P(y)$
as the marginal density of $y$. The prior distribution in GS assume that $y$ was drawn
from a certain distribution. Infinitesimal models assume that the genetic effects follow a
normal distribution \cite{legarra2018}, while the Bayesian frameworks, however, will
assume non-normal distributed marker effects. This can be explained by a two-step
hierarchical distribution. Stage one assumes that every marker has \textit{a priori} a
different variance \cite{legarra2018}.

\begin{equation}
p(a_i|\sigma_{ai}^2) = N (0,\sigma^1_{ai})
 \label{eqn:stageonbayes}
\end{equation}

The second stage assumes prior distributions for the variances.

\begin{equation}
p(a_i| variable ) = P(\dots )
 \label{eqn:stagetwobayes}
\end{equation}

with $variable$ standing for the large variety of prior distributions. In total there are
more than >20 different Bayesian models known to the author. Their main difference
``simply'' lies in the \textit{a priori} assumptions of prior distributions. This change
can make some methods mathematically much more complicated then others. As shown in later
chapters none of the methods is completely superior over others in terms of
prediction accuracy. \\
Approximation to the solution of the linear equations is usually performed by Gibb's
sampling using Markov Chain Monte Carlo (MCMC) simulations \cite{dlc2009}; \cite{BGLR}.
Table \ref{tab:bayesABC} summarizes commonly applied Bayesian methods for genomic
prediction indicating their key differences.

\begin{table}[H]
\caption{Overview of properties of a variety of commonly applied Bayesian methods for genomic prediction. Table altered after \cite{karkkainen2012back}}
\label{tab:bayesABC}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lcccccc}
 \toprule
 Name        & Reference                  & Prior   & Indicator & Hierarchy & Hyperprior & Estimation \\
 \midrule
 BayesA      & \cite{hayes2001}           & Student & No        & Yes       & No         & MCMC       \\
 BayesB      & \cite{hayes2001}           & Student & Yes       & Yes       & No         & MCMC       \\
 BayesC      & \cite{verbyla2009accuracy} & Student & Yes       & Yes       & No         & MCMC       \\
 BL          & \cite{xu2010expectation}   & Laplace & No        & Yes       & No         & EM         \\
 BayesD$\pi$ & \cite{habier2011}          & Student & Yes       & Yes       & Yes        & MCMC       \\
                            \bottomrule                          
                          \end{tabular}}
\caption*{The name is given by the author. The prior column tells which shrinkage prior is used. }
\end{table}


\subsection{Genomic selection using artificial neural networks }

As mentioned in \ref{intro:gs} genomic selection (GS) has been successfully applied in
animal \cite{hayes2010genome}; \cite{gianola2015one} and plant breeding \cite{crossa2010};
\cite{heffner2010plant}; \cite{desta2014genomic}; \cite{crossa2017} as well as in medical
applications since it was first reported \cite{hayes2001}. Since then the repertoire of
methods for predicting phenotypic values has increased rapidly e.g. \cite{dlc2009};
\cite{habier2011}; \cite{gianola2013}; \cite{crossa2017}. Genomic prediction has
repeatedly been proven to outperform pedigree-based methods \cite{crossa2010};
\cite{albrecht2011} and is nowadays used in many plant and animal breeding schemes. It has
also been shown that using whole-genome information is superior to using only
feature-selected markers with known QTLs for a given trait \cite{bernardo2007};
\cite{heffner2011} in most cases. A more recent study \cite{azodi2019} compared 11
different genomic prediction algorithms with a variety of data sets and found
contradicting results, indicating that feature selection can be usefull for some cases
when whole genome regression is performed by neural nets. \\
While every new method is a valuable addition to the toolbox of genomic selection, some
fundamental problems remain unsolved and are the same for every algorithm, of which the
$n>>p$ problematic stands out. Usually in genomic selection settings the size of the
training population (TRN) with $n$ phenotypes is substantially smaller than the number of
markers $p$ \cite{fan2014challenges}, making the number of trainable features immensely
large. Furthermore every marker is treated as an independent observation neglecting
collinearity and linkage disequilibrium (LD) between them. More difficulties arise through
non-additive, epistatic and dominance marker effects. The main issue with epistasis in
quantitative genetics is the almost infinite amount of different marker combinations,
which cannot be represented within the size of TRN in the thousands. The same problems
arises in GWA studies \cite{korte2013advantages}. With already large $p$ the number of
possible additive SNP-SNP interactions potentiates to $p^{(p-1)}$. Methods that attempt to
overcome those issues are EG-BLUP, which us an enhanced epistatic kinship matrix and
reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015}; \cite{martini2017genomic}. \\
In the past 10 years, due to increasing availability of high performance computational
hardware with decreasing costs and parallel development of free easy-to-use software, most
prominent being googles library TensorFlow \cite{TF2016} and Keras \cite{keras2015},
machine learning (ML) has experienced a renaissance. ML is a set of methods and algorithms
used widely for regression and classification problems. Popular among those are
e.g. support vector machines, multi-layer perceptrons (MLP) and convolutional neural
networks. ML has been widely applied in many biological fields \cite{min2017deep};
\cite{lan2018survey}; \cite{mamoshina2016applications}; \cite{angermueller2016};
\cite{webb2018deep}; \cite{rampasek2016tensorflow}. \\
A variety of studies assessed the usability of ML in genomic prediction
\cite{gonzalez2018applications}; \cite{gonza2016}; \cite{ogutu2011comparison};
\cite{montesinos2019benchmarking}; \cite{grinberg2018evaluation}; \cite{cuevas2019deep};
\cite{montesinos2019new}; \cite{ma2017deepgs}; \cite{qiu2016application};
\cite{gonza2012}; \cite{li2018genomic}. Through all those studies the common denominator
is that there is no such thing as a gold standard for genomic prediction. No single
algorithm was able to outperform all the others tested in a single of those studies, let
alone in all. While the general aptitude of ML for genomic selection has been repeatedly
proven, there is no evidence exists that neural networks can generally outperform
mixed-model approaches as GBLUP \cite{hayes2001}. \\
In other fields like image classification neural networks have up to 100s of hidden layers
\cite{he2016deep}. The commonly used fully-connected networks in genomic prediction tend
to have one to three hidden layers. With one layer networks often being the most
successful among those. Contradicting to the idea behind machine learning in genomic
selection one hidden layer networks will be inapt to capture interactions between loci and
thus only account for additive effects. As shown in \cite{azodi2019} convolutional
networks perform worse than fully-connected networks in genomic selection, which again is
contradicting to other fields where convolutional layers are applied successfully, e.g
natural language processing \cite{dos2014deep} or medical image analysis
\cite{litjens2017survey}. Instead of using convolutional layers and fully-connected layers
only, as shown in Pook et al 2019, we also propose to use locally-connected layer in
combination with fully-connected layers. While CL and LCL are closely related they have a
significant difference. In CL weights are shared between neurons and in LCLs each neuron
as its own weight. This leads to a reduced number of parameters to be trained in the
following FCLs and should therefore theoretically lead to a decrease in overfitting. To
evaluate the usefulness of machine learning in GS the data sets generated in the scope of
the 1001 genome project of \textit{A. thaliana} \cite{1001genome} and the MAZE project
were used.

\section{Proof of concept for ANN-based genomic selection} \label{POC}

Having established the quantitative architecture of traits in section \ref{quan} and the
basics of machine learning and neural nets in section \ref{introml}, that knowledge can be
used to provide a proof of concept that neural networks are a candidate for GP. Table
\ref{tab:simmarker} provides all the possible genotypes $G_1 \dots G_4$ that can be
derived by two bi-allelic markers $M_1,M_2$ on a fictional haploid organism. In this
simulation the effect sizes for each marker $\beta_1$ and $\beta_2$ are constant with a
value of 1.

\begin{table}[H]
  \caption{Simple simulated phenotypes and genotypes for genomic prediction with genotypes
    $G_1 \dots G_4$, Markers $M_1$ and $M_2$ and phenotypes based on additive effects or
    $and$, $or$, $xor$ logic gates.}
\label{tab:simmarker}
\centering
\begin{tabular}{ l c c | c c c c c c }
 \toprule
 & $M_1$ & $M_2$ & $Y_{ADD}$ & $Y_{AND}$ & $Y_{OR}$ & $Y_{XOR}$\\
 \midrule
 $G_1$ & 0 & 0 & 0 & 0 & 0 & 0 \\
 $G_2$ & 0 & 1 & 1 & 0 & 1 & 1 \\
 $G_3$ & 1 & 0 & 1 & 0 & 1 & 1 \\
 $G_4$ & 1 & 1 & 2 & 1 & 1 & 0 \\
 \bottomrule
\end{tabular}
\end{table}

The four phenotypes $Y_{ADD}$, $Y_{AND}$, $Y_{OR}$ and $Y_{XOR}$ , which were derived from
their respective marker effects, were used for GP. $Y_{ADD}$ is a phenotype with purely
additive effects. So in the nomenclature introduced in chapter \ref{quan}
$\sigma_A = \sigma_G$ and $\sigma_{I} = 0$. Since the hypothetical organism is haploid
there are no dominance effects to be accounted for $\sigma_D = 0$ and since all the
genetic effects are caused by additive effects and there are also no environmental effects
$\sigma_E$.  The narrow sense heritability $h^2$ - equation \ref{eqn:h2a} - and the broad
sense heritability $H^2$ - equation \ref{eqn:h2G} - are equally 1. The other three
phenotypes are base on epistatic effects $\sigma_I$, generated by passing the markers
$M_1$ and $M_2$ through their respective logic gates. This theoretically results in
$h^2 = 0$ and $H^2 = 1$ because there should be no additive effects. For $y_{AND}$
,however, $h \approx 0.5$ because there is a correlation between $Y_{ADD}$ and
$Y_{AND}$. In practical applications this allows methods like GBLUP, designed to account
for additive genetic effects, to capture some of the epistatic effects of $\sigma_I$
\cite{vieira2017assessing}.

According to chapter \ref{introml} a single perceptron fails to solve $xor$ gates. While a
network with multiple nodes and layers should be able to overcome that deficit. A
relatively simple neural network with two fully-connected hidden layers with 10 and 5
nodes was trained for the prediction of the phenotypes. To keep the simulation as simple
possible, no regularization parameters like dropout etc. were included. The activation
function was ReLU (\ref{eqn:relu}) with an Adam optimizer. The results of the prediction
are shown in table \ref{tab:simgpres}.

\begin{table}[H]
\caption{Results of genomic prediction from phenotypes and genotypes in table \ref{tab:simmarker}}
\label{tab:simgpres}
\centering
\begin{tabular}{ l c c | c c c c c c }
 \toprule
 & $M_1$ & $M_2$ & $\hat{Y}_{ADD}$ & $\hat{Y}_{AND}$ & $\hat{Y}_{OR}$ & $\hat{Y}_{XOR}$\\
 \midrule
 \hline 
 $G_1$ & 0 & 0 & 0.01 & 0.00 & 0.00 & 0.01 \\
 $G_2$ & 0 & 1 & 0.99 & 0.01 & 0.99 & 0.98 \\
 $G_3$ & 1 & 0 & 0.99 & 0.00 & 0.99 & 1.01 \\
 $G_4$ & 1 & 1 & 1.99 & 0.98 & 1.01 & 0.02 \\
 \bottomrule
\end{tabular}
\end{table}


Not surprisingly, the simple network is able to solve all four problems and predict the
phenotypes accurately. The task was rather easy because the training data set and the
testing data set were the same, but it served the purpose of showing that neural networks
are generally apt to solve different marker interactions. \\
\textit{In natura} those interactions and the overall genetic architecture are much more
complex, of course . Effect sizes are not constant and epistasis may be caused by
interactions with more than just two markers. With an increasing number of markers $n$ the
number of possible two way interactions increases even more to $2^{n-1}$. Smaller
interaction effects could be obscured under larger additive effects. Gene-environment
interactions might have a significant influence resulting in a model that does not
converge.

\section{Material}
Two different data sets were used for the genomic prediction trials. A set of
doubled-haploid (DH) populations derived from maize landraces and \textit{A. thaliana}
data sets with genomic data procured along the 1001 genomic project \cite{1001genome} and
various phenotypic trials \cite{seren2016arapheno}.


\subsection{DH populations derived from maize landraces}
The DH populations were produced, propagated and phenotyped in the scope of the MAZE
project phase I, funded by the Federal Ministry of Education and Research (BMBF) (Funding
ID: 031B0195, project “MAZE”) as well as the KWS SAAT SE, by various project partners at
the Technical University of Munich, University of Hohenheim and the KWS. A thorough
description of the germplasm selection and phenotyping was recently published by
\cite{holker2019european}. \\
Modern maize cultivars are almost exclusively high-performing hybrids from two inbreed
lines originating from different heterotic pools. Commonly hybrids are derived from a
cross of European Flint and American Dent maize \cite{dos2004priori};
\cite{brauner2019testcross}. Before hybrid breeding became the predominant method in maize
breeding in the 1960s, landraces were grown by farmers. Landraces are dynamic,
open-pollinated, locally highly-adapted populations. They did not derive from modern
breeding, but from locally confined selection and adaption by farmers to often very
specific needs \cite{arteaga2016genomic}. The hybrids grown today are derived from just a
few landraces as founder lines, while the majority of landraces has been nearly
forgotten. This and high intensity selection over many generation has led to a loss of
genetic diversity $\sigma_G$ in modern maize cultivars.\\
The landrace germplasm present an important and essential stock of genetic variability for
continuous success in maize breeding. The utilization of those germplasms would be
impossible without the invaluable work of institutions like the IPK Gartersleben, whose goal
as genebanks is to maintain and store genetic material for long time periods. From European
three landraces representing large phenotypic and genetic heterogentiy were chosen to be assessed in the scope of the MAZE project:
\begin{enumerate}[(i)]
\item  Kemater Landmais Gelb (KE, Austria)
\item Petkuser Ferdinand Rot (PE, Germany)
\item Lalin (LL,Spain).
\end{enumerate}

They represent 95\% of the molecular variance in a set of
35 landraces analyzed in a preceding project by \cite{mayer2017there}.\\
In total 1015 DH lines (516 KE, 432 PE, 67 LL) were produced with \textit{in vivo} haploid
induction with an inducer line as described in \cite{roeber2005vivo}.


\subsubsection{Genomic maize data}
The genomic maize data was provided by the TUM as described by \cite{holker2019european}.\\
Genotyping was performed with the 600k Affymetrix\textsuperscript{\textregistered}
Axiom\textsuperscript{\textregistered} Maize array \cite{unterseer2014powerful}. The
markers were quality filtered and missing values were imputed individually for each
landrace population using Beagle 5.0 \cite{browning2007rapid};
\cite{browning2018one}. After LD pruning and further quality control 29833 markers
remained for 471 Kemater and 403 PE DHs. LL was excluded from further analyses due to
insufficient amounts of genotypes.

\subsubsection{Phenotypic maze data}
The phenotype data was provided by the TUM as described by \cite{holker2019european}. \\
The traits were evaluated with lattice design in 6 different locations across
Europe. Those traits were:

\begin{enumerate}[(i)]
\item early Vigor (EV) at three different stages (V3, V4, V6)
\item plant height (PH) at two developing stages (V4,V6)
\item the final plant height (PH\_final)
\item male flowering time: days till tasseling (DtTAS)
\item female flowering time: days till silking (DtSILK)) 
\item root lodging (RL)
\end{enumerate}

To account for GxE best linear unbiased estimators were calculated according to
Henderson's model \cite{henderson1975best} and used for further prediction. Once the BLUEs
were calculated across all environments and once they were calculated for the DHs in the six environments individually.


\subsubsection{Single environment prediction}
Next to the across environment BLUEs used for prediction the single environment BLUEs were used for prediction
to able to gain insights of the structure of $\sigma_{GxE}$ of the maize traits. This resulted in 2246
genotype x environment combinations for Kemater and 1975 for Petkuser with at least one data point. This
number is lower than the maximum number of n DHs per populations times the 6 environments, because naturally
not all genotypes yielded reliable data in the environments. Each DH x environment was treated as an
individual in for the across environment prediction. The marker matrix was enhanced with the environmental
origin as cofactors as show in table \ref{tab:envmarker} with one-hot encoded markers.

\onehalfspacing
\begin{table}[H]
 \centering
 \caption{Schematic representation of the enhanced genotype matrix for across environment prediction of maize phenotypes with DHs 1-2 with markers M 1-2 in environments E1-2}
 \label{tab:envmarker}
 \begin{tabular}{l|cccc}
  \toprule
      & M-1 & M-2 & E-1 & E-2 \\
  \midrule
  DH1-E1 & 0  & 1  & 1  & 0  \\
  DH2-E1 & 1  & 0  & 1  & 0  \\
  DH1-E2 & 0  & 1  & 0  & 1  \\
  DH2-E2 & 1  & 0  & 0  & 1  \\                      
  \bottomrule
 \end{tabular}
\end{table}
\doublespacing



\subsection{A. thaliana}

\subsubsection{Genomic data}

The genomic data was generated during the course of the 1001 genome project of \textit{A. thaliana}
\cite{1001genome} producing completed sequenced and assembled genomes from 1035 genomes, along 600k marker
data for 1307 accessions with a small overlap between those groups resulting in a total of 2029 genotyped
accessions. With more than 10 mio. SNPs and Indels on the 5 chromosomes of e \textit{A. thaliana}. Imputation
of missing data and upsampling of the 600k subsets was performed with Beagle3 \cite{browning2007rapid}. For
every one of the 164 phenotypes used for prediction subsets were sampled, LD pruned and MAF filtered. LD
pruning was executed with the R-package SNPRelate \cite{zheng2013tutorial} with a relatively strict LD
threshold of $0.65$ and $MAF > 10 $. This resulted in data sets with approximately 150.000 markers for each
phenotype.

\subsubsection{Phenotypic data}
A complete list of the phenotypes used can be found in Appendix \ref{AppendixB} with the according study
references. The phenotypic trials ranged from 100 to more than 1000 accessions per data set \cite{atwell2010}; \cite{li2010}; \cite{strauch2015}; \cite{me2014}.


\section{Methods}

The theoretical backgrounds of the methods used for genomic prediction were described in section \ref{introml}
for the ANNs and section\ref{blup:bayes} for the Bayesian methods and GBLUP. The next sections are devoted to
explaining who those methods were adapted and implemented for the prediction of maize and \textit{Arabidopsis}
traits.

\subsection{Validation scheme} \label{cv}

The validation approach in this study was a little different than common 5 fold cross validation. All
predictions were run 50 times with different splits of TST and TRN. For the full data set randomly 20\% were
assigned to TST and 80\% to TST. This process was repeated 50 times, reducing the chance of biases due to any
TST-TRN combination being randomly more predictable for on or the other method. The validation scheme was
generated \textit{a priori} and stored in cross-validation files to allow reusing the validation sets.

\subsection{ANN}
The scripts for ANN based GS were written in python using the lower level API TensorFlow \cite{TF2016} and the
higher level API Keras \cite{keras2015}. Both are very versatile, well-documented and are capable of
performing a large variety of machine learning applications. For those reasons they are the most used ML
libraries. Another advantage is that they work well on GPUs, which allows ML algorithms to run a reasonable
amount of time compared to CPU-based calculations. Prior to training the data was split into TRN and TST. The
markers of TRN served as the input layer for the network while the phenotypes were trained upon in the output
layer, which in the present cases consisted of only one node, because GS in the cases applied here is a
regression problem. Preliminary trials showed Adam is the superior optimizer for GS and hence was the only one
further used. Likewise relu was the activation of choice being superior to sigmoid or other non
rectifiers. All the weights and the biases of the kernel were initialized with truncated normal distributed
values. The loss function used was always MSE. \\
Having a few hyperparameters fixed, the other ones were optimized via grid search. For each TRN multiple
networks were trained to fine tune the input parameters. Those were the number of layers, the nodes per layer,
the magnitude of the dropout, the type of dropout used, whether the first layer was locally-connected for
fully-connected and the duration of training via the training epochs. This amount to a total of a little shy
of 260000 trained networks for the 146
\textit{A. thaliana} data sets alone. \\
After another set preliminary runs LCL as the first layer seamed to result in higher accuracies then FLC as
the first layer and where henceforth exclusively used and applied with a stride length of 7. The stride length
determines how many node of the input layer, in this case markers, where combined in the first hidden
layer. The type of drop out used (alpha dropout, Gaussian noise or normal dropout) did not show an effect
therefore the normal dropout function was used. The network training was iterated over the different number
epochs, architecture, drop out values the the cross validation cycles, thus explaining the tremendous amount
of total networks trained. Epochs from $5$ to $60$ in steps $5$ and several 1, 2 or 3 Layer architectures
following the locally-connected layer.

\subsection{GBLUP}

The evaluation of the genomic BLUP was performed with the R-package BGLR \cite{BGLR}. To allow pairwise
comparison of the individual validation runs the same validation scheme as for the ANNs was used with the same
TST and TRN sets. 

\section{Results} \label{res:gp}
\subsubsection{Results of \textit{A. thaliana} prediction}
Table \ref{tab:at_res} show the results for genomic prediction for 146 \textit{A. thaliana} phenotypes with
ANNs and GBLUP and the architecture, determined via grid search, yielding the highest prediction accuracies.
Figure \ref{AC:gp_res} contains scatter plots for the comparison of accuracies for GBLUP and the respective
ANNs for 50 validation sets for all phenotypes.  In table \ref{tab:at_res} phenotypes were ANN outperformed
the genomic BLUP are indicated in red. At first sight ANN can surpass with GBLUP for traits with high
accuracies, compete at an intermediate level and fails to reach to level of GBLUP when overall accuracies are
low.

\singlespacing
\begin{longtable}{p{.5\textwidth} p{.1\textwidth} p{.1\textwidth} p{.15\textwidth} p{.1\textwidth}}
  \caption[Prediction accuracies of \textit{A. thaliana} phenotypes for GBLUP and ANN]{Prediction accuracies of \textit{A. thaliana} phenotypes for GBLUP and ANN} \\
 \toprule
 Phenotype & GBLUP & ANN & Architecture & Epochs \\
 \hline
 FT16 & 0.8237 & 0.8215 & 100 & 10 \\
 2W & 0.8156 & \color{red}{0.8205} & 50, 30 & 35 \\
 FT10 & 0.8249 & 0.8191 & 48 & 50 \\
 LD & 0.8128 & \color{red}{0.8159} & 150 & 30 \\
 DTF sweden 2009 (1st experiment) & 0.8063 & \color{red}{0.8141} & 48 & 30 \\
 DTF sweden 2009 (2nd experiment) & 0.8035 & \color{red}{0.8091} & 50, 30 & 20 \\
 DTF sweden 2008 (2nd experiment) & 0.7986 & \color{red}{0.8057} & 150 & 25 \\
 4W & 0.795 & \color{red}{0.8052} & 50, 35, 15 & 30 \\
 FT22 & 0.8009 & \color{red}{0.8043} & 150 & 15 \\
 DTF spain 2008 (2nd experiment) & 0.7975 & \color{red}{0.8032} & 150 & 40 \\
 LN16 & 0.7996 & \color{red}{0.7999} & 50, 30 & 20 \\
 DTF spain 2009 (2nd experiment) & 0.7917 & \color{red}{0.7988} & 150 & 55 \\
 LDV & 0.8158 & 0.7975 & 150 & 15 \\
 0W GH FT & 0.7873 & \color{red}{0.7942} & 50, 30 & 15 \\
 DTFmainEffect2009 & 0.7794 & \color{red}{0.7855} & 50, 35, 15 & 35 \\
 SD & 0.7905 & 0.7848 & 48 & 30 \\
 DTFplantingSummer2008 & 0.75 & \color{red}{0.7746} & 50, 30 & 20 \\
 FT GH & 0.7693 & \color{red}{0.7702} & 50, 30 & 15 \\
 DTFlocSweden2009 & 0.7595 & \color{red}{0.7626} & 50, 30 & 60 \\
 DTFplantingSummer2009 & 0.7521 & \color{red}{0.7584} & 50, 30 & 50 \\
 0W & 0.7488 & 0.7473 & 48 & 40 \\
 DTF spain 2009 (1st experiment) & 0.7691 & 0.7425 & 48 & 40 \\
 DTF sweden 2008 (1st experiment) & 0.727 & \color{red}{0.728} & 50, 30 & 20 \\
 DTFlocSweden2008 & 0.7161 & \color{red}{0.7271} & 50, 30 & 55 \\
 Seed Dormancy & 0.7014 & \color{red}{0.7241} & 50, 30 & 35 \\
 DTFmainEffect2008 & 0.7102 & \color{red}{0.7142} & 50, 30 & 20 \\
 8W & 0.7259 & 0.7083 & 150 & 50 \\
 LN22 & 0.7004 & \color{red}{0.7069} & 50, 30 & 20 \\
 Size sweden 2009 (1st experiment) & 0.6905 & \color{red}{0.6994} & 48 & 50 \\
 LN10 & 0.6934 & \color{red}{0.698} & 50, 30 & 20 \\
 DTF spain 2008 (1st experiment) & 0.6944 & 0.677 & 150 & 25 \\
 SDV & 0.6775 & 0.6728 & 150 & 15 \\
 8W GH FT & 0.7001 & 0.6546 & 48 & 40 \\
 0W GH LN & 0.6568 & 0.654 & 50, 30 & 20 \\
 Storage 7 days & 0.6496 & \color{red}{0.65} & 50, 30 & 25 \\
 Storage 28 days & 0.6627 & 0.6483 & 50, 30 & 55 \\
 8W GH LN & 0.671 & 0.6434 & 48 & 70 \\
 Size sweden 2009 (2nd experiment) & 0.6114 & \color{red}{0.6268} & 48 & 50 \\
 SizeLocSweden2009 & 0.6144 & \color{red}{0.619} & 150 & 35 \\
 FLC & 0.6118 & \color{red}{0.6161} & 50, 30 & 30 \\
 LFS GH & 0.6178 & 0.6136 & 150 & 35 \\
 FT Field & 0.7324 & 0.6112 & 150 & 60 \\
 LY & 0.6072 & \color{red}{0.6088} & 150 & 60 \\
 Storage 56 days & 0.6085 & 0.5788 & 150 & 15 \\
 LES & 0.56 & \color{red}{0.5764} & 150 & 50 \\
 M216T665 & 0.5155 & \color{red}{0.5674} & 50, 30 & 50 \\
 LC Duration GH & 0.5799 & 0.5664 & 150 & 55 \\
 M172T666 & 0.5165 & \color{red}{0.5487} & 150 & 60 \\
 Trichome avg JA & 0.588 & 0.5343 & 150 & 55 \\
 Secondary Dormancy & 0.5184 & \color{red}{0.5264} & 150 & 30 \\
 SizeMainEffect2009 & 0.52 & 0.5171 & 48 & 50 \\
 DSDS50 & 0.4754 & \color{red}{0.5006} & 50, 30 & 60 \\
 avrPphB & 0.5054 & 0.4942 & 150 & 60 \\
 Hypocotyl length & 0.4934 & 0.4807 & 150 & 50 \\
 Size spain 2009 (1st experiment) & 0.5121 & 0.4751 & 150 & 50 \\
 Yield spain 2009 (1st experiment) & 0.5205 & 0.4719 & 50, 30 & 50 \\
 Leaf serr 10 & 0.4636 & \color{red}{0.4683} & 150 & 55 \\
 Size spain 2009 (2nd experiment) & 0.471 & 0.4623 & 48 & 50 \\
 Trichome avg C & 0.4617 & 0.4385 & 48 & 40 \\
 Germ in dark & 0.4447 & 0.4382 & 150 & 15 \\
 YieldMainEffect2009 & 0.505 & 0.4345 & 150 & 30 \\
 FT Diameter Field & 0.5004 & 0.4274 & 150 & 15 \\
 Bacterial titer & 0.5406 & 0.417 & 150 & 55 \\
 FRI & 0.4011 & \color{red}{0.4119} & 48 & 30 \\
 Rosette Erect 22 & 0.3973 & 0.3934 & 48 & 30 \\
 Area sweden 2009 (1st experiment) & 0.4203 & 0.3895 & 50, 35, 15 & 30 \\
 Width 10 & 0.3932 & 0.3784 & 50, 30 & 60 \\
 Silique 22 & 0.4339 & 0.377 & 50, 30 & 50 \\
 avrRpt2 & 0.3757 & 0.3737 & 50, 30 & 30 \\
 M130T666 & 0.4381 & 0.3733 & 150 & 60 \\
 SizePlantingSummer2009 & 0.3769 & 0.3615 & 150 & 5 \\
 Area sweden 2009 (2nd experiment) & 0.359 & 0.3542 & 48 & 45 \\
 FW & 0.3397 & \color{red}{0.3522} & 50, 30 & 25 \\
 P31 & 0.3632 & 0.3419 & 50, 30 & 45 \\
 MT GH & 0.4016 & 0.3397 & 150 & 50 \\
 avrB & 0.3304 & \color{red}{0.3384} & 50, 30 & 30 \\
 avrRpm1 & 0.361 & 0.3368 & 50, 30 & 20 \\
 Seed bank 133-91 & 0.3446 & 0.3334 & 150 & 5 \\
 Mg25 & 0.5321 & 0.3288 & 50, 30 & 60 \\
 Leaf roll 10 & 0.3558 & 0.3272 & 48 & 40 \\
 Yield spain 2009 (2nd experiment) & 0.4184 & 0.3197 & 20, 10 & 40 \\
 Noco2 & 0.3051 & \color{red}{0.3174} & 48 & 30 \\
 Emwa1 & 0.3226 & 0.3124 & 50, 30 & 30 \\
 FT Duration GH & 0.2659 & \color{red}{0.3123} & 48 & 5 \\
 Leaf serr 22 & 0.3021 & \color{red}{0.3108} & 150 & 60 \\
 Anthocyanin 10 & 0.3198 & 0.3107 & 50, 35, 15 & 60 \\
 Cd114 & 0.3345 & 0.3069 & 50, 30 & 50 \\
 Leaf serr 16 & 0.2895 & \color{red}{0.3011} & 48 & 40 \\
 Fe56 & 0.2802 & \color{red}{0.3006} & 150 & 35 \\
 YieldLocSweden2009 & 0.3431 & 0.2993 & 150 & 60 \\
 Width 16 & 0.3463 & 0.2983 & 150 & 50 \\
 Co59 & 0.2738 & \color{red}{0.2953} & 50, 35, 15 & 25 \\
 K39 & 0.3036 & 0.2952 & 50, 30 & 60 \\
 Leaf roll 16 & 0.3072 & 0.2886 & 150 & 15 \\
 DTFplantingLoc2008 & 0.2971 & 0.275 & 50, 30 & 5 \\
 SizePlantingSummerLocSweden2009 & 0.2803 & 0.2704 & 50, 30 & 60 \\
 Mn55 & 0.2775 & 0.2662 & 50, 30 & 20 \\
 Anthocyanin 22 & 0.2731 & 0.2635 & 150 & 15 \\
 As75 & 0.254 & \color{red}{0.2619} & 50, 30 & 35 \\
 Na23 & 0.2564 & \color{red}{0.2598} & 50, 30 & 15 \\
 Ni60 & 0.2894 & 0.2539 & 150 & 25 \\
 Mo98 & 0.2765 & 0.2537 & 50, 30 & 35 \\
 Chlorosis 22 & 0.2622 & 0.2453 & 50, 35, 15 & 10 \\
 Hiks1 & 0.2441 & \color{red}{0.2452} & 20, 10 & 20 \\
 Zn66 & 0.2553 & 0.2444 & 150 & 35 \\
 B11 & 0.2891 & 0.2392 & 48 & 40 \\
 Germ 16 & 0.2987 & 0.2356 & 50, 30 & 41 \\
 At2 & 0.2147 & \color{red}{0.216} & 150 & 15 \\
 Emco5 & 0.166 & \color{red}{0.2101} & 150, 30 & 20 \\
 Se82 & 0.2192 & 0.2075 & 150 & 25 \\
 Mature cell length & 0.1987 & \color{red}{0.2052} & 150 & 45 \\
 DW & 0.2878 & 0.2048 & 50, 30 & 60 \\
 Yield sweden 2009 (1st experiment) & 0.2274 & 0.2033 & 150 & 55 \\
 As2 & 0.1774 & \color{red}{0.1962} & 150 & 15 \\
 Meristem zone length & 0.1976 & 0.195 & 150 & 50 \\
 Germ 10 & 0.2073 & 0.1873 & 20, 10 & 40 \\
 Anthocyanin 16 & 0.2433 & 0.1867 & 20, 10 & 10 \\
 Width 22 & 0.2224 & 0.1856 & 50, 30 & 50 \\
 YieldPlantingSummerLocSweden2009 & 0.2146 & 0.18 & 150 & 55 \\
 DTFplantingSummerLocSweden2009 & 0.2032 & 0.1775 & 150 & 55 \\
 Bs & 0.2161 & 0.1656 & 50, 30 & 60 \\
 Bs CFU2 & 0.1672 & 0.1584 & 50, 35, 15 & 15 \\
 Germ 22 & 0.1267 & \color{red}{0.1533} & 50, 30 & 35 \\
 Leaf roll 22 & 0.1135 & \color{red}{0.1511} & 48 & 45 \\
 RP GH & 0.1755 & 0.1458 & 150 & 15 \\
 Cu65 & 0.1543 & 0.1315 & 150 & 5 \\
 Li7 & 0.1611 & 0.1297 & 150 & 60 \\
 As & 0.1089 & \color{red}{0.1227} & 100 & 20 \\
 At1 & 0.1473 & 0.1197 & 48 & 40 \\
 S34 & 0.1045 & \color{red}{0.11} & 50, 30 & 60 \\
 YieldPlantingSummer2009 & 0.1265 & 0.0984 & 150 & 50 \\
 Silique 16 & 0.2366 & 0.0884 & 50, 30 & 60 \\
 Chlorosis 10 & 0.0243 & \color{red}{0.088} & 50, 35, 15 & 55 \\
 Ca43 & 0.3333 & 0.0732 & 50, 35, 15 & 55 \\
 Seedling Growth & 0.0813 & 0.0636 & 48 & 30 \\
 Vern Growth & -0.0096 & \color{red}{0.0422} & 150 & 15 \\
 At2 CFU2 & 0.0694 & 0.0378 & 150 & 25 \\
 Yield sweden 2009 (2nd experiment) & 0.0536 & 0.0355 & 150 & 25 \\
 As CFU2 & 0.0312 & \color{red}{0.035} & 150 & 5 \\
 At1 CFU2 & 0.0818 & 0.0319 & 50, 30 & 50 \\
 Aphid number & -0.0246 & \color{red}{0.029} & 50, 35, 15 & 10 \\
 After Vern Growth & -0.1433 & \color{red}{0.0057} & 50, 35, 15 & 5 \\
 Chlorosis 16 & -0.0313 & \color{red}{-0.0121} & 150 & 5 \\
 As2 CFU2 & 0.0504 & -0.0325 & 50, 30 & 60 \\
\bottomrule
\label{tab:at_res}
\end{longtable}
\doublespacing



\begin{figure}[H]
  \centering\includegraphics[height=.59\textheight, width=1.0\textwidth]{ann_vs_gblup}
  \decoRule
  \caption[Scatterplot comparing prediction accuaracies of ANN and GBLUP in \textit{A. thaliana}]{Scatterplot comparing prediction accuaracies of ANN and GBLUP in \textit{A. thaliana}. Greyscale indicates the magnitude of the difference between the methods}
\label{fig:annblup}
\end{figure}

\subsection{Results of maize prediction}
\subsubsection{Across environments}

\begin{figure}[H]
 \centering \includegraphics[angle=0,height=.49\textheight, width=1.1\textwidth]{gp_kemater}
 \decoRule
\caption[Violinplot comparing the results for GP in the DH population Kemater for ANN and GBLUP]{Violinplot comparing the results for GP in the DH population Kemater for ANN and GBLUP }
\label{fig:ke_ann}
\end{figure}

\begin{figure}[H]
 \centering \includegraphics[angle=0,height=.495\textheight, width=1.1\textwidth]{gp_petkuser}
 \decoRule
 \caption[Violinplot comparing the results for GP in the DH population Petkuser for ANN and GBLUP]{Violinplot comparing the results for GP in the DH population Petkuser for ANN and GBLUP }
\label{fig:pe_ann}
\end{figure}


\onehalfspacing
\begin{table}[H]
\centering
\begin{tabular}{lcc|cc}
  \toprule
  & \multicolumn{2}{c}{\textbf{Kemater}} & \multicolumn{2}{c}{\textbf{Petkuser}} \\
  Phenotype & GBLUP & ANN & GBLUP & ANN \\ 
  \midrule
  EV\_V3 & 0.44 & 0.46 & 0.31 & 0.25 \\ 
  EV\_V4 & 0.47 & 0.49 & 0.31 & 0.25 \\ 
  EV\_V6 & 0.43 & 0.44 & 0.38 & 0.33 \\ 
  DtTAS & 0.47 & 0.44 & & \\ 
  PH\_V4\_mean & 0.54 & 0.56 & 0.46 & 0.44 \\ 
  PH\_V6\_mean & 0.53 & 0.56 & 0.51 & 0.48 \\ 
  PH\_final & 0.69 & 0.70 & 0.68 & 0.67 \\ 
  DtSILK & 0.57 & 0.53 & 0.54 & 0.52 \\ 
  \bottomrule
\end{tabular}
\end{table}
\doublespacing

\subsubsection{Single environments}

The prediction of the single environment BLUEs with the environmentally enhanced marker matrix yielded
substantially higher prediction accuracies than the prediction with the across environment BLUEs (previous section)-

\begin{figure}[H]
 \centering \includegraphics[angle=0,height=.895\textheight, width=1.1\textwidth]{SL_pred}
 \decoRule
 \caption[Results of genomic prediction across single environments for Kemater and Petkuser DH populations]{Results of genomic prediction across single environments for \textbf{A} Kemater and \textbf{B} Petkuser DH populations}
\label{fig:sl_pred}
\end{figure}

\onehalfspacing
\begin{table}[H]
 \centering
 \caption[Comparison of prediction results of ANN within locations and across locations for Kemater and Petkuser]{Comparison of prediction results of ANN within locations (WL) and across locations (AL) for Kemater and Petkuser}
 \begin{tabular}{lrrr|rrr}
  \toprule
  & \multicolumn{3}{c}{\textbf{Kemater}} & \multicolumn{3}{c}{\textbf{Petkuser}} \\
  Phenotype & AL  & WL  & $\Delta$ & AL  & WL & $\Delta$ \\ 
  \midrule
  EV\_V3 & 0.73 & 0.46 & 0.27 & 0.72 & 0.25 & 0.47 \\ 
  EV\_V4 & 0.72 & 0.49 & 0.23 & 0.66 & 0.25 & 0.40 \\ 
  EV\_V6 & 0.70 & 0.44 & 0.26 & 0.65 & 0.33 & 0.33 \\ 
  PH\_V4 & 0.84 & 0.56 & 0.28 & 0.84 & 0.44 & 0.41 \\ 
  PH\_V6 & 0.80 & 0.56 & 0.25 & 0.80 & 0.48 & 0.31 \\ 
  PH\_final & 0.78 & 0.70 & 0.08 & 0.76 & 0.67 & 0.09 \\ 
  DtSILK & 0.76 & 0.53 & 0.23 & 0.77 & 0.52 & 0.25 \\ 
  \bottomrule
 \end{tabular}
\end{table}
\doublespacing

\section{Discussion}\label{gpdis}

\onehalfspacing
\begin{table}[H]
 \centering
 \caption[ANN architectures of ANN resulting in highest prediction accuracies]{ANN architectures resulting in highest prediction accuracies, with number of hidden layer (HL) and the total count (n)}
 \begin{tabular}{cccc}
 \toprule
  LCL & Architecture & HL & n \\ 
  \midrule
  True & 150    & 2 & 56 \\ 
  True & 50, 30   & 3 & 47 \\ 
  True & 48     & 2 & 23 \\ 
  True & 50, 35, 15 & 4 & 11 \\ 
  True & 20, 10   & 3 &  5 \\ 
  True & 100    & 2 &  2 \\ 
  True & 150, 30  & 3 & \\
   \bottomrule
\end{tabular}
\end{table}
\doublespacing