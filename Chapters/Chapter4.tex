\chapter{Genomic prediction of phenotypic values of quantitative traits using artificial neural networks}

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 


\section{Introduction}
\subsection{A brief history of machine learning} \label{introml}
\subsubsection{Basic perceptron model}

While machine learning, neural networks and deep learning became essential tools for many applications in more
recent years, their mathematical principals date back to the early 1950s and 1960s. Figure
\ref{fig:perceptron} schematically show the basic perceptron model as proposed by Rosenblatt, a founder of
machine learning as it would be defined today,  which was designed to mimic the information flow in biological
nervous systems \cite{rosenblatt1961}.

\begin{figure}[th]
 \centering \includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/perceptron.png} \decoRule
\caption[Basic perceptron model]{Basic perceptron model as proposed by Rosenblatt}
\label{fig:perceptron}
\end{figure}

This basic perceptron, which contrary to perceptrons used nowadays does not have an embedded activation
function, takes $n$ binary inputs $x_1 , x_2 .... x_n$ and produces a single, likewise binary, output $y$
after being processed by the perceptron or neuron. To achieve this Rosenblatt introduced the concept of
weights, which indicated a certain input's relative importance to the outcome of the output $w_1 , w_2 ... w_n$. The
output $y$ is determined by the weighted sum of the weights and biases $\sum_i w_ix_i $ and iff a certain
threshold value is met the neuron is either activated and outputs 1 or not activated resulting in and output
of 0. This is algebraically represented in equation \ref{eqn:weights}:

\begin{subequations}
 \begin{align}
   0 = \mbox{if } \sum_i^n w_j x_i - \theta \leq 0 \\
   1 = \mbox{if } \sum_i^n w_i x_i - \theta > 0
 \end{align}
 \label{eqn:weights}
\end{subequations}

\noindent
Next to the weights $w_n$ and the inputs $x_n$ a third term $\theta$ is introduced in equation
\ref{eqn:weights} which represents the activation threshold and per definition is of negative value. A single
perceptron is a linear classifier and can only be trained on linearly separable functions and can applied, as
shown by \cite{rosenblatt1961}, to solve simple logical operations as AND, OR and NOT. The simple perceptron
fails, however, due to non-linearity, to perform XOR operations, proven by \cite{marvin1969}. This discovery
let to a near stillstance in the research of artificial neural networks in the 1970s. This time period is now
often referred to as the first AI-winter. Another reason that massively hindered the applications and research
of machine learning during that span was the, compared to
modern times, incredibly small amount of computational power available \cite{nguyen1990truck}. \\
More complex decision making, like solving XOR problems, requires more complex structures than a single
perceptron. Continuing the trend of mimicking human neural networks, multiple artificial neurons were stacked
into layers and these layers were connected to each other allowing communication between the many perceptrons
in such a network. Figure \ref{fig:nn} shows schematically the basic structure of an artificial neural network,
now harboring three types of layers. (i) the input layer, (ii) one or more hidden layers and (iii) one output
layer, which in this case only consists of one only neuron.

\begin{figure}[H]
\centering
\includegraphics[height=.25\textheight, width=.5\textwidth]{Figures/neuralnet}
\decoRule
\caption[Schematic layout of a simple multi-layer perceptron]{Schematic layout of a simple multi-layer perceptron}
\label{fig:nn}
\end{figure}

In the sample layout of figure \ref{fig:nn} the neurons in the first column weigh the inputs and pass those
the the neurons in the second layer. In this case all neurons on the first layer or connected to all neurons
on the second layer, such layers are referred to fully-connected layers (FLC) , and their resulting networks
are often called multi-layer perceptrons (MLP). This architecture enables the network to perform more complex
calculations and result in more abstract decisions than single neurons or single layer architectures. There
are also layers were neurons in the previous layer are only connected with neighboring neurons in the
succeeding layers. Those are known as locally-connected layers (LCL) or related to them are convolutional
layers which shared weights between selected neurons, building convolutional neural networks (CNN) \cite{lecun1999object}.

\subsubsection{Activation functions}

The neurons discussed so far are only capable of outputting binary results. Either 0 or 1, depending on
whether threshold values are being reached or not. For more complex estimations it is desirable that small
changes in the input also result in small changes of the output. This requirement can not be met with binary
outputs. Activation functions for a given node provide more sophisticated rules for the output in accordance
to their inputs \cite{vzilinskas2006practical}.

\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.85\textwidth]{Figures/activation}
\decoRule
\caption[Popular activation functions for neural networks]{Popular activation functions used in neural networks.
 \textbf{A} Binary step activation function.
 \textbf{B} Identity activation function.
 \textbf{C} Sigmoid or logistic activation function.
 \textbf{D} tangens hyperbolicus activation function.
 \textbf{E} rectified linear units activation function .
 \textbf{F} SoftPlus activation function.}
\label{fig:activation}
\end{figure}

Figure \ref{fig:activation} shows six of the most commonly used activation functions
\cite{warner1996understanding}. The simplest one was introduced , is the binary step activation function \textbf{A} in
equation \ref{eqn:binary}, which properties have been discussed along the perceptron model. All other
activation produce continuous outputs from any given input. Any mathematical function can serve as
an activation function in neural nets, starting with a simple identity function \ref{eqn:ident} ,
\ref{fig:activation} \textbf{B}.  The sigmoid functions \ref{fig:activation} \textbf{C}, equation \ref{eqn:sigmoid}
and tanh figure \ref{fig:activation} \textbf{D}, equation \ref{eqn:tanh}, when $x \rightarrow \infty$ or
$x \rightarrow -\infty$ they have similar properties to the binary function, but produce continuous output
around 0.

\begin{equation}
 f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
  0 \; for \; x < 0 \\ 
  1 \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:binary}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = x
 \label{eqn:ident}
\end{equation}
       
\begin{equation}
 f(x) = \sigma(x) = \frac{1}{1+e^{-x}} 
 \label{eqn:sigmoid}
\end{equation}

\begin{equation}
 f(x) = \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
 \label{eqn:tanh}
\end{equation}

\begin{equation}
  f(x)= \sigma(x) = \left\{
 \begin{array}{ll}
  0 \; for \; x < 0 \\ 
  x \; for \; x \geq 0
 \end{array}
\right .
\label{eqn:relu}
\end{equation}

\begin{equation}
  f(x) = ln(1+e^x)
 \label{eqn:softplus}
\end{equation}

ReLU (equation \ref{eqn:relu}) and the softplus (equation \ref{eqn:softplus}) share similar properties as
well, the latter one being a smoothed version of ReLU. Rectifiers as activation functions have been introduced
in 2000s \cite{hahnloser2000digital} and have since then overtaken all others as the most popular activations
functions in neural networks and deep learning today \cite{lecun2015deep} and they have proven to be superior
in many deep-learning applications over sigmoid or logistic functions. One of the advantages leading to the
superiority of ReLUs is that with randomly initialized weights only half of the ReLU neurons are activated at start,
compared to tanh and sigmoid activation \cite{glorot2011deep}. All activation functions shown in figure
\ref{fig:activation}, but the binary step function, share one common property: a small change of the input
weight will result in small changes in the output, while a small change of the input for the binary step
function leads to either no or a complete change of the output. This property is, as described below, is an
important prerequisite for networks being able to learn. \\

\subsubsection{Gradient descent algorithm}

Let the network shown in \ref{fig:nn} be for the classification of a arbitrary phenotype like blue petals with
$x_1 \dots x_4$ on the input layers being genetic markers as features. And the output layer displaying a value
from 0 to 1, meaning yes: blue petals from 0 - 0.5 and no blue petals from 0.5 to 1. To quantify how well the
network performs on achieving that goal a loss function is applied \cite{schmidhuber2015deep}. There is a
large variety of different loss functions available for neural networks like mean squared error (MSE), root
mean squared error (RMSE), cross-entropy and many others. In general MSE and RMSE are commonly used for regression
problems, with the latter being less popular and cross-entropy also called log-loss is used for binary or
multi-class classification problems \cite{janocha2017loss}. Since all problems presented in due course are
regression problems, that use MSE as their loss function, this will be the only loss function used.

\begin{equation}
 MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \widetilde{y})^2
 \label{eqn:MSE}
\end{equation}

Equation \ref{eqn:MSE} shows the MSE function which is the sum of the squares of the differences of all the
predicted and the real values. The same function can be rewritten with the previously used terminology of
weights and biases in equation \ref{eqn:MSE}.

\begin{equation}
  L(w,b) =  \frac{1}{2n} \sum_x \| y(x) - \widetilde{y}\|^2.
 \label{eqn:MSE2}
\end{equation}

With $w$ and $b$ as the collection of all the weights and the biases in the network used to optimize the
function $y(x)$. Giving the quadratic nature of the function the $L(w,b)$ will always be positive. And if
$L(w,b) \rightarrow 0$ the loss is minimal, meaning that the real and predicted values are close together and
the network found weights and biases that explain the output well. \\
A widely used function to find the optimum for such a loss function is gradient descent. Its objective is to
fine the minimum for the loss function \cite{bottou1991stochastic}.  The behind gradient descent or other
optimizing algorithms is start with randomly initialized weights and biases and repeatedly move them in
direction $\Delta w$ and $\Delta b$. This results in a change of the loss function as shown in equation
\ref{eqn:deltaL}, making use of partial derivatives.

\begin{equation}
 \Delta L = \frac{\partial L}{\partial w} \Delta w + \frac{\partial L}{\partial b} \Delta b
\label{eqn:deltaL}  
\end{equation}

Ideally $\Delta L$ is negative and the optimization algorithm found $\Delta w$ and $\Delta b$ that lead to a
reduction of the loss. To simplify this problem let $\Delta d$ be the vector of changes:
$\Delta d = (\Delta w , \Delta b)^T $ and $\nabla L$ the vector of the partial derivatives: equation
\ref{eqn:nabla}

\begin{equation}
 \nabla L = \left(\frac{\partial L}{\partial w}, \frac{\partial L}{\partial w}\right)^T
\label{eqn:nabla}
\end{equation}

Having defined $\nabla L$ and $\Delta d$ the term \ref{eqn:deltaL} can be simplified as equation \ref{eqn:cd}

\begin{equation}
\Delta C = \nabla L * \Delta d
 \label{eqn:cd}
\end{equation}

Now the task of gradient descent or any other optimizer is to find $\Delta d$ that results in $\Delta C$ being negative
as shown in equation \ref{eqn:eta}

\begin{equation}
 \Delta d = -\eta \nabla L
 \label{eqn:eta}
\end{equation}

In this case $\eta$ is a small positive decimal number, commonly referred to as the learning rate, which
usually, but not exclusively ranges from 0.1 to 0.001. However it can be larger or much smaller in some
cases. Having found a way to ensure that $\Delta L$ always decreases according to equation \ref{eqn:eta} it is
utilized to repeatedly update the gradient $\nabla L$. To make the gradient descent algorithm efficient the
learning rate $\eta$ must be chosen correctly. If $\eta$ is too large, the gradient $\Delta L$ might end up
being larger than zero, leading to an increase of the loss, and if the step size is too small convergence will
either take too long or not take place at all \cite{bergstra2011algorithms}. In practical machine learning
approaches different learning rates are tested. There are also algorithmic approaches. While equation
\ref{eqn:deltaL} only accounts for two inputs features, it can be generalized to compute $n$ inputs shown in
equation \ref{eqn:gd}.

\begin{eqnarray}
 \nabla L = \left(\frac{\partial L}{\partial w_1}, \ldots ,
 \frac{\partial L}{\partial w_n}\right)^T
 \label{eqn:gd}
\end{eqnarray}

Equation \ref{eqn:gdwb} shows the gradient descent how it is used to repetitively update the weights and
biases to optimize the loss function $L(c,w)$ with $w$ and $b$ as the weight and bias matrices and the
learning rate $\eta$. In machine learning each iterational update of the network is often called epoch or
training epoch.

\begin{subequations}
 \begin{align}
   w = w_i - \eta \frac{\partial}{\partial w}L(w) \\
   b = b_i - \eta \frac{\partial}{\partial b}L(b) \\
 \end{align}
 \label{eqn:gdwb}
\end{subequations}

Substituting the partial differentials with $\nabla L$ equation \ref{eqn:gdwb} a simplifies to:

\begin{equation}
  w = w_i - \eta  \nabla L
  \label{eqn:simplegd}  
\end{equation}

\subsubsection{Optimizers}
The previous section introduced the concept of gradient descent, an algorithm to minimize the loss function of
the weights and biases of a neural network. All other optimizers introduced here, are either variations or
extensions of the basic gradient descent algorithm (GD) shown in \ref{eqn:gdwb}. One disadvantage of gradient
descent is that if the data sets grow larger, the demand in memory for computation increases
exponentially. Taking into consideration machine learning is a popular method ind big data applications this
is a serious drawback. Methods to overcome that are stochastic gradient descent and mini-batch gradient
descent.  The idea behind the latter is to randomly divide the entity of the training data in sub-samples
called mini-batches \cite{bottou-bousquet-2008}. The network is then trained iteratively over the mini
batches. The batch size influences the accuracy and the training speed and is another hyperparameter which has
to be tuned. If the batch size is 1 mini batch GD is also referred to as stochastical gradient descent (SGD).
During the optimization process optimizers can find local minima in the cost function without being able to
overcome them to find the desired global minimum. An algorithm extending GD to accelerate the search of the
global minimum is momentum. Which allows the GD to speed up when the loss is decreasing and to slow down when
going in the wrong direction - increasing the loss function $L(w,b)$. This is achieved by accounting for the
gradient of the previous step in the calculation of the current step. This concept was introduced by
\cite{polyak1964} and re-popularized alongside backpropagation learning by \cite{rumelhart1988learning}.

\begin{equation}
  w = w_i - \eta \nabla L + \alpha \Delta w
  \label{eqn:momentum}
\end{equation}

Equation \ref{eqn:momentum} shows how the momentum is mathematically represented in GD to update the weights
$w$ or likewise the biases the delta of the weights multiplied by the coefficient $\alpha$ - the momentum,
which usually ranges from 0.1 to 0.9 and is another parameter to tuned for successful training. If the
momentum is two small the GD will not be able to overcome local minima and if $\alpha$ is two large the loss
functions tends to oscillate without finding an optimum \cite{lecun2015deep}.  For both of the momentum and
the learning rate it is impractical to remain on the same level during all training epochs. Because after each
epoch the loss function is either closer or further away from its global minima and depending on the distance
to that minimum it is desirable to have larger or smaller learning rates and momenta. This can be achieved
with naive approaches for example using a step function to gradually decrease those values after each
iteration, or to utilize algorithmic approaches \cite{michie1994machine}.  There is a large variety of
optimizers trying to find optimal values for $\alpha$ and $\eta$ and till today this field is under active
research \cite{goodfellow2016deep}. Popular among those are: RMSprop \cite{hinton2012neural}; Nesterov
momentum \cite{dozat2016incorporating}; Adadelta \cite{zeiler2012adadelta}; Adagrad \cite{ruder2016overview}
and Adam \cite{kingma2014adam}. With Adam being the most widely used optimizer today.  Nesterov momentum is
slight change to the
normal momentum capable of having huge impacts in practical applications, because it helps avoiding oscillations around the minimum by using intermediate information to adapt the momentum. \\
RMSProp - root mean square propagation - is a method aiming to adapt the learning rate algorithmically, by
choosing $\eta$ for each parameter.  And lastly the wide-spread Adam optimizer combines both of the features
of momentum and RMSProp and adapts the learning rate as well as the momentum iteratively
\cite{kingma2014adam}.


\subsubsection{Backpropagation}
maybe i will leave out backpropagation
$\odot$

Backpropagation \cite{rumelhart1988learning}

\subsubsection{Regularization parameters}

When applying the combined aforementioned algorithms and optimizers to find global minima of a loss function
of a neural network the problem of overfitting arises, because optimizers like Adam work ``too'' well. This
issue is due to the fact that neural networks have 100s of thousand of free parameters to be trained, deep
neural networks have billions and trillions of parameters. If training of the neural net continues for enough
epochs eventually the loss function will approach a minimum and as $ L(w,b)\rightarrow 0 $ the initially drawn
conclusion could mislead to assuming that training was quite successful, but when trying to apply the network trained on
the training data set (TRN) to a testing data set (TST) the loss and accuracy of the prediction of TST and
TST are very large or accordingly small. This phenomenon is know as overfitting and a lot of fine tuning of
hyperparameters is devoted to minimizing this effect \cite{tetko1995neural}. Figure \ref{fig:overfitting}
visualizes the effects of overfitting during training \cite{goodfellow2016deep}.

\begin{figure}[H]
   \centering \includegraphics[height=.35\textheight, width=1.1\textwidth]{Figures/overfitting} \decoRule
   \caption[Training vs. validation loss over time]{Learning curves showing how a loss function changes during training
     in the loss and validation data set. While the training loss approaches 0 the validation loss starts increasing
     after hitting a minimum. This effect is due to overfitting on the training data set. Figure from
     \cite{goodfellow2016deep}.}
 \label{fig:overfitting}
\end{figure}

\textbf{Cross-validation} \\

A method that is used in basically every training of neural network is splitting up the data sets in multiple
sub-sets.  More specifically a training set (TRN) and a testing set (TST). The training set is used to
minimize the loss functions and its success is evaluated on the TRN set, by comparing the predicted values
$\hat{y}$ with the real values in TST $y$.  For all neural nets in this study person's correlation coefficient
was chosen as performance metric, as in equation \ref{eqn:pearson} \cite{soper1917distribution}.

\begin{equation}
\rho(y,\hat{y})  =  \frac{cov(y,\hat{y})}{  \sigma_y \sigma_{\hat{y}}}
  \label{eqn:pearson}
\end{equation}

There are other popular performance metrics, especially for classification problems, like AUC (area under the
curve) and ROC (receiver operating characteristics), which basically evaluate by weighing sensitivity and
specificity.  In cross-validation compared to single validation the initial data set is split into TRN and TST
multiple times e.g. if the ratio is 80:20 5 times, and each TRN-TST pair is evaluated individually. Sometimes
it becomes necessary to use a third subset - the validation data. Because hyperparameter tuning is performed
with the TRN and TST sets, a third portion of the data needs to be assessed to check whether the neural
network is able to generalize on global data.

\textbf{L1 and L2 loss} \\
 
L1 and L2


\textbf{Dropout}


\subsection{On the nature of quantitative traits} \label{quan} According to the omnigenic model which is an
extension of the polygenic model proposed by \cite{boyle2017expanded} and thoroughly reviewed in
\cite{timpson2018} all traits or phenotypic values are influenced by a great number or all genes in the
genome. Therefore resulting in traits following certain gradual statistical distributions instead of being
binned in classes or even binary. Intuitively this might be contradicting with the foundation of modern
Genetics - Mendel's three laws. That where derived from observations with where mainly influenced by one
locus. But staying with one of Mendel's examples the round or wrinkled surfaces of peas \textit{Pisum
  sativum}, an assessment of a couple of thousands peas, would most likely inevitably lead to the conclusion
that form the ``roundest'' to the ``wrinkliest'' pea any gradual step between those is possible and
observable. Mendel's third law of independent segregation also only holds true under certain assumptions. The
most simplest one being that the traits under investigation have to be located on different linkage
groups. Otherwise for the 7 traits used in Mendel's initial studies would not have segregated
independently. The odds of 7 randomly selected traits being on 7 different linkage groups are rather small,
especially taking into account, that the genome of the \textit{P. sativum} consists of only 7 chromosomes
itself \cite{kalo2004}. Mendel probably new about traits not following its own laws, as well as being aware of
the quantitative nature of traits such as the constitution of surfaces of peas or the color of petals. But
being the pioneer of a then rather unexplored field of science, some of which big questions we fail to
satisfactory answer today, he did not have the resources or the knowledge to explain behavior's not
``mendeling'', that were only able to be deciphered in later decades and centuries
based on his ground-breaking work. \\
Initially thought to be contradicting to Mendel's ideas Darwin proposed the concept's of evolution due to
natural selection which also introduce the idea of traits following a gradual distribution
\cite{darwin1859}. This contrast led to a long lasting debate in the scientific community in the early 1900s,
between the Mendelians and the biometricians who believed in the quantitative nature of continuous
traits. This conflict has eventually been solved by Fisher's fundamental work published in 1918
\cite{fisher1919xv}. His theories combined the then in all fields of science popular research of distributions
with genomics. He he mathematically proved that traits influenced by many genes, with randomly-sampled alleles
follow a continuous normal distribution in a population. While this combined the ideas of Mendel and the
biometricians it opened an other long debated question of effect size and the overall architecture of complex
traits. While in the theory of monogenic traits the effect size of the single gene on the trait is 1 or 100 \%
with an increasing number of genes influencing a complex traits the \textit{per sè} contribution of single
gene has to decrease with an increasing number of loci determining the value a given trait. In the 1990s it
has been thought, that complex traits are predominantly controlled from few genes with a large to medium
effect size, while others had a minimal influence
\cite{zhang2018esti}. \\
With the upcoming popularity of GWAS as the favored method to decipher genetic architectures of traits, or
having pioneered in human genetics in became clear that the majority of the effect sizes are tiny < 1 \% while
there are very few loci which have a moderate effect on the phenotypic variance of a population with around 10
\% or less \cite{korte2013advantages}, \cite{stringer2011}. This nature of quantitative traits present great
challenges to animal \cite{goddard2009} and plant breeding \cite{wurschum2012}, in further improving crop or
livestock performances, as well complicating the decomposition of genomic causes for diseases like
schizophrenia or autism in human medicine \cite{de2014}, \cite{purcell2014}. \\
While the complex nature of the architecture of quantitative traits provide enough challenges as is, all
traits will also be influenced by the environment from which an individual originates. Therefore the
distribution of trait values in a given population can be expressed as the addition of the variances of its
genetic and the environmental effects \ref{eqn:PGE}.

\begin{equation}
  \sigma_{P} = \sigma_{G} + \sigma_{E}
  \label{eqn:PGE}
\end{equation}

The genomic and the environmental effects not only influence the phenotypic variance directly, but the
environment also has an influence on gene expression methylation of DNA bases etc. and therefore the equation
\ref{eqn:PGE} needs to be extend by the variance of the gene-environment interactions $\sigma_{GxE}$
\ref{eqn:PGGE} , \cite{lynch1998}, \cite{walsh2018}.
        
\begin{equation}
 \sigma_{P} = \sigma_{G} + \sigma_{E} + \sigma_{GxE}
 \label{eqn:PGGE}
\end{equation}

Equation \ref{eqn:PGGE} shows the decomposition of the phenotypic variance, to thoroughly understand complex
genetic architectures of traits the genetic variance needs to be decomposed further in its additive, dominance
and epistatic components \ref{eqn:GAD}

\begin{equation}
  \sigma_{G} = \sigma_{A} + \sigma_{D} + \sigma_{I}
  \label{eqn:GAD}
\end{equation}

The additive effects are caused by single, for this model mostly homozygous, loci while the variance caused by
dominance effects, is caused by heterozygous loci and their resulting interactions being full-, over- , co- or
underdominant. And lastly the interaction effects that are a result of two or more genes only having an impact
if the involved genes co-occur in a certain state. The resulting variance is commonly known
as gene-gene interactions and/or epistasis \cite{falconer1996}. \\
Since possible interactions in a genome can happen between additive or dominant or a combination of those
loci. The variance due to interaction effects $\sigma_{I}$ can be further dissembled in the variance resulting
from additive-additive $\sigma_{AA}$ dominant-dominant $\sigma_{DD}$ and additive-dominant $\sigma_{AD}$ terms as
represented in equation \ref{eqn:IAA}.


\begin{equation}
  \sigma_{I} = \sigma_{AxA} + \sigma_{DxD} + \sigma_{AxD}
 \label{eqn:IAA}
\end{equation}

Knowledge of the variance components involved in the expression of a trait in population, lead up to the
estimation of the total influence of all genetic variances and the environmental variance one the phenotypic
distribution. This concept is called heritability. The heritability of a trait $H^2$ accounts for the
proportion of the phenotypic variance controlled by the total genetic variance as shown in equation
\ref{eqn:h2G}. This is also referred to as broad sense heritability, because all genetic effects including
additive, dominance and epistatic effects are included \cite{brooker1999genetics}.


\begin{equation}
 H^2 = \frac{\sigma_{A} + \sigma_{D} + \sigma_{I}}{\sigma_{P}}
 \label{eqn:h2G}
\end{equation}

The concept of narrow-sense heritability \ref{eqn:h2a} is similar to the broad-sense heritability, but only
the additive genetic effects are included in the genetic part of the equation. This differentiation is import
for natural and artificial selection and thus is commonly used in evolutionary genomics and breeding. Because
in diploid species each parent only passes down on a single a allele of a given locus, dominance effects or
interaction effects are not commonly inherited from one parent. Therefore it is mainly the additive
genetic effects of a parent that influences its offspring. While the dominance and epistatic variances are
controlled by the combination of the parents \cite{falconer1996}, \cite{walsh2018}.

\begin{equation}
 h^2 = \frac{\sigma_{A}}{\sigma_{P}}
 \label{eqn:h2a}
\end{equation}



\subsection{Artificial selection in plant and animal breeding in the genomics era}
\subsubsection{Introduction to genomic selection }

Genomic prediction has been applied to almost all relevant crop and model species. Including:
\textit{A.thaliana} \cite{hu2015}; \cite{shen2013novel}. Alfalfa (\textit{Medicago sativa})
\cite{li2012applied}; \cite{annicchiarico2015accuracy}; \cite{li2015genomical}; \cite{biazzi2017genome};
\cite{hawkins2018recent}. Barley \cite{neyhart2019}; \cite{oakey2016}; \cite{zhong2009factors}. Cassava
(\textit{Manihot esculenta}) \cite{elias2018}; \cite{elias2018improving}. Cauliflower (\textit{Brassica
  olearacea spp}) \cite{thorwarth2018genomic}. Cotton (\textit{Gossiypium spp.}) \cite{gapare2018}. Maze
(\textit{Zea mays}) \cite{moeinizade2019}; \cite{allier2019usefulness}; \cite{brauner2018genomic};
\cite{schrag2018beyond}; \cite{schopp2017genomic}; \cite{e2017genomic}; \cite{schopp2017accuracy};
\cite{kadam2016genomic}; \cite{bustos2016improvement}; \cite{montesinos2015threshold};
\cite{owens2014foundation}; \cite{lehermeier2014usefulness}; \cite{technow2014genome};
\cite{peiffer2014genetic}; \cite{riedelsheimer2013genomic}; \cite{guo2013accuracy}; \cite{technow2013genomic};
\cite{windhausen2012}; \cite{rincent2012}. Potato (\textit{Solanum tuberosum}); \cite{enciso2018genomic};
\cite{Endelman2018pot}. Rape seed (\textit{Brassica naps}) \cite{wurschum2014potential};
\cite{jan2016genomic}; \cite{luo2017genomic}; \cite{werner2018effective}; \cite{snowdon2012potential};
\cite{qian2014sub}. Rice (\textit{Oryza sativa}) \cite{Momen2019}; \cite{BenHassen2018}; \cite{Xu2013rice};
\cite{Grenier2015}. Rye (\textit{Secale cerale}) \cite{auinger2016model}; \cite{bernal2014importance};
\cite{wang2014accuracy}; \cite{bernal2017genomic}; \cite{marulanda2016optimum}. Sugar beet (\textit{Beta
  vulgaris}),\cite{wurschum2013genomic}; \cite{biscarini2014genome}.  Sugar cane (\textit{Saccharum officinarum})
\cite{gouy2013experimental} Soybean (\textit{Glycine max}) \cite{Stewart_Brown_2019}; \cite{Jarquin_2016};
\cite{Xavier_2016}. Switchgrass (\textit{Panicum virgatum}) \cite{Poudel_2019}; \cite{Ramstein_2019};
\cite{Ramstein_2016}. Wheat (\textit{Triticum aestivum}) \cite{Cuevas_2019}; \cite{Howard_2019};
\cite{Krause_2019}; \cite{Rincent_2018}; \cite{Norman_2018}; \cite{Belamkar_2018}; \cite{Ovenden_2018};
\cite{Sukumaran_2016}; \cite{Bustos_Korts_2016}; \cite{Gianola_2016_wheat}; \cite{Crossa_2016_wheat};
\cite{Thavamanikumar_2015}; \cite{Lopez_Cruz_2015}. As well as various tree species \cite{deAlmeidaFilho2019};
\cite{Rincent_2018}; \cite{Kainer_2018}; \cite{Ratcliffe_2017}; \cite{GamalElDien_2016}; \cite{Kumar_2015};
\cite{Jaramillo_Correa_2014};
\cite{Zapata_Valenzuela_2013}; \cite{Holliday_2012}; \cite{Resende_2012}.\\
Even though GS finds broad application in plant breeding it has been originally developed for the use in
animal breeding \cite{hayes2010genome}; \cite{goddard2011using}. The gold standard is a method known as
genomic BLUP \cite{vanraden2008efficient} which utilizes a relationship matrix based on the co-occurrence of
genetic markers. This method is derived from the pre-genomic era in animal breeding where the relationship
matrix was constructed after pedigrees according to the best linear unbiased predictors based linear mixed
models developed by \cite{henderson1975best}. GBLUP accounts only for additive-genetic effects
\cite{vanraden2008efficient}. There are other methods that are able to account for more complex genomic
effects that are non-additive. Popular among those are reproducing Kernel Hilbert Spaces (RKHS)
\cite{gianola2008reproducing}. Alternatively to linear mixed models a variety of different Bayesian methods
became popular, basically differing in the degree of shrinkage of the assumed marker distribution
\cite{hayes2001}; \cite{gianola2009}; \cite{habier2011}; \cite{gianola2013}; \cite{crossa2017};
\cite{azodi2019}.

\subsubsection{Genomic selection in recurrent selection and the breeders equation}

While the quantitative genetic methods breeders utilize are complex their goals can be defined in one
sentence: To genetically improve plant germplasms for agriculture. The breeding process started at the same
time as farming was first practiced ~ 10.000 BC in the region between the Euphrat and Tigris rivers known as
the fertile crescent. This changed the phenotypic appearance of the early crops dramatically to the point that
they share little external traits with their wild ancestors. Those changes have also been deeply carved into
the genomes, that underwent serious alterations, including hybridization, duplications etc. Leading to most
crop plants not having any wild ancestors with whom they could naturally mate. For example wheat
(\textit{Triticum aestivum}), one of the three most important sources of food on a global scale underwent
multiple hybridization steps \cite{ozkan2001allopolyploidy}. Wheat is a hybrid from either the diploid emmer
(\textit{T. diccoides}) or durum wheat (\textit{T. durum}) and \textit{{Aegilops tauschii}}, while emmer and
durum are hybrids derived from wild emmer which is a hybrid of wild grass of the genus of \textit{Aegilops}
and \textit{T. urata} \cite{friebe2000development}; \cite{feldman2012genome}. While being ignorant of modern
genetics early ``plant breeders'' must have had an intuitive, yet naive, understanding of the general concept
of heritability in a way that they must have established that offsprings share properties of their parent
generation, which lead to regrowing individuals with desired traits generations after generation. This lead to
many changes including that artificial selected plants are commonly largely inbred. This process could be
considered an early form of recurrent truncation selection. Truncation selection on a normal distributed
phenotype is shown in figure \ref{fig:trunSel}.
 
\begin{figure}[H]
   \centering \includegraphics[height=.25\textheight, width=0.6\textwidth]{Figures/truncSel} \decoRule
   \caption[Truncation selection of a normal distributed phenotype]{Truncation selection from a normal distributed phenotype with a threshold value of $T$, $\mu$ as the mean of the total population and $\mu^{\ast}$ as the mean of the selected phenotypes. Graphic from \cite{walsh2018short}}
 \label{fig:trunSel}
\end{figure}


Like the early breeders modern breeders of to determine the selection threshold $T$ to divide the total population with the mean $\mu$ into two groups: the individuals culled and the ones allowed to reproduce with the mean $\mu^{\ast}$. The difference between those two is the selection differential $S$:

\begin{equation}
  S =  \mu^{\ast} - \mu
\label{eqn:S}
\end{equation}

which in the case of normal distributed data as depicted in figure \ref{trunSel} can be expressed as:

\begin{equation}
S = \varphi (\frac{T - \mu}{\sigma}) \frac{\sigma}{p}
\end{equation}

From which we can obtain the selection intensity $i$, which makes $i$ solely a function of $p$ 

\begin{equation}
i = \frac{S}{\sigma} = \frac{\varphi (z_{|1-p|})}{p}
\end{equation}


With recurrent truncation selection over many generations the population mean of the trait $\mu$ will change
(hopefully in the desired direction), if the heritability (in this case the narrow sense heritability) $h^2$ >
0. It is impossible to breed for traits that do not contain any genetic components in its architecture
\cite{walsh2018}.  Next to $i$ the intensity and $h^2$ the additive portion of the heritability the accuracy
of the selection $r_{uA}$ is important for the success of a breeding program. Those three terms can be applied
to compute the gain of selection $R$ over one generation (equation \ref{eqn:Breeders}). Due to its importance
in the evaluation of breeding schemes it is known as the breeder's equation \cite{mousseau1987natural};
\cite{falconer1996}; \cite{kingsolver2001strength}.

\begin{equation}
  R = i r_{uA} \sigma_A
\label{eqn:Breeders}
\end{equation}

The accuracy $r_{uA}$ of equation \ref{eqn:Breeders} in cases only phenotypic selection is conducted the
heritability and in cases where the selection process is aided by genomic prediction it is the prediction
accuracy.  According to breeder's equation there are three parameters which can be influenced through genomic
prediction. \newline (i) The prediction accuracy, which is usually smaller than the heritability, varies for
different prediction equations and an increase in the accuracy will lead to an proportional increase in $R$
the gain per generation cycle. For this reasons since 2001 in quantitative genetics one very active field of
research was and still is to find new algorithms that are superior to others as presented in the next chapter
(\ref{blup:bayes}.  As later evaluated on more than 150 phenotypes in chapter \ref{gp:res} $h^2$ is always
larger than $r_{uA}$.  Which if it was the only variable factor in equation \ref{eqn:Breeders} make genomic
selection inferior to phenotypic selection. Which from a certain point of view it is. Phenotypic trials are
better approximation for phenotypic appearance as GEBVs. However as the cost of genotyping have decreased
dramatically in the last 20 years. Phenotyping in the field remains tedious, laborious and mostly vastly
expensive. Taking into account that field trials have to be repeated in several years and locations to produce
reliable accounts it becomes clear that genotyping 10th of thousands of accessions is much cheaper than
conducting field trials with 1000 of them. \newline (ii) The selection intensity can much stricter if the
total population that is selected from is larger and in genomic prediction settings they are, because it
allows selection from two pools. The first the pool of plants with known phenotyped \underline{and} known
genotype and from those were just genomic data is available. When selecting from a pool of 1000 with $p=0.05$
with the goal to keep 50 plants in the next breeding cycle, the same goal can be reached when genomically
selecting from a pool of 10000 with and intensity of $p=0.05$. \newline (iii) The decreased time per
generation is probably the largest advantage of genomic selection when applied to breeding. While in field
trials it is only possible to have one generation per year. Genomic selection does not require the plants to
be grown in the field. For selection it is only necessary to grow enough so that DNA can be extracted from the
tissue and evaluated. After selection only the ones above the threshold are grown until they bear seats (or
any other reproductive organ) and be used for the next selection cycle, allowing up to ten generations per
year. This development hast lead to the rise to a new branch of breeding: speed breeding
\cite{ghosh2018speed}; \cite{watson2018speed}.  In practical, company-level genomic prediction as largely
contributed to an increase by a factor of 2 of the gain in selection in recent years  (personal
communication with breeding company employees ).\newline The last term in equation \ref{eqn:Breeders} the additive genetic variance $\sigma_A$
is not directly yet heavily influenced by the described breeding scheme. Artificial selection has similar
effects on the genetic variance as bottlenecks in natural selection have: it decreases, thus making it harder
to increase $R$ in later selection cycles \cite{walsh2018}.


\subsubsection{Genomic BLUP and Bayesian methods}\label{blup:bayes}

All methods share a common statistical obstacle which is commonly referred to as the $n >> p$ problematic,
which arises because the number $n$ marker is usually significantly larger than the number of observations
$p$. In practical applications it is not uncommon the have more than 100k markers while the number of
phenotypes is no larger than 100. This does not allow to obtain genomic estimated breeding values (GEBV) by
single marker regression as done by GWAS, which have highly inflated over all SNP-effects
\cite{korte2013advantages}. One possible is to include effect sizes as random effects and make prior
assumptions about their distribution. The difference in prior distribution is the main distinction between the
methods of the Bayesian alphabet
\cite{gianola2013}.\\

\textbf{Genomic BLUP} \newline In the early years of research on genomic prediction algorithms they were not
solely benchmarked against each other, but had to compete with the previously popular pedigree
methods. Quickly in the course of the first decade of this millennium the superiority of the genomic method
became elucidated in livestock and plant breeding \cite{habier2007impact}; \cite{vanraden2008efficient};
\cite{vanraden2008reliability}; \cite{harris2009genomic}.  While the genomic methods are superior to
non-genomic methods, there is no clear evidence that either of the genomic methods are superior to each other
and there is lack of empirical evidence that the Bayesian methods generally outperform GBLUP
\cite{moser2009comparison} ; \cite{bernardo2010breeding}; \cite{azodi2019}.  Alike in the pedigree BLUP in
genomic BLUP the co-variance between related individuals is used for the prediction. In the later case it is
constructed from marker information. In the GWAS terminology the relationship matrix is referred to as $K$ for
kinship in GWAS literature, while in GS circumstances it is also called GRM (genomic relationship matrix) or
simply abbreviated as $G$. This study will remain consistent with the circumstantial literature and therefore
purposely inconsistent within itself. In the chapter addressing GWAS the it will be called $K$ for kinship
matrix and it the following chapter elucidating GBLUP it will be referred to as $G$.  The general genomic
prediction model \ref{eqn:blup} is derived from the mixed model \cite{henderson1975best};
\cite{vanraden2008efficient} and implemented as:

\begin{equation}
Y = X \beta  +  Zu + \varepsilon
  \label{eqn:blup}
\end{equation}

where $Y$ is and $nx1$ vector of phenotypic observations, $X$ the matrix of the fixed effects and $\beta$ the
vector of the fixed effects. $Z$ the incidence matrix for the combined marker effects and $u$ a $(nx1)$ vector
of the additive genetic effect the vector of the residuals $\varepsilon$.  To construct the model lets assume
a matrix of size $(n x m)$ with $n$ individuals and $m$ loci $M$ containing marker information for 3
individuals on 4 loci, thus being of size $3x4$. The 4 markers of matrix \ref{arr:M} can take values of $-1$,
$0$ and $1$ translating into minor allele, heterozygous locus and major allele. The following example
calculation has been adapted from \cite{isik2013}.

\begin{equation}
  M = 
  \begin{pmatrix}[r]
    -1 & 0 &  1 & -1 \\
    -1 & 0 &  0 &  0 \\
     0 & 1 &  1 & -1 
  \end{pmatrix}
  \label{arr:M}
\end{equation}

The $M$ matrix contains all the information that is necessary for the computation of the K matrix and other viable genetic parameters. The $MM'$ matrix of size $n x n$ (\ref{MM'}) bears additional parameters.

\begin{equation}
  MM' = 
  \begin{pmatrix}[r]
    3 &  1 & 2 \\
   -1 &  1 & 0 \\
    2 &  0 & 3 
  \end{pmatrix}
  \label{arr:MM'}
\end{equation}


The diagonal show the number of homozygous loci per individual, while the other elements of the matrix
indicate the number of markers shared by related individuals and is an indicator for the distance of the
relationship as defined by identity-by-descent \cite{vanraden2008efficient}; \cite{misztal2013methods}.  While
matrix \ref{arr:MM'} calculates the metrics per individual the $M'M$ matrix (\ref{arr:M'M}) counts those
metrics per marker. Likewise the diagonal contains the number of homozygous individuals per marker.

\begin{equation}
  M'M = 
  \begin{pmatrix}[r]
    3 & -1 & 0 & 0 \\
   -1 &  1 & 1 & 1 \\
    0 &  1 & 2 & 1 \\
    0 &  1 & 1 & 2 
  \end{pmatrix}
  \label{arr:M'M}
\end{equation}

The next step is to obtain a matrix of the allele frequencies at each locus of the size ($n x m$) as matrix
M. For the design of matrix P (\ref{arr:P}) let the minor allele frequencies $p_1 \dots p_4$ be
$\{0.3, 0.2, 0.1, 0.15\}$. The allele frequency of the $i^{th}$ column of $P$ is expressed, according the $,^{th}$ marker of matrix $M$ as $P_i = 2(p_i - 0.5)$ resulting in: 

\begin{equation}
  P = 
  \begin{pmatrix}[r]
   -0.4 & -0.6 & -0.8 & -0.7 \\
   -0.4 & -0.6 & -0.8 & -0.7 \\
   -0.4 & -0.6 & -0.8 & -0.7
  \end{pmatrix}
  \label{arr:P}
\end{equation}

The allele frequencies as in this simulated example should be taken from the entire population and not the
subsample used in this calculation \cite{vanraden2008efficient}.  The final step to obtain the Z matrix of
equation \ref{eqn:blup} is to subtract the P matrix from the M matrix $Z= M - P$ resulting in:

\begin{equation}
  Z = 
  \begin{pmatrix}[r]
    1.4 & 0.6 & 1.8 & -0.3 \\
   -0.6 & 0.6 & 0.8 & 0.7 \\
    0.4 & 1.6 & 1.8 & -0.3 \\

  \end{pmatrix}
  \label{arr:Z}
\end{equation}

In $Z$ the mean values of the allele effects are set to 0 and the subtraction of $P$ emphasizes the effect of
rare variants \cite{vanraden2008efficient}.  There is a large variety of methods to generate the genomic
relationship matrices and here lies the major difference between different genomic BLUP methods. But they all
have in common that K is always of size $n x n$.

(i) The naive is approach to iterate over each individual and count the common markers with every other
individual. This approach is not uncommon and suited for inbred or doubled-haploid populations, less so for
outcrossed populations with high degrees of heterozygosity because in the sample implementation it does only
account for homozygous loci. This method becomes computationally intense when the data sets grow larger as
common today (personal observation).

(ii) Probably the most popular method in GS is to obtain $K$ as proposed by \cite{vanraden2008efficient} designed after Wright's \cite{wright1922coefficients} equations for the covariance in structured populations, as described by equation \ref{eqn:vanraden} with $Z$ as in \ref{arr:Z}.

\begin{equation}
  G = \frac{ZZ'}{2 \Sigma p_i (1-p_i)} 
\label{eqn:vanraden}
\end{equation}

(iii) The unified additive relationship $G_{UAR}$ according to \cite{yang2010common} and equation \ref{eqn:uar}

\begin{equation}
  G_{UAR} =  A_{jk} = \frac{1}{N} \Sigma_i{A_{ijk}} = \left\{
    \!\begin{aligned}
      \frac{1}{N} \Sigma_{i} \frac{(x_{ij} - 2p_i)(x_{ik} - 2p_i)}{2p_i  (1-p_i)}, j \ne k \\
      1 + \frac{1}{N} \Sigma_i \frac{x_{ij}^2 (1+2p_i) x_{ij} + 2p_i^2 }{2p_i (1-p_i)}, j = k     
    \end{aligned}
  \right.
  \label{eqn:uar}
\end{equation}

where $p_i$ is the allele frequency at locus $i$ and $x_{ij}$ the genotype for the $j^{th}$ individual at the
$i^th$ locus. Another method also proposed by \cite{yang2010common} is to adjust $G_{UAR}$ with $\beta$ as in
equation \ref{uaradj}

\begin{equation}
  G_{UARadj} = \left\{
    \!\begin{aligned}
      \beta A_{jk}, \;\; j \ne k \\
      1 + \beta (A_{jk}-1 ), \;\; j = k      
    \end{aligned}
    \right.
  \label{eqn:uaradj}
\end{equation}

(iv) Another approach is to weigh marker by the reciprocals of their expected variance according to the model
\ref{eqn:ZDZ} originally designed to investigate population structures in human data
\cite{leutenegger2003estimation}; \cite{amin2007genomic}.

\begin{equation}
  \!\begin{aligned}
    G = ZDZ' , with \\
    D_{ii} = \frac{1}{m | 2p_i(1-p_i) | }
  \end{aligned}
  \label{eqn:ZDZ}
\end{equation}


(v) Other methods like the gaussian kernel compute kinship between individuals by the euclidean distance
between the respective genotypes \cite{morota2014kernel}.

\begin{equation}
  \!\begin{aligned}
    K(x_i,x_j) = exp (- \theta d_{ij}^2) \\
    = \prod_{k=1}^m exp (- \theta(x_{ik} - x_{jk})^2)
  \end{aligned}
  \label{eqn:gauss}
\end{equation}

with $d_{ij} = \sqrt{(x_{i1} -  x_{j1})^2 + \dots + (x_{ik} - x_{jk})^2 + \dots +  (x_{im} - x_{j,a})^2 }$ and
$ x_{ik}(i,j = 1, \dots , n,k = 1, \dots , m)$ and $x_{ik}$ as the $i^{th}$ individual at SNP $k$. \\


The linear model in equation \ref{eqn:blup} $Y = X \beta + Zu + \varepsilon$ with $\beta$ as the vector fixed
effects and $u$ as the vector of additive genetic effects.
The mixed model can be solved to obtain genomic estimated breeding values as:


\begin{equation}
  \begin{pmatrix}[ccc]
    X'X & X'Z & 0 \\ 
    Z'X & Z'Z + G^{11} & G^{12} \\ 
    0 & G^{21} & G^{22} \\ 
  \end{pmatrix}
  \begin{pmatrix}[r]
    \hat{b} \\ 
    \hat{y}_1 \\ 
    \hat{y}_2 \\ 
  \end{pmatrix}
  =
  \begin{pmatrix}[r]
    X'y \\ 
    Z'y \\ 
    0 \\ 
  \end{pmatrix}
  \label{eqn:pBLUP}
\end{equation}

with $G^{12}$ as the part of $G^{-1}$ containing individuals \underline{with} phenotypic data.
with $G^{22}$ as the part of $G^{-1}$ containing individuals \underline{without} phenotypic data and just SNP  information. 

The GEBV of the unknown phenotypes $\hat{y}_2$ can thus be predicted as:

\begin{equation}
\hat{y}_2 = -\left( G^{22}\right)^{-1}G^{21}\hat{y}_1
\label{eqn:gpred}
\end{equation}

GBLUP is fairly easy compared to more complex Bayesian methods and be quickly implemented in any programming
language capable of solving liner equations. Computational as the the number of phenotypes in the study
increases in numbers the timed demand grows exponentially, because the kinship matrix quadruples in size and
it becomes more complicated to compute the inverse of $G$ (personal observations). \\

\textbf{Bayesian methods} \newline 

Next to the universal GBLUP a set of related algorithms became popular for solving the mixed models involved in genomic selection, known as the Bayesian alphabet \cite{gianola2009}; \cite{gianola2013}. They are all based on Bayes' fundamental theorem in equation \ref{eqn:bayes}

\begin{equation}
P(\theta | y) = \frac{P(\theta )P(y | \theta)}{P(y)}  
\label{eqn:bayes}
\end{equation}

with $P(\theta )$ as the prior distribution, $P(y|\theta )$ as the likelihood and $P(y)$ as the marginal density of $y$. The prior distribution in GS assume that $y$ was drawn from a certain distribution. Infinitesimal models assume that the genetic effects follow a normal distribution \cite{legarra2018}. The Bayesian frameworks however will assume non-normal distributed marker effects. This can be explained by a two-step hierarchical distribution.
Stage one  assumes that every marker has \textit{a priori} a different variance \cite{legarra2018}.

\begin{equation}
p(a_i|\sigma_{ai}^2) = N (0,\sigma^1_{ai})
  \label{eqn:stageonbayes}
\end{equation}

The second stage assumes prior distributions for the variances.

\begin{equation}
p(a_i| variable ) = P(\dots )
  \label{eqn:stagetwobayes}
\end{equation}

with $variable$ standing for the large variety of prior distribution. In total there are more than >20
different Bayesian models known to the authors with unknown number of different methods proposed. Their main
difference ``simply'' lies in the a priori assumptions. This change can make some methods mathematically much
more advanced then others. And as shown in later chapters none of the methods is completely superior over
others in terms of prediction accuracy. Approximation to the solution of the linear equations is usually performed by Gibb's sampling using Markov Chain Monte Carlo (MCMC) simulations \cite{dlc2009}; \cite{BGLR}.
Table \ref{tab:bayesABC} summarizes commonly applied Bayesian methods for genomic prediction indicating the key differences between them. 

\begin{table}[H]
\caption{Overview of properties of a variety of commonly applied Bayesian methods for genomic prediction. Table altered after \cite{karkkainen2012back}}
\label{tab:bayesABC}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{lcccccc}
  \toprule
  Name      & Reference & Prior & Indicator & Hierarchy & Hyperprior & Estimation \\
  \midrule
  \hline 
  BayesA       & \cite{hayes2001}           & Student & No  & Yes & No  & MCMC \\
  BayesB       & \cite{hayes2001}           & Student & Yes & Yes & No  & MCMC \\
  BayesC       & \cite{verbyla2009accuracy} & Student & Yes & Yes & No  & MCMC \\
  BL           & \cite{xu2010expectation}   & Laplace & No  & Yes & No  & EM   \\
  BayesD$\pi$  & \cite{habier2011}          & Student & Yes & Yes & Yes & MCMC \\
                            \bottomrule                                                   
                          \end{tabular}}
\caption*{The name is given by the author. The prior column tells which shrinkage prior is used. }
\end{table}


\subsubsection{cross validation}



\subsection{Genomic selection using artificial neural networks }
Genomic selection (GS) has been successfully applied in animal \cite{gianola2015one}; \cite{hayes2010genome}
and plant breeding \cite{crossa2010}; \cite{desta2014genomic}; \cite{heffner2010plant};
\cite{crossa2017} as well as in medical applications, since it was first reported
\cite{hayes2001}. Since then the repertoire of methods for predicting phenotypic values has increased rapidly
e.g. \cite{dlc2009}; \cite{habier2011}; \cite{gianola2013}; \cite{crossa2017}. The most commonly applied
methods include GULP and a set of related algorithms known as the bayesian alphabet
\cite{gianola2009}. Genomic prediction in general has repeatedly been shown to outperform pedigree-based
methods \cite{crossa2010}; \cite{albrecht2011} and is nowadays used in many plant and animal breeding
schemes. It has also been shown that using whole-genome information is superior to using only feature-selected
markers with known QTLs for a given trait \cite{bernardo2007}; \cite{heffner2011} in some cases. A more recent
study \cite{azodi2019} compared 11 different genomic prediction algorithms with a variety of data sets and
found contradicting results, indicating that feature selection can be usefull in some cases the when the whole
genome regression is performed by neural nets 1 While every new method is a valuable addition to the tool-kits
for genomic selection, some fundamental problems remain unsolved, of which the n>>p problematic stands
out. Usually in genomic selection settings the size of the training population (TRN) with n phenotypes is
substantially smaller than the number of markers ($p$) \cite{fan2014challenges}. Making the number of features
immensely large, even when SNP-SNP interactions are not considered. Furthermore each marker is treated as an
independent observation neglecting collinearity and linkage disequilibrium (LD). Further difficulties arise
through non-additive, epistatic and dominance marker effects. The main problem with epistasis issue
quantitative genetics is the almost infinite amount of different marker combinations, that cannot be
represented within the size of TRN in the thousands, the same problems arises for example in GWA studies
\cite{korte2013advantages}. With already large p the number of possible additive SNP-SNP interactions
potentiates to $p^{(p-1)}$. Methods that attempt to overcome those issues are EG-BLUP, using an enhanced
epistatic kinship matrix and reproducing kernel Hilbert space regression (RKHS) \cite{jiang2015};
\cite{martini2017genomic}.

In the past 10 years, due to increasing availability of high performance computational hardware with
decreasing costs and parallel development of free easy-to-use software, most prominent being googles library
TensorFlow \cite{TF2016} and Keras \cite{keras2015}, machine learning (ML) has experienced a renaissance. ML
is a set of methods and algorithms used widely for regression and classification problems. popular among those
are e.g. support vector machines, multi-layer perceptrons (MLP) and convolutional neural networks. The machine
learning mimics the architecture of neural networks and are therefore commonly referred to as artificial
neural networks (ANN). Those algorithms have widely been applied in many biological fields \cite{min2017deep}; \cite{lan2018survey}; \cite{mamoshina2016applications}; \cite{angermueller2016}; \cite{webb2018deep};
\cite{rampasek2016tensorflow}.

A variety of studies assessed the usability of ML in genomic prediction \cite{gonzalez2018applications};
\cite{gonza2016}; \cite{ogutu2011comparison}; \cite{montesinos2019benchmarking}; \cite{grinberg2018evaluation}
; \cite{cuevas2019deep}; \cite{montesinos2019new}; \cite{ma2017deepgs}; \cite{qiu2016application};
\cite{gonza2012} \cite{li2018genomic}. Through all those studies the common denominator is that there is no
such thing as a gold standard for genomic prediction. No single algorithm was able to outperform all the
others tested in a single of those studies, let alone in all. While the generally aptitude of ML for genomic
selection has been repeatedly shown, how no evidence exists that neural networks can outperform or in many
cases perform on that same level as mixed-model approaches as GBLUP \cite{hayes2001}. While in other fields
like image classification neural networks have up to 100s of hidden layers \cite{he2016deep} the commonly used
fully-connected networks in genomic prediction of 1 - 3 hidden layers. With 1 layer networks often being the
most successful among those. Contradicting to the idea behind machine learning in genomic selection 1 hidden
layer networks will be inapt to capture interactions between loci and thus only account for additive
effects. As shown in \cite{azodi2019} convolutional networks perform worse than fully-connected networks in
genomic selection, which again is contradicting to other fields where convolutional layers are applied
successfully, e.g natural language processing \cite{dos2014deep} or medical image analysis
\cite{litjens2017survey}. Instead of using convolutional layers and fully-connected layers only, as show in
Pook et al 2019, we also propose to use locally-connected layer in combination with fully-connected
layers. While CL and LCL are closely related they have a significant difference. While in CL weights are
shared between neurons in LCLs each neuron as its own weight. This leads to a reduced number of parameters to
be trained in the following FCLs, and should therefore theoretically lead to a decrease in overfitting a
common problem in machine learning. To evaluate the results of Pook et al. 2019 accomplished with simulated
data we used the data sets generated in the scope of the 1001 genome project of \textit{Arabidopsis thaliana}
\cite{1001genome}


\section{Proof of concept for ANN-based genomic selection} \label{POC}

Having established the quantitative architecture of traits in section \ref{quan} an the basics of machine
learning and neural nets in section \ref{introml}, that knowledge can be used to provide a proof of concept
that neural networks are a candidate for GP.  Table \ref{tab:simmarker} provides also the possible genotypes
that can be derived by two bi-allelic markers $G_1 \dots G_4$ on a fictional haploid organism. In this
simulation the effect sizes for each marker $\beta_1$ and $\beta_2$ are constant with a value of 1.

\begin{table}[H]
  \caption{Simple simulated phenotypes and genotypes for genomic prediction with genotypes $G_1 \dots G_4$, $M_1$ and
    $M_2$ and phenotypes based on additive effects or $and$, $or$, $xor$ logic gates.}
\label{tab:simmarker}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $Y_{ADD}$ & $Y_{AND}$ & $Y_{OR}$ & $Y_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0 & 0 & 0 & 0 \\
  $G_2$ & 0 & 1 & 1 & 0 & 1 & 1 \\
  $G_3$ & 1 & 0 & 1 & 0 & 1 & 1 \\
  $G_4$ & 1 & 1 & 2 & 1 & 1 & 0 \\
  \bottomrule
\end{tabular}
\end{table}

The four phenotypes $Y_{ADD}$, $Y_{AND}$, $Y_{OR}$ and $Y_{XOR}$ , which were derived from their respective
marker effects. $Y_{ADD}$ is a phenotype with purely additive effects. So in the nomenclature introduced in
chapter \ref{quan} $\sigma_A = \sigma_G$ and $\sigma_{I} = 0$. Since the hypothetical organism is haploid
there are dominance effects to be accounted for $\sigma_D = 0$. Since all the genetic effects are caused by
additive effects and there are now environmental effects $\sigma_E$, the narrow sense heritability $h^2$ -
equation \ref{eqn:h2a} - and the broad sense heritability $H^2$ - equation \ref{eqn:h2G} - are equally 1. The
other three phenotypes are base on epistatic effects $\sigma_I$ generated by passing the markers $M_1$ and
$M_2$ through their respective logic gates. This theoretically in results in $h^2 = 0$ and $H^2 = 1$, because
there are no additive effects. For $y_{AND}$ however $h \approx 0.5$, because there is a correlation between
$Y_{ADD}$ and $Y_{AND}$. In practical applications this allows methods like GBLUP, designed to account for
additive genetic effects to capture some of the epistatic effects of $\sigma_I$ \cite{vieira2017assessing}.

According to chapter \ref{introml} a single perceptron would fail to solve $xor$ gates. While a network with
multiple nodes and layers should be able to overcome that deficit. A relatively simple neural network with two
fully-connected hidden layers with 10 and 5 nodes, was trained for the prediction of each phenotypes. To keep
the simulation as possible, no regularization parameters, dropout etc. was included. The activation function
was ReLU (\ref{eqn:relu}) with an Adam optimizer. The results of the prediction are shown in table
\ref{tab:simgpres}.

\begin{table}[H]
\caption{Results of genomic prediction from phenotypes and genotypes in table \ref{tab:simmarker}}
\label{tab:simgpres}
\centering
\begin{tabular}{ l c c | c c c c c c }
  \toprule
  & $M_1$ & $M_2$ & $\hat{Y}_{ADD}$ & $\hat{Y}_{AND}$ & $\hat{Y}_{OR}$ & $\hat{Y}_{XOR}$\\
  \midrule
  \hline 
  $G_1$ & 0 & 0 & 0.01 & 0.00 & 0.00 & 0.01 \\
  $G_2$ & 0 & 1 & 0.99 & 0.01 & 0.99 & 0.98 \\
  $G_3$ & 1 & 0 & 0.99 & 0.00 & 0.99 & 1.01 \\
  $G_4$ & 1 & 1 & 1.99 & 0.98 & 1.01 & 0.02 \\
  \bottomrule
\end{tabular}
\end{table}


Not surprisingly, the simple network is able to solve all four phenotypes and predicting the phenotypes
accurately. The task was rather easy because the training data set and the testing data set were the same, but
it served the purpose of showing that neural networks a generally apt to solve different marker
interactions. \textit{In natura} those interactions and the overall genetic architecture is much more
complex. Effect sizes are not constant and epistasis may be caused be interactions my more than just two
markers, and with an increasing number of markers $n$ the number of possible two-way interactions increases
even more so to $2^{n-1}$. Smaller interaction effects could be obscured under larger additive effects,
gene-environment might have a significant influence leading to a model that does not converge.


\section{Material}
Two different data sets were used for the genomic prediction trials.
A set of maze doubled-haploid (DH) populations, derived from MAZE landraces. And \textit{A. thaliana} data sets, with genomic data procured along the 1001 genomic project \cite{1001genome} and various phenotypic trials.


\subsection{DH populations derived from maize  landraces}
The DH populations were produced, propagated and phenotyped in the scope of the MAZE project phase I, funded
by the Federal Ministry of Education and Research (BMBF) (Funding ID: 031B0195, project “MAZE”) as well as the
KWS SAAT SE, by various project partners at the Technical University of Munich, University of Hohenheim and
the KWS. A thorough description of the germplasm selection and phenotyping was recently published by
\cite{holker2019european}. \newline Modern maize cultivars are almost exclusively high-performing hybrids from
two inbreed lines from different heterotic pools. Commonly hybrids are derived from the cross European Flint x
American Dent \cite{dos2004priori}; \cite{brauner2019testcross}. Before hybrid breeding became the predominant
method in maze breeding in the 1960s landraces were grown by farmers. Landraces are dynamic, open-pollinated,
locally highly-adapted populations. That did not derive from modern breeding but from locally confined
selection and adaption by farmers to often very specific needs \cite{arteaga2016genomic}. They hybrids grown
today are derived from just a few landraces as founder lines, while the majority of landraces has been nearly
forgotten. This and high intensity selection over many generation has led to a loss of genetic diversity
$\sigma_G$ in modern maize cultivars. Therefore the landrace germplasm present a important, essential stock of
genetic variability for continuous success in maize breeding. The utilization of those germplasms would be
impossible without the invaluable work of institutions as the IPK Gartersleben whose goal as genebanks is to
maintain and store genetic material for long time periods.  Of those landraces three, representing large
phenotypic heterogentiy were chosen: (i) Kemater Landmais Gelb (KE, Austria), Petkuser Ferdinand Rot (PE,
Germany) and Lalin (LL, Spain). They represent 95 \% of the molecular variance if a set of
35 landraces analyzed in a preceding project by \cite{mayer2017there}.\newline
In total 1015 DH lines (516 KE, 432 PE, 67 LL) were produced with \textit{in vivo} haploid induction with an inducer line as described in \cite{roeber2005vivo}.


\subsubsection{Genomic maize data}
(The genomic data was provided by the TUM as described by \cite{holker2019european}) \newline Genotyping was
performed with the 600k Affymetrix\textsuperscript{\textregistered} Axiom\textsuperscript{\textregistered}
Maize array \cite{unterseer2014powerful}. The markers were quality filters and missing values were imputed
individually for each landrace population using Beagle 5.0 \cite{browning2007rapid};
\cite{browning2018one}. After LD pruning and further quality control 29833 markers remained for 471 Kemater
and 403 PE DHs. LL was excluded from further analyses due to insufficient amounts of genotypes.

\subsubsection{Phenotypic maze data}
(The phenotype data was provided by the TUM as described by \cite{holker2019european}) \newline The traits
were evaluated with lattice design in 6 different locations across Europe.  Those traits were: Early Vigor
(EV) at three different stages (V3, V4, V6); Plant height (PH) at two stages (V4,V6) and the final plant
height (PH\_final), as well as male and female flowering time (Days till Tasseling (DtTAS) and Days till
Silking (DtSILK)) and root lodging (RL).  To account for GxE best linear unbiased estimators were calculated
according to Henderson's model \cite{henderson1975best} and use for further prediction.

\subsubsection{Single environment prediction}
Next to the across environment BLUEs used for prediction the single environment BLUEs were used for prediction
to able to gain insights of the structure of $\sigma_{GxE}$ of the maize traits. This resulted in 2246
genotype x environment combinations for Kemater and 1975 for Petkuser with at least one data point.  This
number is lower than the maximum number of n DHs per populations times the 6 environments, because naturally
not all genotypes yielded reliable data in the environments. Each DH x environment was treated as an
individual in for the across environment prediction. The marker matrix was enhanced with the environmental
origin as cofactors as show in table \ref{tab:envmarker} with one-hot encoded markers.

\onehalfspacing
\begin{table}[H]
  \centering
  \caption{Schematic representation of the enhanced genotype matrix for across environment prediction of maize phenotypes with DHs 1-2 with markers M 1-2 in environments E1-2}
  \label{tab:envmarker}
  \begin{tabular}{l|cccc}
    \toprule
           & M-1 & M-2 & E-1 & E-2 \\
    \midrule
    DH1-E1 & 0   & 1   & 1   & 0   \\
    DH2-E1 & 1   & 0   & 1   & 0   \\
    DH1-E2 & 0   & 1   & 0   & 1   \\
    DH2-E2 & 1   & 0   & 0   & 1   \\                                           
    \bottomrule
  \end{tabular}
\end{table}
\doublespacing



\subsection{A. thaliana}

\subsubsection{Genomic data}

The genomic data was generated during the course of the 1001 genome project of \textit{A. thaliana}
\cite{1001genome} producing completed sequenced and assembled genomes from 1035 genomes, along 600k marker
data for 1307 accessions with a small overlap between those groups resulting in a total of 2029 genotyped
accessions. With more than 10 mio. SNPs and Indels on the 5 chromosomes of e \textit{A. thaliana}. Imputation
of missing data and upsampling of the 600k subsets was performed with Beagle3 \cite{browning2007rapid}.  For
every one of the 164 phenotypes used for prediction subsets were sampled, LD pruned and MAF filtered. LD
pruning was executed with the R-package SNPRelate \cite{zheng2013tutorial} with a relatively strict LD
threshold of $0.65$ and $MAF > 10 $. This resulted in data sets with approximately 150.000 markers for each
phenotype.

\subsubsection{Phenotypic data}
A complete list of the phenotypes used can be found in Appendix \ref{AppendixB} with the according study
references.  The phenotypic trials ranged from 100 to more than 1000 accessions per data set \cite{atwell2010}; \cite{li2010}; \cite{strauch2015}; \cite{me2014}.


\section{Methods}

The theoretical backgrounds of the methods used for genomic prediction were described in section \ref{introml}
for the ANNs and section\ref{blup:bayes} for the Bayesian methods and GBLUP. The next sections are devoted to
explaining who those methods were adapted and implemented for the prediction of maize and \textit{Arabidopsis}
traits.

\subsection{Validation scheme} \label{cv}

The validation approach in this study was a little different than common 5 fold cross validation. All
predictions were run 50 times with different splits of TST and TRN. For the full data set randomly 20\% were
assigned to TST and 80\% to TST. This process was repeated 50 times, reducing the chance of biases due to any
TST-TRN combination being randomly more predictable for on or the other method. The validation scheme was
generated \textit{a priori} and stored in cross-validation files to allow reusing the validation sets.


\subsection{ANN}
The scripts for ANN based GS were written in python using the lower level API TensorFlow \cite{TF2016} and the
higher level API Keras \cite{keras2015}. Both are very versatile, well-documented and are capable of
performing a large variety of machine learning applications. For those reasons they are the most used ML
libraries. Another advantage is that they work well on GPUs, which allows ML algorithms to run a reasonable
amount of time compared to CPU-based calculations. Prior to training the data was split into TRN and TST. The
markers of TRN served as the input layer for the network while the phenotypes were trained upon in the output
layer, which in the present cases consisted of only one node, because GS in the cases applied here is a
regression problem.  Preliminary trials showed Adam is the superior optimizer for GS and hence was the only
one further used. Likewise relu was the activation of choice being superior to sigmoid or other non
rectifiers. All the weights and the biases of the kernel were initialized with truncated normal distributed
values. The loss function used was always MSE. \newline Having a few hyperparameters fixed, the other ones
were optimized via grid search.  For each TRN multiple networks were trained to fine tune the input
parameters. Those were the number of layers, the nodes per layer, the magnitude of the dropout, the type of
dropout used, whether the first layer was locally-connected for fully-connected and the duration of training
via the training epochs. This amount to a total of a little shy of 260000 trained networks for the 146
\textit{A. thaliana} data sets alone. \newline After another set preliminary runs LCL as the first layer
seamed to result in higher accuracies then FLC as the first layer and where henceforth exclusively used and
applied with a stride length of 7. The stride length determines how many node of the input layer, in this case
markers, where combined in the first hidden layer.  The type of drop out used (alpha dropout, Gaussian noise
or normal dropout) did not show an effect therefore the normal dropout function was used.  The network
training was iterated over the different number epochs, architecture, drop out values the the cross validation
cycles, thus explaining the tremendous amount of total networks trained. Epochs from $5$ to $60$ in steps $5$
and several 1, 2 or 3 Layer architectures following the locally-connected layer.

\subsection{GBLUP}

The evaluation of the genomic BLUP was performed with the R-package BGLR \cite{BGLR}. To allow pairwise
comparison of the individual validation runs the same validation scheme as for the ANNs was used with the same
TST and TRN sets. 



\singlespacing
\section{Results} \label{res:gp}
\subsubsection{Results of \textit{A. thaliana} prediction}
\ref{AC:gp_res}
\begin{longtable}{p{.5\textwidth} p{.1\textwidth} p{.1\textwidth} p{.15\textwidth} p{.1\textwidth}}
  \hline
  Phenotype & GBLUP & ANN & Architecture & Epochs \\
  \hline
  YEL  & 0.9259 & 0.891 & 20, 10 & 20 \\
  FT16 & 0.8237 & 0.8215 & 100 & 10 \\
  2W & 0.8156 & \color{red}{0.8205} & 50, 30 & 35 \\
  FT10 & 0.8249 & 0.8191 & 48 & 50 \\
  LD & 0.8128 & \color{red}{0.8159} & 150 & 30 \\
  DTF sweden 2009 (1st experiment) & 0.8063 & \color{red}{0.8141} & 48 & 30 \\
  DTF sweden 2009 (2nd experiment) & 0.8035 & \color{red}{0.8091} & 50, 30 & 20 \\
  DTF sweden 2008 (2nd experiment) & 0.7986 & \color{red}{0.8057} & 150 & 25 \\
  4W & 0.795 & \color{red}{0.8052} & 50, 35, 15 & 30 \\
  FT22 & 0.8009 & \color{red}{0.8043} & 150 & 15 \\
  DTF spain 2008 (2nd experiment) & 0.7975 & \color{red}{0.8032} & 150 & 40 \\
  LN16 & 0.7996 & \color{red}{0.7999} & 50, 30 & 20 \\
  DTF spain 2009 (2nd experiment) & 0.7917 & \color{red}{0.7988} & 150 & 55 \\
  LDV & 0.8158 & 0.7975 & 150 & 15 \\
  0W GH FT & 0.7873 & \color{red}{0.7942} & 50, 30 & 15 \\
  DTFmainEffect2009 & 0.7794 & \color{red}{0.7855} & 50, 35, 15 & 35 \\
  SD & 0.7905 & 0.7848 & 48 & 30 \\
  DTFplantingSummer2008 & 0.75 & \color{red}{0.7746} & 50, 30 & 20 \\
  FT GH & 0.7693 & \color{red}{0.7702} & 50, 30 & 15 \\
  DTFlocSweden2009 & 0.7595 & \color{red}{0.7626} & 50, 30 & 60 \\
  DTFplantingSummer2009 & 0.7521 & \color{red}{0.7584} & 50, 30 & 50 \\
  0W & 0.7488 & 0.7473 & 48 & 40 \\
  DTF spain 2009 (1st experiment) & 0.7691 & 0.7425 & 48 & 40 \\
  DTF sweden 2008 (1st experiment) & 0.727 & \color{red}{0.728} & 50, 30 & 20 \\
  DTFlocSweden2008 & 0.7161 & \color{red}{0.7271} & 50, 30 & 55 \\
  Seed Dormancy & 0.7014 & \color{red}{0.7241} & 50, 30 & 35 \\
  DTFmainEffect2008 & 0.7102 & \color{red}{0.7142} & 50, 30 & 20 \\
  8W & 0.7259 & 0.7083 & 150 & 50 \\
  LN22 & 0.7004 & \color{red}{0.7069} & 50, 30 & 20 \\
  Size sweden 2009 (1st experiment) & 0.6905 & \color{red}{0.6994} & 48 & 50 \\
  LN10 & 0.6934 & \color{red}{0.698} & 50, 30 & 20 \\
  DTF spain 2008 (1st experiment) & 0.6944 & 0.677 & 150 & 25 \\
  SDV & 0.6775 & 0.6728 & 150 & 15 \\
  8W GH FT & 0.7001 & 0.6546 & 48 & 40 \\
  0W GH LN & 0.6568 & 0.654 & 50, 30 & 20 \\
  Storage 7 days & 0.6496 & \color{red}{0.65} & 50, 30 & 25 \\
  Storage 28 days & 0.6627 & 0.6483 & 50, 30 & 55 \\
  8W GH LN & 0.671 & 0.6434 & 48 & 70 \\
  Size sweden 2009 (2nd experiment) & 0.6114 & \color{red}{0.6268} & 48 & 50 \\
  SizeLocSweden2009 & 0.6144 & \color{red}{0.619} & 150 & 35 \\
  FLC & 0.6118 & \color{red}{0.6161} & 50, 30 & 30 \\
  LFS GH & 0.6178 & 0.6136 & 150 & 35 \\
  FT Field & 0.7324 & 0.6112 & 150 & 60 \\
  LY  & 0.6072 & \color{red}{0.6088} & 150 & 60 \\
  Storage 56 days & 0.6085 & 0.5788 & 150 & 15 \\
  LES & 0.56 & \color{red}{0.5764} & 150 & 50 \\
  M216T665 & 0.5155 & \color{red}{0.5674} & 50, 30 & 50 \\
  LC Duration GH & 0.5799 & 0.5664 & 150 & 55 \\
  M172T666 & 0.5165 & \color{red}{0.5487} & 150 & 60 \\
  Trichome avg JA & 0.588 & 0.5343 & 150 & 55 \\
  Secondary Dormancy & 0.5184 & \color{red}{0.5264} & 150 & 30 \\
  SizeMainEffect2009 & 0.52 & 0.5171 & 48 & 50 \\
  DSDS50 & 0.4754 & \color{red}{0.5006} & 50, 30 & 60 \\
  avrPphB & 0.5054 & 0.4942 & 150 & 60 \\
  Hypocotyl length & 0.4934 & 0.4807 & 150 & 50 \\
  Size spain 2009 (1st experiment) & 0.5121 & 0.4751 & 150 & 50 \\
  Yield spain 2009 (1st experiment) & 0.5205 & 0.4719 & 50, 30 & 50 \\
  Leaf serr 10 & 0.4636 & \color{red}{0.4683} & 150 & 55 \\
  Size spain 2009 (2nd experiment) & 0.471 & 0.4623 & 48 & 50 \\
  Trichome avg C & 0.4617 & 0.4385 & 48 & 40 \\
  Germ in dark & 0.4447 & 0.4382 & 150 & 15 \\
  YieldMainEffect2009 & 0.505 & 0.4345 & 150 & 30 \\
  FT Diameter Field & 0.5004 & 0.4274 & 150 & 15 \\
  Bacterial titer & 0.5406 & 0.417 & 150 & 55 \\
  FRI & 0.4011 & \color{red}{0.4119} & 48 & 30 \\
  Rosette Erect 22 & 0.3973 & 0.3934 & 48 & 30 \\
  Area sweden 2009 (1st experiment) & 0.4203 & 0.3895 & 50, 35, 15 & 30 \\
  Width 10 & 0.3932 & 0.3784 & 50, 30 & 60 \\
  Silique 22 & 0.4339 & 0.377 & 50, 30 & 50 \\
  avrRpt2 & 0.3757 & 0.3737 & 50, 30 & 30 \\
  M130T666 & 0.4381 & 0.3733 & 150 & 60 \\
  SizePlantingSummer2009 & 0.3769 & 0.3615 & 150 & 5 \\
  Area sweden 2009 (2nd experiment) & 0.359 & 0.3542 & 48 & 45 \\
  FW  & 0.3397 & \color{red}{0.3522} & 50, 30 & 25 \\
  P31 & 0.3632 & 0.3419 & 50, 30 & 45 \\
  MT GH & 0.4016 & 0.3397 & 150 & 50 \\
  avrB & 0.3304 & \color{red}{0.3384} & 50, 30 & 30 \\
  avrRpm1 & 0.361 & 0.3368 & 50, 30 & 20 \\
  Seed bank 133-91 & 0.3446 & 0.3334 & 150 & 5 \\
  Mg25 & 0.5321 & 0.3288 & 50, 30 & 60 \\
  Leaf roll 10 & 0.3558 & 0.3272 & 48 & 40 \\
  Yield spain 2009 (2nd experiment) & 0.4184 & 0.3197 & 20, 10 & 40 \\
  Noco2 & 0.3051 & \color{red}{0.3174} & 48 & 30 \\
  Emwa1 & 0.3226 & 0.3124 & 50, 30 & 30 \\
  FT Duration GH & 0.2659 & \color{red}{0.3123} & 48 & 5 \\
  Leaf serr 22 & 0.3021 & \color{red}{0.3108} & 150 & 60 \\
  Anthocyanin 10 & 0.3198 & 0.3107 & 50, 35, 15 & 60 \\
  Cd114 & 0.3345 & 0.3069 & 50, 30 & 50 \\
  Leaf serr 16 & 0.2895 & \color{red}{0.3011} & 48 & 40 \\
  Fe56 & 0.2802 & \color{red}{0.3006} & 150 & 35 \\
  YieldLocSweden2009 & 0.3431 & 0.2993 & 150 & 60 \\
  Width 16 & 0.3463 & 0.2983 & 150 & 50 \\
  Co59 & 0.2738 & \color{red}{0.2953} & 50, 35, 15 & 25 \\
  K39 & 0.3036 & 0.2952 & 50, 30 & 60 \\
  Leaf roll 16 & 0.3072 & 0.2886 & 150 & 15 \\
  DTFplantingLoc2008 & 0.2971 & 0.275 & 50, 30 & 5 \\
  SizePlantingSummerLocSweden2009 & 0.2803 & 0.2704 & 50, 30 & 60 \\
  Mn55 & 0.2775 & 0.2662 & 50, 30 & 20 \\
  Anthocyanin 22 & 0.2731 & 0.2635 & 150 & 15 \\
  As75 & 0.254 & \color{red}{0.2619} & 50, 30 & 35 \\
  Na23 & 0.2564 & \color{red}{0.2598} & 50, 30 & 15 \\
  Ni60 & 0.2894 & 0.2539 & 150 & 25 \\
  Mo98 & 0.2765 & 0.2537 & 50, 30 & 35 \\
  Chlorosis 22 & 0.2622 & 0.2453 & 50, 35, 15 & 10 \\
  Hiks1 & 0.2441 & \color{red}{0.2452} & 20, 10 & 20 \\
  Zn66 & 0.2553 & 0.2444 & 150 & 35 \\
  B11 & 0.2891 & 0.2392 & 48 & 40 \\
  Germ 16 & 0.2987 & 0.2356 & 50, 30 & 41 \\
  At2 & 0.2147 & \color{red}{0.216} & 150 & 15 \\
  Emco5 & 0.166 & \color{red}{0.2101} & 150, 30 & 20 \\
  Se82 & 0.2192 & 0.2075 & 150 & 25 \\
  Mature cell length & 0.1987 & \color{red}{0.2052} & 150 & 45 \\
  DW  & 0.2878 & 0.2048 & 50, 30 & 60 \\
  Yield sweden 2009 (1st experiment) & 0.2274 & 0.2033 & 150 & 55 \\
  As2 & 0.1774 & \color{red}{0.1962} & 150 & 15 \\
  Meristem zone length & 0.1976 & 0.195 & 150 & 50 \\
  Germ 10 & 0.2073 & 0.1873 & 20, 10 & 40 \\
  Anthocyanin 16 & 0.2433 & 0.1867 & 20, 10 & 10 \\
  Width 22 & 0.2224 & 0.1856 & 50, 30 & 50 \\
  YieldPlantingSummerLocSweden2009 & 0.2146 & 0.18 & 150 & 55 \\
  DTFplantingSummerLocSweden2009 & 0.2032 & 0.1775 & 150 & 55 \\
  Bs & 0.2161 & 0.1656 & 50, 30 & 60 \\
  Bs CFU2 & 0.1672 & 0.1584 & 50, 35, 15 & 15 \\
  Germ 22 & 0.1267 & \color{red}{0.1533} & 50, 30 & 35 \\
  Leaf roll 22 & 0.1135 & \color{red}{0.1511} & 48 & 45 \\
  RP GH & 0.1755 & 0.1458 & 150 & 15 \\
  Cu65 & 0.1543 & 0.1315 & 150 & 5 \\
  Li7 & 0.1611 & 0.1297 & 150 & 60 \\
  As & 0.1089 & \color{red}{0.1227} & 100 & 20 \\
  At1 & 0.1473 & 0.1197 & 48 & 40 \\
  S34 & 0.1045 & \color{red}{0.11} & 50, 30 & 60 \\
  YieldPlantingSummer2009 & 0.1265 & 0.0984 & 150 & 50 \\
  Silique 16 & 0.2366 & 0.0884 & 50, 30 & 60 \\
  Chlorosis 10 & 0.0243 & \color{red}{0.088} & 50, 35, 15 & 55 \\
  Ca43 & 0.3333 & 0.0732 & 50, 35, 15 & 55 \\
  Seedling Growth & 0.0813 & 0.0636 & 48 & 30 \\
  Vern Growth & -0.0096 & \color{red}{0.0422} & 150 & 15 \\
  At2 CFU2 & 0.0694 & 0.0378 & 150 & 25 \\
  Yield sweden 2009 (2nd experiment) & 0.0536 & 0.0355 & 150 & 25 \\
  As CFU2 & 0.0312 & \color{red}{0.035} & 150 & 5 \\
  At1 CFU2 & 0.0818 & 0.0319 & 50, 30 & 50 \\
  Aphid number & -0.0246 & \color{red}{0.029} & 50, 35, 15 & 10 \\
  After Vern Growth & -0.1433 & \color{red}{0.0057} & 50, 35, 15 & 5 \\
  Chlorosis 16 & -0.0313 & \color{red}{-0.0121} & 150 & 5 \\
  As2 CFU2 & 0.0504 & -0.0325 & 50, 30 & 60 \\
\bottomrule
\end{longtable}
\doublespacing



\begin{figure}[H]
 \centering\includegraphics[height=.59\textheight, width=1.0\textwidth]{ ann_vs_gblup} \decoRule
\caption[Scatterplot comparing prediction accuaracies of ANN and GBLUP in \textit{A. thaliana}]{Scatterplot comparing prediction accuaracies of ANN and GBLUP in \textit{A. thaliana}. Greyscale indicates the magnitude of the difference between the methods}
\label{fig:annblup}
\end{figure}

\subsection{Results of maize prediction}
\subsubsection{Across environments}

\begin{figure}[H]
  \centering \includegraphics[angle=0,height=.49\textheight, width=1.1\textwidth]{gp_kemater}
  \decoRule
\caption[Violinplot comparing the results for GP in the DH population Kemater for ANN and GBLUP]{Violinplot comparing the results for GP in the DH population Kemater for ANN and GBLUP }
\label{fig:ke_ann}
\end{figure}

\begin{figure}[H]
  \centering \includegraphics[angle=0,height=.495\textheight, width=1.1\textwidth]{gp_petkuser}
  \decoRule
  \caption[Violinplot comparing the results for GP in the DH population Petkuser for ANN and GBLUP]{Violinplot comparing the results for GP in the DH population Petkuser for ANN and GBLUP }
\label{fig:pe_ann}
\end{figure}


\onehalfspacing
\begin{table}[H]
\centering
\begin{tabular}{lcc|cc}
  \toprule
  & \multicolumn{2}{c}{\textbf{Kemater}}  & \multicolumn{2}{c}{\textbf{Petkuser}} \\
  Phenotype & GBLUP & ANN & GBLUP & ANN \\ 
  \midrule
  EV\_V3 & 0.44 & 0.46 & 0.31 & 0.25 \\ 
  EV\_V4 & 0.47 & 0.49 & 0.31 & 0.25 \\ 
  EV\_V6 & 0.43 & 0.44 & 0.38 & 0.33 \\ 
  DtTAS & 0.47 & 0.44 &  &  \\ 
  PH\_V4\_mean & 0.54 & 0.56 & 0.46 & 0.44 \\ 
  PH\_V6\_mean & 0.53 & 0.56 & 0.51 & 0.48 \\ 
  PH\_final & 0.69 & 0.70 & 0.68 & 0.67 \\ 
  DtSILK & 0.57 & 0.53 & 0.54 & 0.52 \\ 
  \bottomrule
\end{tabular}
\end{table}
\doublespacing

\subsubsection{Single environments}

\begin{figure}[H]
  \centering \includegraphics[angle=0,height=.695\textheight, width=1.1\textwidth]{SL_pred}
  \decoRule
  \caption[Results of genomic prediction across single environments for Kemater and Petkuser DH populations]{Results of genomic prediction across single environments for \textbf{A} Kemater  and \textbf{B} Petkuser DH populations}
\label{fig:sl_pred}
\end{figure}

\onehalfspacing
\begin{table}[H]
  \centering
  \caption[Comparison of prediction results of ANN within locations and across locations for Kemater and Petkuser]{Comparison of prediction results of ANN within locations (WL)  and across locations (AL) for Kemater and Petkuser}
  \begin{tabular}{lrr|rr}
    \toprule
    & \multicolumn{2}{c}{\textbf{Kemater}}  & \multicolumn{2}{c}{\textbf{Petkuser}} \\
    Phenotype & AL   & WL   & AL    & WL \\ 
    \midrule
    EV\_V3    & 0.73 & 0.46 & 0.72 & 0.25 \\ 
    EV\_V4    & 0.72 & 0.49 & 0.65 & 0.25 \\ 
    EV\_V6    & 0.70 & 0.44 & 0.65 & 0.33 \\ 
    PH\_V4    & 0.84 & 0.56 & 0.84 & 0.44 \\ 
    PH\_V6    & 0.80 & 0.56 & 0.80 & 0.48 \\ 
    PH\_final & 0.78 & 0.70 & 0.76 & 0.67 \\ 
    DtSILK    & 0.76 & 0.53 & 0.77 & 0.52 \\ 
    \bottomrule
  \end{tabular}
\end{table}
\doublespacing

\section{Discussion}

\onehalfspacing
\begin{table}[H]
  \centering
  \caption[ANN architectures of ANN resulting in highest prediction accuracies]{ANN architectures resulting in highest prediction accuracies, with number of hidden layer (HL) and the total count (n)}
  \begin{tabular}{cccc}
  \toprule
    LCL & Architecture & HL & n \\ 
   \midrule
    True &  150        & 2 &  56  \\ 
    True &  50, 30     & 3 &  47  \\ 
    True &  48         & 2 &  23  \\ 
    True &  50, 35, 15 & 4 &  11  \\ 
    True &  20, 10     & 3 &   5  \\ 
    True &  100        & 2 &   2  \\ 
    True &  150, 30    & 3 &   1  \\ 
   \bottomrule
\end{tabular}
\end{table}
\doublespacing