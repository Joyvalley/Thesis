% Chapter 1
\newcommand{\formatprogramnames}[1]{\texttt{#1}}
\newcommand{\ce}{\formatprogramnames{chloroExtractor}}
\newcommand{\oa}{\formatprogramnames{ORG.Asm}}
\newcommand{\fp}{\formatprogramnames{Fast-Plast}}
\newcommand{\ioga}{\formatprogramnames{IOGA}}
\newcommand{\np}{\formatprogramnames{NOVOPlasty}}
\newcommand{\go}{\formatprogramnames{GetOrganelle}}
\newcommand{\cassp}{\formatprogramnames{Chloroplast assembly protocol}}



\chapter{Benchmarking of Chloroplast Genome Assembly tools } % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1}
This chapter orientates on \cite{freudenthal2019landscape} only the chapters from the publication which the author majorly contributed are included. If not cited otherwise the plots even though they were published along the aforementioned paper, were designed and generated by the author.

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Introduction} \label{intro:cp}
\subsection{Motivation}

Circular DNA of a size between 120 kBP to 160 kBP \cite{palmer_1985}.  First chloroplast sequenced as early as
1986 \textit{Marchantia polymorpha} and \textit{Nico} \cite{ohyama_chloroplast_1986};
\cite{shinozaki_complete_1986}.  Review genome structure \cite{green_chloroplast_2011};
\cite{wicke_evolution_2011}.  Chloroplast genomes widely used in evolutionary studies
\cite{martin_evolutionary_2002}; \cite{xiao-ming_inferring_2017}.  Chloroplast genomes are small through
endosymbiotic gene transfer \cite{martin_evolutionary_2002}; \cite{deiner_environmental_2017}. Up to 14 \% of
the the core genome of \textit{Arabidopsis thaliana} is made up of genes previously from the chloroplast
(fancy citation), while 100-120 genes remain on the chloroplast \cite{wicke_evolution_2011}.  However
chloroplast between plant species show large variety e.g. parasitic plants.  Chloroplast genomes are much
smaller than core genomes e.g \textit{A. thaliana} ca. 125 MBP. Highly conserved high gene content, therefore
changes are more likely to be functional.  Single chloroplast contain up to hundreds of copies of its genome
\cite{kumar_2014}; \cite{bendich_1987}, and photosynthetic activate cells contain multiple chloroplasts,
therefore the copy number of chloroplast genomes is much higher than number of core genomes per cell.
Structurally chloroplast genomes consist of two inverted repeats (IR) - $IR_A$ and $IR_B$ - ranging from 10
kbp to 76 kB the divide the chloroplast genome into two distinct regions the large single copy (LSC) and the
small single copy (SSC) as shown in \ref{fig:cpast_genome} \cite{palmer_1985}. Taking into account that the
majority of assembly tools has been designed to assemble linear core genomes, the structure of chloroplast
genomes is a major obstacle when wanting to assemble those genomes with modern short read technologies,
especially solving and aligning the IR \cite{Wang2018}.

\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.95\textwidth]{Figures/cpast}
\decoRule
\caption[Structure of a chloroplast genome]{Structure of the chloroplast genome of \textit{A. thaliana} with SSC-Region and LSC-Region. Length of the genome and its parts in kbp. Graphic from \cite{olejniczak2016chloroplasts}}
\label{fig:cpast_genome}
\end{figure}

Chloroplast genomes present biologist with many obstacles of which heteroplasmy stands out and immensely
complicated genome assemblies and downstream analyses \cite{corriveau_1988}; \cite{Chat2002}.  Heteroplasmy
describes the phenomenon of co-existence of multiple versions of the chloroplast genomes in a single organism
and single cells of that respective organism. The underlying evolutionary mechanisms behind heteroplasmy are
no fully elucidated and eventually existing fitness advantages fueling heteroplasmy cannot be explained
satisfactory by evolutionary methods \cite{Scar2016}.  There is a variety of databases containing short read
data for species without an chloroplast genome available. For example NCBI's the short read archive (SRA)
\cite{SRA2010}. Because most plant DNA extraction protocols use green leaf tissue as basis, they usually
contain a large amount of plastid DNA. Having larger number of assembled and annotated chloroplast genomes
publicly available would be beneficial for evolutionary studies and could be a useful addition to bar-coding
and super-barcoding \cite{coissac_barcodes_2016} and other biotechnological applications
\cite{daniell_chloroplast_2016}. To achieve this there is a variety of tools available. The study presented in
this chapter, assesses the availability, usability and overall performance of 7 of those tools and ultimately
makes use of the newly gained insights to attempt \textit{de novo} assemble more than 100 chloroplast




\subsection{Extraction of chloroplast reads from whole genome data and general assembly workflow}
There is a variety of strategies to assemble chloroplast genomes from raw sequencing data
\cite{twyford_strategies_2017}. In general the process involves three steps: (i) extraction of plastid reads
from the WGS data, (ii) assembly of the plastid genome, (iii) solving the circular structure of the chromosome
with the IRs. There are two distinct ways to tackle step (i): The first one is to map all the reads to a
reference chloroplast \cite{Vinga2012}, which works reasonably well if there is one available for the same, or
at least a closely related species. The second is to make use of the much larger coverage of chloroplast DNA
compared to core DNA, with a k-mer analysis \cite{Chan2013}, this is for example done by \ce
\cite{j_ankenbrand_chloroextractor:_2018}. The third way to accomplish this is to combine as done by \np
\cite{dierckxsens_novoplasty:_2017}.  Figure \ref{fig:cpast_workflow} shows the general workflow of
chloroplast assembly too.


\begin{figure}[H]
\centering
\includegraphics[height=.65\textheight, width=.95\textwidth]{Figures/CE_workflow}
\decoRule
\caption[Chloroplast genome assembly workflow]{Standard workflow of chloroplast genome assembly. Graphic from \cite{j_ankenbrand_chloroextractor:_2018} }
\label{fig:cpast_workflow}
\end{figure}


\subsubsection{Purpose and scope of benchmarking the landscape of chloroplast assembly tools}

The purpose of this study is to provide insights into the landscape of chloroplast assembly tools, to
recommend best practices for organelle genome assemblies and to provide \textit{de novo} assemblies for many
species and family's without a reference chloroplast available so far.  To be included into this study the
software including the source code must be publicly available and published under the terms of a liberal free
open source software e.g. GPL or MIT-license. The study was also limited to paired-end Illumina data sets as
their sole input source, because they are abundantly available for this benchmark.The seven tools that me the
requirements and were therefore included in this study were: \ce, \oa, \fp, \ioga, \np, \go and \cassp. As
later thoroughly described in section \ref{results:ca}, there are huge differences between those tools and
some will not be recommend to be used in scientific applications, while others outstand for most but not all
cases.


%----------------------------------------------------------------------------------------
\section{Material and Methods}
\subsection{Methods}
\subsubsection{Data and code availability}
All the source code and data used in the scope of this study is publicly available under the terms of the MIT-License. The source code has been published on github \cite{github-benchmark-repo} and archived on zenodo \cite{zenodorepo} . The docker images are available on dockerhub \cite{dockerhub-benchmark}

\subsubsection{Tools}
There were certain requirements to be met to be included into the study as aforementioned. The only technical
requirements was, being able to assemble chloroplast genomes from paired-end Illumina reads. The other
requirements were due to reproducibility. (i) The software must be open-source and available under the terms
of a liberal software license. In the authors opinion obscuring reproducibility under paywalls cannot be
considered good scientific practice. (ii) It must be a command line tool, GUI-only tools are not suited for
highly repetitive automated analyses.  In total there were 7 tools that met those requirements: (i)
\hspace{0.5ex} \ce \hspace{0.5ex} \cite{j_ankenbrand_chloroextractor:_2018}; (ii) \hspace{0.5ex} \cassp
\hspace{0.5ex} \cite{sancho_comparative_2018}; (iii)\hspace{0.5ex} \go \hspace{0.5ex}
\cite{jin_getorganelle:_2018}; (iv) \hspace{0.5ex} \oa \hspace{0.5ex} \cite{coissac_barcodes_2016}; (v)
\hspace{0.5ex} \ioga \hspace{0.5ex} \cite{bakker_herbarium_2016}; (vi) \hspace{0.5ex} \fp \hspace{0.5ex}
\cite{mckain__fast-plast_2017}; (vii) \hspace{0.5ex} \np \hspace{0.5ex} \cite{dierckxsens_novoplasty:_2017}.
There are other tools available, that did not meet the requirements.

\subsubsection{Standardization and reproducibility}

The main goal of the study was to provided deep insights into the landscape of chloroplast assembly tools. To
accomplish that we tried to use the highest standards in bioinformatics when it comes to standardization and
reproducibility. Along the study we also wanted to publish easy and ready-to-use versions of all the involved
programs, working with standardized input. To accomplish that we decided to make us of containerization in
form of docker containers \cite{merkel2014docker} to work with the containers in a closed HPC environments we
decided a related software - singularity \cite{kurtzer2017singularity}. Therefore users do not have anything
else to do than provide two files one for the forward reads (\texttt{forward.fq}) and one for the reverse
reads (\texttt{reverse.fq}). Both files are required to be in FASTQ format. If docker or singularity are
installed no further setup is requirement, because all dependencies and packages are installed and run from
the containers on any given environment. Besides the individual output files, recording the process of the
respective program, all programs write the assembly products in to files called \texttt{output.fa} in FASTA
format.  For the quantitative and consistency measurements the singularity containers were run on the Julia
HPC-Cluster of the University of W\"{u}rzburg using the SLURM workload manager \cite{Jette02slurm}.  All runs
for all assemblies were set with a time limit of 48h. This was necessary because some assemblers e.g. \ioga,
if not finishing after at least 12 hours appear to continue in some kind of loop and never finish.


\subsection{Data}
Three different data sets were used for this study. (i) Simulated data from \textit{A. thaliana}
chloroplasts. (ii) real data with known reference chloroplast to rate the success of the assemblies. (iii)
Novel data sets from NCBI's SRA without a know reference chloroplast to apply the gained knowledge for the
\textit{de novo} assembly of more than 100 chloroplasts.

\subsubsection{Simulated}

For first steps in any benchmarking process it is always useful to start with simulated data, where the
investigators have full control over all the parameters involved. In the case of the present study the data's
input parameters thought to be influential on the outcome where: The read length, the ratio between
chloroplast and core genome reads. The simulations were based on data from the TAIR10 genome \cite{tair10} and
performed using \texttt{seqkit} \cite{seqkit}. Ratios simulated were: 0:1, 1:10, 1:1000 and 1:1000, with read
length of 150 and 250 bp. The simulated data consisted either of 2 million read pairs or the full data
available. The simulation process is documented and the code and the data is available on github and zenodo
\cite{zenododataset}

\subsubsection{Real data set}\label{sec:cp_real}

Real data was selected from the SRA database table \ref{tab:sra_real} lists the search terms that had to be
met according to the aforementioned search terms.


\begin{table}[H]
\caption{Data selection criteria for real data sets from SRA}
\label{tab:sra_real}
\centering
\begin{tabular}{ccl}
  \hline
  choice & option & explanation \\
  \hline
  green plants & Organism & include only photosynthetic plants e.g. no algae  \\
  wgs & Strategy & only data from wgs projects included \\
  Illumina & Platform & include only paired-en Illumina reads \\
  biomol DNA & Properties & include only biomol. DNA samples (e.g. no RNA) \\
  paired & Layout & exclude single-end reads  \\
  random & Selection & \\
  public & Access & Only publicly available data included \\
  \hline                                         
\end{tabular}
\end{table}

In total this resulted in 369 data sets representing a broad variety of the plant kingdom with many different
families and genera included.

\subsubsection{Novel data sets}

To assess the performance on assemblies without a published chloroplast on \texttt{CpBase} \cite{cpbase} 105
data sets were selected from SRA, it was emphasized that such data sets were selected, with the next relatives
with a reference chloroplast as distantly related as possible in taxonomic terms according to NCBI
\cite{ncbitaxonomy}


\subsection{Evaluation}
\subsubsection{Quantitative}

Each assembly from each assembler was compared to their respective reference genome by alignment using
minimap2 \cite{li2018minimap2} and based on those alignments scores were calculated following equation
\ref{eqn:score_ass} from 0 to 100 with 100 being a perfect score. Four different metrics contributed equally
to the final score. (i) The coverage of the assembled genome compared to the reference genome $cov_{ref}$ as
an estimate for the completeness. (ii) The vice versa case $cov_{qry}$ as a measure for the correctness of the
assembly. (iii) The success of resolving the IR correct, estimated from the size difference from the reference
and the newly assembled genome is:
$min\left\{ \frac{cov_{qry}}{cov_{ref}}, \frac{cov_{ref}}{cov_{qry}}\right\}$.  (iv) The number of total
contigs were weighted as $\frac{1}{n_{contigs}}$ giving a chloroplast with 1 contig the optimal score.

\begin{equation}
  score = \frac{1}{4} \cdot \left( cov_{ref} +  cov_{qry} + min\left\{ \frac{cov_{qry}}{cov_{ref}}, \frac{cov_{ref}}{cov_{qry}}\right\} + \frac{1}{n_{contigs} }\right) \cdot 100
  \label{eqn:score_ass}
\end{equation}

While it is difficulty to assess the success or failure of assemblies on a continuous scale, equation
\ref{eqn:score_ass} allows for objective and unbiased measurements. SNPs or other small variants do not
influence the outcome of the score, because it much more likely that there are due to in-species variation of
the plastid's genome and not caused be the assembly, and even if the latter is true it would be difficult to
evaluate that.  
  
\subsubsection{Consistency}

In any given bioinformatical application consistency is a desired trait. All software (exceptions excluded)
should given repeatedly the same input provide the same output. The same obviously or even more so holds true
for assembly tools. To evaluate the reproducibility of the the 7 tools. All the 369 real data sets described
in section \ref{sec:cp_real} has been used twice for assembly with each assembler and scored twice with
equation \ref{eqn:score_ass}. The correlation between the first and the second scores and runs was used the
measure for the robustness of a program.

\section{Results} \label{results:ca}
\subsection{Quantitative}
\subsubsection{Simulated data}
\label{results:sim}

The simulated data was assembled and scored with all the tools as described above. Figure \ref{fig:sim_tiles}
shows as a tile plot the results for all data sets and assemblers. While at first sight there is no clear
correlation with the input data sets. It is obvious that there are grave differences between the
assemblers. Two programs namely \cassp \hspace{0.5ex} and \ioga \hspace{0.5ex}failed to correctly assemble a
single chloroplast genome. \ioga even fails to provide output at all for the majority of the data sets. While
those two stand out as a negative example \fp and \go stand out as a positive example perfectly or nearly
perfectly assembling all the data sets, with \go surpassing the performance of \fp. In the middle of the filed
are \ce, \oa and \np performing reasonably well, but sometimes lacking to solve the IRs and the circular
structure.


\begin{figure}[H]
\centering
\includegraphics[height=.55\textheight, width=.99\textwidth]{Figures/sim_tiles}
\decoRule
\caption[Score of assemblies of simulated data sets]{Results of assemblies executed with simulated data sets.}
\label{fig:sim_tiles}
\end{figure}

While there is a significant difference between the assemblers the same is not necessarily true in general for
the other varying parameters. While \fp deals with the shorter reads of 150 bp much better than with the
longer reads of 250 bp. The outcome of the other assemblies does not seem to be influenced by that. There is
no difference between the full and the subsampled data sets. And while all assemblers appear to be more
challenge by low chloroplast to core genome ratios of 1:10 larger rations do not affect the quality of the
result as expected.  Table \ref{tab:scores_simulated} shows all the individual results for all data sets and
assemblers. For the fields with no entry the respective assembler failed to provide an output.


\begin{table}[ht]
\caption{Scores of assemblies of simulated data}
\label{tab:scores_simulated}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{rlrrrrrrr}
 \hline
 & data set & CAP & CE & Fast-Plast & GetOrganelle & IOGA & NOVOPlasty & org.ASM \\ 
 \hline
 1 & sim\_150bp.0-1 & 79.10 & 100.00 & 99.48 & 100.00 & & 91.52 & 100.00 \\ 
 2 & sim\_150bp.0-1.2M & 79.10 & 100.00 & 99.72 & 100.00 & 79.10 & 91.52 & 91.50 \\ 
 3 & sim\_150bp.1-10 & & 56.44 & 100.00 & 76.98 & & 91.52 & 78.00 \\ 
 4 & sim\_150bp.1-10.2M & & & 99.97 & 100.00 & & 91.52 & 82.72 \\ 
 5 & sim\_150bp.1-100 & 75.72 & 100.00 & 99.48 & 100.00 & 66.09 & 91.52 & 91.50 \\ 
 6 & sim\_150bp.1-100.2M & & 100.00 & 99.47 & 100.00 & & 100.00 & 100.00 \\ 
 7 & sim\_150bp.1-1000 & 79.10 & & 99.72 & 100.00 & & 91.52 & 100.00 \\ 
 8 & sim\_150bp.1-1000.2M & 79.10 & 100.00 & 99.72 & 100.00 & & 91.52 & 100.00 \\ 
 9 & sim\_250bp.0-1 & 79.10 & 100.00 & 93.82 & 100.00 & & 91.52 & 91.50 \\ 
 10 & sim\_250bp.0-1.2M & 79.10 & 100.00 & 93.83 & 100.00 & & 91.52 & 91.50 \\ 
 11 & sim\_250bp.1-10 & & 54.98 & 68.45 & 78.89 & 52.71 & 91.52 & 40.20 \\ 
 12 & sim\_250bp.1-10.2M & & & 93.00 & 100.00 & 52.67 & 87.40 & 40.20 \\ 
 13 & sim\_250bp.1-100 & 72.81 & 100.00 & 93.82 & 100.00 & & 87.40 & 100.00 \\ 
 14 & sim\_250bp.1-100.2M & & 100.00 & 93.83 & 100.00 & & 87.40 & 100.00 \\ 
 15 & sim\_250bp.1-1000 & 79.10 & 21.30 & 93.83 & 100.00 & 76.96 & 91.52 & 91.50 \\ 
 16 & sim\_250bp.1-1000.2M & 79.10 & 100.00 & 93.83 & 100.00 & 67.55 & 87.40 & 100.00 \\
 \hline
\end{tabular}}
\end{table}



\subsubsection{Real data sets}

Table \ref{tab:scores_real} gives the results from the assemblies of 369 data sets with 7 assemblers. Similar
to the results of the previous section there is a significant difference between the tools. Likewise \go is
the most successful assembler by a large margin with 210 of 369 perfectly assembled chloroplast genomes,
failing to provide output for only 9 data sets, resulting in a media score > 99.  Contrary to \go, \cassp and
\ioga fail to completely assemble a single genome. The performance of \fp could be considered reasonably well,
completing approximately half as many genomes as \go and being the only other tool whose average score is
larger than 90. Similar to the trials with the simulated data in chapter \ref{results:sim} in the middle of the field are \ce, \hspace{0.5ex} \np  \hspace{0.5ex} and \oa.


\begin{table}[H]
\caption{Mean scores of chloroplast genome assemblers}
\label{tab:scores_real}
\centering
\begin{tabular}{rlrrrr}
  \hline
  & assembler & Median & IQR & N\_perfect & N\_tot \\ 
  \hline
  1 & CAP & 45.25 & 50.19 &  0 & 369 \\ 
  2 & CE & 56.55 & 71.50 & 14 & 369 \\ 
  3 & Fast-Plast & 92.80 & 23.59 & 113 & 369 \\ 
  4 & GetOrganelle & 99.83 & 20.94 & 210 & 360 \\ 
  5 & IOGA & 71.10 & 11.21 & 0 & 338 \\ 
  6 & NOVOPlasty & 75.95 & 48.69 & 58 & 369 \\ 
  7 & org.ASM & 67.35 & 91.69 & 46 & 348 \\ 
  \hline
\end{tabular}
\end{table}


Figure \ref{fig:swarm} emphasizes the large differences between the assemblers shown in table
\ref{tab:scores_real}. The swarm plots show some distinct bands for some assemblers e.g. \np and \oa,
suggesting that multiple assemblies fail to be solved into a single contig genome at a certain point. As
thoroughly discussed in section \ref{dis_cp} just from this swarm plot it is debatable if all the tools could
be recommended for the purpose they were designed for.

\begin{figure}[H]
\centering
\includegraphics[height=.45\textheight, width=.95\textwidth]{Figures/swarm}
\decoRule
\caption[Scores of assemblies from real data sets]{Box and swarm plots depict the results from the scoring shown in \ref{eqn:score_ass}}
\label{fig:swarm}
\end{figure}



\subsubsection{Consistency}

Consistency testing was done by re-running every assembly for the real data sets twice and comparing the
scores. \ce was the only tool that was 100 \% consistent over 2 runs. Followed by \fp and \np. The consistency
plot figure \ref{fig:consisplot} for both of them results in an arrowhead shaped plot. With the main differences
between the first and second run in the best scores. All other assemblers appear to produce the same output in
the two runs except if the fail to complete the assembly at all. This is less pronounced for \cassp and \go
and is a grave issue for \oa and \ioga.

\begin{figure}[H]
\centering
\includegraphics[height=.45\textheight, width=.95\textwidth]{Figures/repro}
\decoRule
\caption[Comparison between two runs with the same assembler for consistency testing ]{Swarm plots depict the
  results from the scoring shown in \ref{eqn:score_ass} for two independent runs for each assembler on each of
  the data sets}
\label{fig:consisplot}
\end{figure}


\subsubsection{Novel}
 
The final assessment in the scope of the study was to test the assemblers on novel data sets without a
published chloroplast. This step is important for two reasons. (i) There could be the possibility that certain
tools perform well on known chloroplasts because they somehow of knowledge over those and simply apply it
during those assemblies and therefore the previous results will not be able to generalize on unknown
data. (ii) To apply and test the gained insights in the hope of providing the scientific community with a
larger variety of published chloroplast genomes. Again the most successful assembler was \go, with 49 out of
105 data sets completely assembled. Lacking a reference genome the success had to be defined differently and
equation \ref{eqn:score_ass} was not suitable to evaluate the novel assemblies. Metrics influencing the score
of the novel assemblies were the number of contigs, solving the IRs and the size of the SSC and LSC. This
known to the authors might be very biased and not true for all chloroplast and assumes that all chloroplast
genomes follow the general structure for their genomes described in chapter \ref{intro:cp}.
Figure \ref{fig:upset_novel} compares the results for the assemblies with at least one successful assembly.

\begin{figure}[H]
\centering
\includegraphics[height=.45\textheight, width=.95\textwidth]{Figures/upset_novel}
\decoRule
\caption[Upset plot comparing the success rates for novel data sets]{Upset plot comparing the success rates of the different assemblers}
\label{fig:upset_novel}
\end{figure}


\section{Discussion} \label{dis_cp}


The study compromised of two goals. (i) to assess the overall performance of a variety of tools designed
specifically for the assembly of circular chloroplast genomes from paired-end Illumina data and (ii) to
\textit{de novo} assemble a variety of yet unpublished chloroplast genomes from existing data. To accomplish
the first goal 16 simulated and 369 simulated data sets were used adding up to a total of 5166 assemblies for
the real data sets and 112 for the simulated data, along 735 assemblies for the novel data sets, thus
underlying the statistical powers of this benchmarking study.  The most successful tools were \go
\hspace{.5ex} and \fp \hspace{.5ex} which are recommended to be used complementary, because as shown in figure
\ref{fig:upset} they succeed for the most data sets compared to other assemblers and accomplish to
satisfactory assemble chloroplast genomes where the other fails. If both of them fail it might be worthwhile
to repeat the runs because other results could be expected as shown in the scatter plots of figure
\ref{fig:consisplot}, especially \fp might be able to improve the previously achieved sore. Only if both of
them fail it might, even though improbable, that \np might lead to a successful assembly. The other assemblers
should be used with caution. While \ce might be good for a quick overview due to its relatively low demand in
computational time \cite{freudenthal2019landscape}; \cassp, \oa and \ioga are not be recommended to used as
the primary tools in organelle genome assembly projects, as used in this study. 

\begin{figure}[H]
\centering
\includegraphics[height=.65\textheight, width=.99\textwidth]{Figures/upset}
\decoRule
\caption[Upset plot comparing the success rates of of all assemblers]{Upset plot showing the intersections of
  success rates between assemblers. A successful assembly was defined with a score > 99 according to equation
  \ref{eqn:score_ass}. The colored, horizontal barplot indicates the total number of successful assemblies for
  an individual tool. The black, vertical barplot gives the magnitude of the intersection between the
  assemblers indicated with the dot. Therefore there first bar is to be interpreted as follows: 77 data sets
  were only successfully assembled by \ce, likewise 48 genomes were assembled completely by \go \hspace{0.5ex}
  and \fp \hspace{0.5ex} and so on. }
\label{fig:upset}
\end{figure}


It might be possible that overall performance of a specify tool might change significantly by fine tuning the
input parameters of the tool, which was purposely not done in the scope of the present study, because this
study was designed to mimic the behavior have end-users and not developers of such tools and the assumption is
proposed that end-users with little experience in bioinformatics would be inclined to use the basic
configurations of such a tool.


While there is a huge differences for all assemblers they are presented with the same challenges and the
bottlenecks are similar for each of them, the success rate of passing those differs however. Figure
\ref{fig:alitv} shows the alignment of the 7 chloroplast genomes of \textit{Oryza brachyantha} a grass
distantly related to cultivated rice \textit{Orayza sativa} and the respective reference genome. For the need
of a linear representation of the circular genome the convention is to present chloroplast genomes in the
order LSC - IRa - SSC -IRb. \textit{O. brachyantha} was chosen because multiple tools successfully or at least
almost assembled the full genome. Only \cassp is singled out, which only managed to assemble a few fragments
on the SCC and the IRs on many contigs. A common mistake is to return 3 contigs as \ioga \hspace{.5ex}
did. They represent the LSC the SSC and one IR but failed to resolve those regions into a one circular
contig. \go \hspace{.5ex} and \fp \hspace{.5ex} were able to reproduce the structure of the reference, while
\ce \hspace{.5ex} flipped the LSC and \np  and \oa \hspace{.5ex} were not able to construct the single contig
into the conventional structure. All of this are common mistakes appearing more or less rare in all the
assemblers. This could be a good starting point for the developers to further improve their tools. In this
example all but \cassp \hspace{.5ex} were able to construct all the parts of the chloroplast's genome, and the
main mistake was to resolve the structure of the genome into a circular, one contig version.


\begin{figure}[H]
\centering
\includegraphics[height=.60\textheight, width=.99\textwidth]{Figures/AliTV.png}
\decoRule
\caption[AliTV plot of alignments of assemblies of \textit{Oryza brachyantha} of all assemblers]{ AliTV plot
  \cite{alitv} from \cite{freudenthal2019landscape} showing the alignments of \textit{Oryza brachyantha}
  chloroplast genomes fro all 7 assemblers. Regions in adjacent assemblies are connected with colored ribbons
  for similar regions in alignment with the identity coded as described in the legend. The purple arrows
  indicate the IR regions}
\label{fig:alitv}
\end{figure}

\section{Conclusion \& outlook}

Organelle genomics is promising field in plant genetics. As described in section \ref{intro:cp} chloroplast
genomes are well-suited for applications in evolutionary sciences, taxonomy and barcoding applications. Alike
its mother branch genomics for comparative chloroplast genomics its just as crucial to obtain high quality
genomes. And the quality is mainly influenced by two major factors: the quality of the genome sequencing
protocol and the quality of the assembly. As shown the latter varies massively between tools and not all tools
are recommend equally from the conclusions drawn for the above experiments. All tools have room for
improvement, this is meant to criticize the respectable work of the developers, but to encourage them to
further develop tools and publish them under terms of liberal software licenses for the greater benefit of the
entire scientific community.
