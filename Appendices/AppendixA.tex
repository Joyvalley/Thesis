% Appendix A


\chapter{Source code} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}

\linespread{1.5}
\section{GWAS-Flow}
\subsection{gwas.py}
\begin{lstlisting}[language=Python]
import os
import sys
import time
import numpy as np
import pandas as pd
import main
import h5py

# set defaults 
mac_min = 1
batch_size =  500000 
out_file = "results.csv"
m = 'phenotype_value'
perm = 1
mac_min= 6

X_file = 'gwas_sample_data/AT_geno.hdf5'
Y_file = 'gwas_sample_data/phenotype.csv'
K_file = 'gwas_sample_data/kinship_ibs_binary_mac5.h5py'



for i in range (1,len(sys.argv),2):
    if sys.argv[i] == "-x" or sys.argv[i] == "--genotype":
        X_file = sys.argv[i+1]
    elif sys.argv[i] == "-y" or sys.argv[i] == "--phenotype":
        Y_file = sys.argv[i+1]
    elif sys.argv[i] == "-k" or sys.argv[i] == "--kinship":
        K_file = sys.argv[i+1]
    elif sys.argv[i] == "-m":
        m = sys.argv[i+1]
    elif sys.argv[i] == "-a" or sys.argv[i] == "--mac_min":
        mac_min = int(sys.argv[i+1])
    elif sys.argv[i] == "-bs" or sys.argv[i] == "--batch-size":
        batch_size = int(sys.argv[i+1])
    elif sys.argv[i] == "-p" or sys.argv[i] == "--perm":
        perm  = int(sys.argv[i+1])
    elif sys.argv[i] == "-o" or sys.argv[i] == "--out":
        out_file = sys.argv[i+1]
    elif sys.argv[i] == "-h" or sys.argv[i] == "--help":
        print("-x , --genotype :file containing marker information in csv or hdf5 format of size")
        print("-y , --phenotype: file container phenotype information in csv format"  )
        print("-k , --kinship : file containing kinship matrix of size k X k in csv or hdf5 format")
        print("-m : name of columnn containing the phenotype : default m = phenotype_value")
        print("-a , --mac_min : integer specifying the minimum minor allele count necessary for a marker to be included. Default a = 1" )
        print("-bs, --batch-size : integer specifying the number of markers processed at once. Default -bs 500000" )
        print("-p , --perm : single integer specifying the number of permutations. Default 1 == no perm ")
        print("-o , --out : name of output file. Default -o results.csv  ")
        print("-h , --help : prints help and command line options")
        quit()
    else:
        print('unknown option ' + str(sys.argv[i]))
        quit()



print("parsed commandline args")

start = time.time()

X,K,Y_,markers = main.load_and_prepare_data(X_file,Y_file,K_file,m)


## MAF filterin
markers_used , X , macs = main.mac_filter(mac_min,X,markers)

## prepare
print("Begin performing GWAS on ", Y_file)

if perm == 1:
    output = main.gwas(X,K,Y_,batch_size)   
    if( X_file.split(".")[-1] == 'csv'):
        chr_pos = np.array(list(map(lambda x : x.split("- "),markers_used)))
    else: 
        chr_reg = h5py.File(X_file,'r')['positions'].attrs['chr_regions']
        mk_index= np.array(range(len(markers)),dtype=int)[macs >= mac_min]
        chr_pos = np.array([list(map(lambda x: sum(x > chr_reg[:,1]) + 1, mk_index)), markers_used]).T
        my_time = np.repeat((time.time()-start),len(chr_pos))
    pd.DataFrame({
        'chr' : chr_pos[:,0] ,
        'pos' : chr_pos[:,1] , 
        'pval': output[:,0] ,
        'mac' : np.array(macs[macs >= mac_min],dtype=np.int) ,
        'eff_size': output[:,1] ,
        'SE' : output[:,2]}).to_csv(out_file,index=False)
elif perm > 1:
    min_pval = []
    perm_seeds = []
    my_time = []
    for i in range(perm):
        start_perm = time.time()
        print("Running permutation ", i+1, " of ",perm)
        my_seed  = np.asscalar(np.random.randint(9999,size=1))
        perm_seeds.append(my_seed)
        np.random.seed(my_seed)
        Y_perm = np.random.permutation(Y_)
        output = main.gwas(X,K,Y_perm,batch_size)
        min_pval.append(np.min(output[:,0]))
        print("Elapsed time for permuatation",i+1 ," with p_min", min_pval[i]," is",": ", round(time.time() - start_perm,2))
        my_time.append(time.time()-start_perm)
    pd.DataFrame({
        'time': my_time ,
        'seed': perm_seeds ,
        'min_p': min_pval }).to_csv(out_file,index=False)

print("done")
 
end = time.time()
eltime = np.round(end -start,2)

if eltime <= 59:
    print("Total time elapsed",  eltime, "seconds")
elif eltime > 59 and eltime <= 3600:
    print("Total time elapsed",  np.round(eltime / 60,2) , "minutes")
elif eltime > 3600 :
    print("Total time elapsed",  np.round(eltime / 60 / 60,2), "hours")

  \end{lstlisting}



  \subsection{main.py}
  \begin{lstlisting}[language=Python]
    import pandas as pd 
    import numpy as np
    from scipy.stats import f
    import tensorflow as tf
    import limix
    import herit
    import h5py
    import limix
    import multiprocessing as mlt

    def load_and_prepare_data(X_file,Y_file,K_file,m):
    type_K = K_file.split(".")[-1]
    type_X = X_file.split(".")[-1]
    
    ## load and preprocess genotype matrix 
    Y = pd.read_csv(Y_file,engine='python').sort_values(['accession_id']).groupby('accession_id').mean()
    Y = pd.DataFrame({'accession_id' :  Y.index, 'phenotype_value' : Y[m]})
    if type_X == 'hdf5' or type_X == 'h5py'  :
        SNP = h5py.File(X_file,'r')
        markers= np.asarray(SNP['positions'])
        acc_X =  np.asarray(SNP['accessions'][:],dtype=np.int)
    elif type_X == 'csv' :
        X = pd.read_csv(X_file,index_col=0)
        markers = X.columns.values
        acc_X = X.index
        X = np.asarray(X,dtype=np.float32)/2
    else :
        sys.exit("Only hdf5, h5py and csv files are supported")
      
    if type_K == 'hdf5' or type_K == 'h5py':
        k = h5py.File(K_file,'r')
        acc_K = np.asarray(k['accessions'][:],dtype=np.int)
    elif type_K == 'csv':
        k = pd.read_csv(K_file,index_col=0)
        acc_K = k.index
        k = np.array(k, dtype=np.float32)

    acc_Y =  np.asarray(Y[['accession_id']]).flatten()
    acc_isec = [isec for isec in acc_X if isec in acc_Y]
        
    idx_acc = list(map(lambda x: x in acc_isec, acc_X))
    idy_acc = list(map(lambda x: x in acc_isec, acc_Y))
    idk_acc = list(map(lambda x: x in acc_isec, acc_K))

    Y_ = np.asarray(Y.drop('accession_id',1),dtype=np.float32)[idy_acc,:]

    if type_X == 'hdf5' or type_X == 'h5py' :
        X = np.asarray(SNP['snps'][0:(len(SNP['snps'])+1),],dtype=np.float32)[:,idx_acc].T
        X = X[np.argsort(acc_X[idx_acc]),:]
        k1 = np.asarray(k['kinship'][:])[idk_acc,:]
        K  = k1[:,idk_acc]
        K = K[np.argsort(acc_X[idx_acc]),:]
        K = K[:,np.argsort(acc_X[idx_acc])]
    else:
        X  = X[idx_acc,:]
        k1 = k[idk_acc,:]
        K  = k1[:,idk_acc]
        
       
    print("data has been imported")
    return X,K,Y_,markers


def mac_filter(mac_min, X, markers):
    ac1 = np.sum(X,axis=0)
    ac0 = X.shape[0] - ac1
    macs = np.minimum(ac1,ac0)
    markers_used  = markers[macs >= mac_min]
    X = X[:,macs >= mac_min]
    return markers_used, X, macs

def gwas(X,K,Y,batch_size):
    n_marker = X.shape[1]
    n = len(Y)
    ## REML   
    K_stand = (n-1)/np.sum((np.identity(n) - np.ones((n,n))/n) * K) * K
    vg, delta, ve  = herit.estimate(Y,"normal",K_stand,verbose = False)
    print(" Pseudo-heritability is " , vg / (ve + vg + delta))
    print(" Performing GWAS on ", n , " phenotypes and ", n_marker ,"markers")
    ## Transform kinship-matrix, phenotypes and estimate intercpt
    Xo = np.ones(K.shape[0]).flatten()
    M = np.transpose(np.linalg.inv(np.linalg.cholesky(vg * K_stand + ve  * np.identity(n)))).astype(np.float32)
    Y_t = np.sum(np.multiply(np.transpose(M),Y),axis=1).astype(np.float32)
    int_t = np.sum(np.multiply(np.transpose(M),np.ones(n)),axis=1).astype(np.float32)
    ## EMMAX Scan
    RSS_env = (np.linalg.lstsq(np.reshape(int_t,(n,-1)) , np.reshape(Y_t,(n,-1)))[1]).astype(np.float32)
    ## calculate betas and se of betas 
    def stderr(a,M,Y_t2d,int_t):
         x = tf.stack((int_t,tf.squeeze(tf.matmul(M.T,tf.reshape(a,(n,-1))))),axis=1)
         coeff = tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(tf.transpose(x),x)),tf.transpose(x)),Y_t2d)
         SSE = tf.reduce_sum(tf.math.square(tf.math.subtract(Y_t,tf.math.add(tf.math.multiply(x[:,1],coeff[0,0]),tf.math.multiply(x[:,1],coeff[1,0])))))
         SE = tf.math.sqrt(SSE/(471-(1+2)))
         StdERR = tf.sqrt(tf.linalg.diag_part(tf.math.multiply(SE , tf.linalg.inv(tf.matmul(tf.transpose(x),x)))))[1]
         return tf.stack((coeff[1,0],StdERR))
    ## calculate residual sum squares 
    def rss(a,M,y,int_t):
         x_t = tf.reduce_sum(tf.math.multiply(M.T,a),axis=1)
         lm_res = tf.linalg.lstsq(tf.transpose(tf.stack((int_t,x_t),axis=0)),Y_t2d)
         lm_x = tf.concat((tf.squeeze(lm_res),x_t),axis=0)
         return tf.reduce_sum(tf.math.square(tf.math.subtract(tf.squeeze(Y_t2d),tf.math.add(tf.math.multiply(lm_x[1],lm_x[2:]), tf.multiply(lm_x[0],int_t)))))
    ## loop over the batches 
    for i in range(int(np.ceil(n_marker/batch_size))):
        tf.reset_default_graph()
        if n_marker < batch_size:
            X_sub = X
        else:
            lower_limit = batch_size * i 
            upper_limit = batch_size * i + batch_size
            if upper_limit <= n_marker :
                X_sub = X[:,lower_limit:upper_limit]
                print("Working on markers ", lower_limit , " to ", upper_limit, " of ", n_marker )    
            else:
                X_sub = X[:,lower_limit:]
                print("Working on markers ", lower_limit , " to ", n_marker, " of ", n_marker )    
        config = tf.ConfigProto()
        n_cores = mlt.cpu_count()
        config.intra_op_parallelism_threads = n_cores
        config.inter_op_parallelism_threads = n_cores
        sess = tf.Session(config=config)                                             
        Y_t2d = tf.cast(tf.reshape(Y_t,(n,-1)),dtype=tf.float32)                     
        y_tensor =  tf.convert_to_tensor(Y_t,dtype = tf.float32)                                      
        StdERR = tf.map_fn(lambda a : stderr(a,M,Y_t2d,int_t), X_sub.T)              
        R1_full = tf.map_fn(lambda a: rss(a,M,Y_t2d,int_t), X_sub.T)
        F_1 = tf.divide(tf.subtract(RSS_env, R1_full),tf.divide(R1_full,(n-3)))
        if i == 0 :
            output = sess.run(tf.concat([tf.reshape(F_1,(X_sub.shape[1],-1)),StdERR],axis=1))
        else :
            tmp = sess.run(tf.concat([tf.reshape(F_1,(X_sub.shape[1],-1)),StdERR],axis=1))
            output = np.append(output,tmp,axis=0)
        sess.close()
        F_dist = output[:,0]
    pval  = 1 - f.cdf(F_dist,1,n-3)
    output[:,0] = pval
    return output 


  \end{lstlisting}

\subsection{herit.py}
\begin{lstlisting}[language=Python]
  
def estimate(y, lik, K, M=None, verbose=True):
    from numpy_sugar.linalg import economic_qs
    from numpy import pi, var, diag
    from glimix_core.glmm import GLMMExpFam
    from glimix_core.lmm import LMM
    from limix._data._assert import assert_likelihood
    from limix._data import normalize_likelihood, conform_dataset 
    from limix.qtl._assert import assert_finite
    from limix._display import session_block, session_line
    lik = normalize_likelihood(lik)
    lik_name = lik[0]
    with session_block("Heritability analysis", disable=not verbose):
        with session_line("Normalising input...", disable=not verbose):
            data = conform_dataset(y, M=M, K=K)
        y = data["y"]
        M = data["M"]
        K = data["K"]
        assert_finite(y, M, K)
        if K is not None:
           # K = K / diag(K).mean()
            QS = economic_qs(K)
        else:
            QS = None
        if lik_name == "normal":
            method = LMM(y.values, M.values, QS, restricted=True)
            method.fit(verbose=verbose)
        else:
            method = GLMMExpFam(y, lik, M.values, QS, n_int=500)
            method.fit(verbose=verbose, factr=1e6, pgtol=1e-3)
        g = method.scale * (1 - method.delta)
        e = method.scale * method.delta
        if lik_name == "bernoulli":
            e += pi * pi / 3
        v = var(method.mean())
        return g , v , e 


      \end{lstlisting}





\section{Genomic prediction}
\subsection{GP ANN}
\begin{lstlisting}[language=Python]

import os,sys,gc
import pandas as pd
import numpy as np
import timeit
from datetime import datetime
import keras
import tensorflow as tf
from keras import backend as K
from keras import layers
from keras.models import Sequential
from keras.layers import Dense, Dropout, GaussianNoise, AlphaDropout, Reshape
from keras.layers import Flatten, LocallyConnected1D, LocallyConnected2D
from keras.optimizers import Adam, Adagrad, Adadelta
from keras.backend.tensorflow_backend import set_session 

##set default values

learning_rate = 0.01
JobID = 1
ps = 25
optim = "adam"
X_file = "KE.geno.csv"
Y_file = "KE_pheno.csv"
CV_file = "KE_cv_pw.csv"
label = "DtSILK"
start_time = timeit.default_timer()
act="relu"
drop_rate = str('0.5,0.5,0.5')
arc = str('63,63')
DG = 'D,D,D,D,D,G'
LC = True
training_epochs = 25
hyp = False

### parse command line arguments

for i in range (1,len(sys.argv),2):
    if sys.argv[i] == "-x":
        X_file = sys.argv[i+1]
    elif sys.argv[i] == "-y":
        Y_file = sys.argv[i+1]
    elif sys.argv[i] == "-cv":
        CV_file = sys.argv[i+1]
    elif sys.argv[i] == "-JobID":
        JobID = int(sys.argv[i+1])
    elif sys.argv[i] == "-label":
        label = sys.argv[i+1]
    elif sys.argv[i] == "-act":
        act = str(sys.argv[i+1])
    elif sys.argv[i] == "-epochs":
        training_epochs = int(sys.argv[i+1])
    elif sys.argv[i] == "-lr":
        learning_rate = float(sys.argv[i+1])
    elif sys.argv[i] == "-arc":
        arc = sys.argv[i+1]
    elif sys.argv[i] == "-ps":
        ps = int(sys.argv[i+1])
    elif sys.argv[i] == "-dr":
        drop_rate=str(sys.argv[i+1])
    elif sys.argv[i] == "-LC":
         LC = bool(sys.argv[i+1])
    elif sys.argv[i] == "hyp":
        hyp = bool(sys.argv[i+1])
    else:
        print('unknown option ' + str(sys.argv[i]))
        quit()
       
        
x = pd.read_csv(X_file, index_col = 0)
y = pd.read_csv(Y_file, index_col = 0)
cv_folds = pd.read_csv(CV_file,index_col=0)

## select column of phenotype file via columnname

y = y[[label]]
## activity_regularizer=regularizers.l1(0.01)))

def build_network(arc,drop_rate,LC,DG):
    def add_drops(model,drop_out,k):
        if DG[k].upper() == 'D':
            model.add(Dropout(drop_out[0]))
        elif DG[k].upper() == 'G':
            model.add(GaussianNoise(drop_out[k]))
        elif DG[k].upper() == "A":
            model.add(AlphaDropout(drop_out[k]))
        else:
            pass
        return model    
    DG = DG.strip().split(",")
    arc = arc.strip().split(",")
    archit = []
    for layer in  arc:
        archit.append(int(layer))
    layer_number = len(archit)        
    drop_rate = drop_rate.strip().split(",")
    drop_out = []
    for drops in drop_rate:
        drop_out.append(float(drops)) 
    model = Sequential()
    if LC == True:
        model.add(Reshape(input_shape=(x_train.shape[1],),target_shape=(x_train.shape[1],1)))
        model.add(LocallyConnected1D(1,10,strides=7,input_shape=(x_train.shape[1],1)))
        model.add(Flatten())
        start = 0
        model = add_drops(model,drop_out,start)
    elif LC == False:
        model.add(Dense(archit[0], kernel_initializer='truncated_normal', activation=act, input_shape=(x_train.shape[1],)))
        model = add_drops(model,drop_out,start)
    start = 1
    for k in range(start,len(archit)):
        model.add(Dense(archit[k], kernel_initializer='truncated_normal', activation=act))
        model = add_drops(model,drop_out,k)
    model.add(Dense(1, kernel_initializer='truncated_normal'))
    return(model)
     
config = tf.ConfigProto()
#config.gpu_options.per_process_gpu_memory_fraction = 0.1
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))

if not os.path.isfile("RESULTScv50.txt"):
    out2 = open("RESULTScv50.txt",'w')
    out2.write('DateTime\tCompTime\tDF\tGenos\tPhenos\tCV_fold\tArchit\tConv\tActFun\tEpochs\tdrop_rate\tAccuracy\n' )
    
for k in range(1,51):
    print("Training on cv fold "+ str(k))
    cv = cv_folds['cv_' + str(k)]
    num_cvs = np.ptp(cv) + 1
    
    i = 1
    x_train = x[cv != i] 
    x_test = x[cv == i] 
    y_train = y[cv != i]
    y_test = y[cv == i]

    yhat = np.zeros(shape = y_test.shape)

    model = build_network(arc,drop_rate,LC,DG)
    model.compile(loss='mse', optimizer=Adam(lr=0.01,decay = 0.001),metrics=['accuracy'])
    model.fit(x_train,y_train, epochs=training_epochs , verbose=0) 
#    score = model.evaluate(x_test, y_test, verbose=0)
    bla = model.predict(x_test)
    y_sub= y[np.asarray(cv == i)]
    
    print(model.summary())
    print('\n')
    print(label)        

    comp_time = int(round(timeit.default_timer() - start_time,0))

    DateTime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    acc = np.corrcoef(bla[:,0],np.asarray(y_sub)[:,0])[0,1]

    out2 = open("RESULTScv50.txt", 'a')
    out2.write('%s\t%i\t%s\t%s\t%s\t%i\t%s\t%s\t%s\t%i\t%s\t%0.5f\n' % (
        DateTime, comp_time, label, X_file, Y_file, int(k), arc, LC,  act,int(training_epochs), drop_rate, round(acc,4)))

    del model,bla, x_train, x_test, y_train, y_test 
    K.clear_session() 
    gc.collect()
    
    config = tf.ConfigProto()
    #config.gpu_options.per_process_gpu_memory_fraction = 0.1
    config.gpu_options.allow_growth = True
    set_session(tf.Session(config=config))
\end{lstlisting}

\subsection{GBLUP}
\begin{lstlisting}[language=R]
    geno_pred <- function(phenocsv,genocsv,cvfcsv,cvf=1,mod = "BRR",label,phe)
{
    my_phe <- phe
    depends<- c("BGLR","doBy","doParallel",'R.utils',"BBmisc","dplyr")
    foo <- sapply(depends,
                  function(X){if(!suppressPackageStartupMessages(require(X,character.only = T))){install.packages(X)}})
    foo <- sapply(depends,function(X){suppressPackageStartupMessages(library(X,character.only=TRUE))})
    rm(foo) 
     
    maze <- read.csv(genocsv, row.names = 1)
    phe <- read.csv(phenocsv, row.names = 1)
    cvffolds <- read.csv(cvfcsv,row.names=1)
    
    X <- scale(maze)
    y <- phe[[label]]
    if(any(is.na(y))){
        rms <- which(is.na(y))
        y <- y[-rms]
        X <- X[-rms,]
    }
    for(i in 1:50){
        cvf = i
        n=length(y)
        seed <- sample(1:100,1)
                                        #set.seed(seed)
                                        #folds=sample(1:cvf,size=n,replace=T)
        folds = cvffolds[,cvf]
        yHatCV=rep(NA,n)
        
        for(i in 1:max(folds)){
            cat("Predicting cv-fold ",i," of ", max(folds))
            tst=which(folds==i)
            yNA=y
            yNA[tst]=NA
            fm=BGLR(y=yNA,ETA=list(list(X=X,model=mod)),verbose =F ,nIter=7000,burnIn=1000)
            yHatCV[tst]=fm$yHat[tst]
            cat("   done\n")
        }
        
        my_cor <- cor(yHatCV,y,use = "complete.obs")
        print(c("Corrleation of GP", mod, my_cor))
        filename = paste0(my_phe,"_gp_results.csv")
        print(filename)
        if(!any(dir() == filename)){
            res <- matrix(ncol=8, nrow = 1) %>%
                setColNames(c("geno","pheno","cv_folds","seed","label", "cor","method","nmark"))
            res[1,] <- c(as.character(genocsv),as.character(phenocsv),as.character(cvf),as.character(seed),
                         as.character(label),as.character(my_cor),as.character(mod),dim(X)[2])
            print("#################")
	    print(res)
	    print("############")
            write.csv(res,filename)
        }else{
            res <- read.csv(filename,row.names = 1)
            for(i in 1:7){
                res[,i] <- as.character(res[,i])
            }
            res[dim(res)[1]+1,] <- c(as.character(genocsv),phenocsv,cvf,seed,as.character(label),my_cor,mod,dim(X)[2])
            write.csv(res,filename)
        }
    }
    
  }
## execute this script with: Rscript ex.gblup.r -x genofile -y phenofile -c cv file
source("~/PHD/Projects/gblup/bglr.r")


my.args <- commandArgs(trailingOnly = TRUE)
#my.args <- c("-x", "gent_geno.csv","-y" , "gent_pheno.csv")
### set defaults 
#cvf.name = NA

## parsing the command line options 
all.opts <- c("-x","-y","-label","-h","-cv","-phe")
for(i in 1:length(my.args)){
    if( i %% 2 == 1){
        if(!my.args[i] %in%  all.opts){
            cat("unknown option", my.args[i], "Use only", all.opts , "\n")
            cat("use -h for help \n")
            quit()
        }
    }    
    if(my.args[i] == "-x"){
        geno.name <- as.character(my.args[i+1])
    } else if(my.args[i] == "-y") {
        pheno.name <- as.character(my.args[i+1])
    } else if(my.args[i] == "-label") {
        my_ph <- as.character(my.args[i+1])
    } else if(my.args[i] == "-cv"){
        cv.name = as.character(my.args[i+1])
    } else if(my.args[i] == "-phe"){
        my_phe <- as.character(my.args[i+1])
    } else if(my.args[i] == "-h") {
        print(" This script takes as a minimum two intputs\n")
        print(" -x genotypefile")
        print(" -y phenotypefile ")
        print(" -cv cross-valiadtion file : is optional if none is specified random 5 fold cv will be used")
        print(" -JobID : specify column number to use in your cross validation file")
        print(" -label : use header of phenotype file column you want to use")
        quit()
    }
}


#pheno.name <- my.args[1]
#geno.name <- my.args[2]
#cvf.name <- my.args[3]

geno_pred(phenocsv = pheno.name,genocsv=geno.name, cvfcsv = cv.name, label=my_ph,mod = "BRR", phe =my_phe)

\end{lstlisting}

%$
